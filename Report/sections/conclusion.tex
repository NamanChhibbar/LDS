\section{Future Work}
\label{sec:future-work}

To segment the document, we use a basic sentence tokenizer (\href{https://www.nltk.org/api/nltk.tokenize.sent_tokenize.html}{nltk.sent\_tokenize}) with some modifications to control the minimum number of words in a segment.
In our experiments, we find that segmentation is a crucial step in the distillation process and can greatly influence the output summary, indicating that good segmentation is vital for good distillation of text.
Ensuring the uniformity of the length of the segments while preserving coherence within a segment is also essential for better utilization of the context size of the model.
We encourage future work to experiment with different kinds of segmenters.

Future work can also focus on extending the "Summarization with Keyword Extraction" method (\autoref{method:keyword}).
Many potential ways exist to use the extracted keywords we do not touch upon.


\section{Conclusion}
\label{sec:conclusion}

Our experiments show that "Document Skimming with post-sampling removal" (\autoref{method:skimming}) performs well while being efficient.
The "Central Truncation" method (\autoref{method:truncation}) also shows good results, which shows that simple methods can also be effective when dealing with long inputs.
The last two methods, "Skimming with pre-sampling removal" (\autoref{method:skimming}) and "Summarization with Keyword Extraction" (\autoref{method:keyword}), achieve the best results but are computationally expensive.

Our experiments show significant improvement in BERTScore compared to Unlimiformer \cite{bertsch2023unlimiformer} on the GovReport dataset, showing that our pipelines can efficiently utilize details in long texts.
Even though our ROUGE-2 scores are lower than the baselines, ROUGE-1 and ROUGE-L scores are competitive.
Since BERTScore is better at capturing semantic similarity, we highlight the use of BERTScore compared to ROUGE scores.
Hence, we contend that our pipelines can generate better summaries than the baselines with higher ROUGE scores.
Also, note that the models used in our experiments have smaller context sizes than the baselines, indicating that our algorithms have a greater potential if used with larger models.


\noreview{
  \section*{Acknowledgement}
  
  All work herein reported is supported by the National Science Foundation under Grant No. 2349452.
  Any opinion, finding, or conclusion in this study is that of the authors and does not necessarily reflect the views of the National Science Foundation.
}


\section*{Supplementary Materials}

The datasets used in this study are available here:
\href{https://gov-report-data.github.io/}{GovReport},
\href{https://evasharma.github.io/bigpatent/}{BigPatent}

The code used in this study is available here:
\href{https://github.com/NamanChhibbar/Long-Document-Summarizer.git}{GitHub}
