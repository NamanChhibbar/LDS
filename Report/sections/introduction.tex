\section{Introduction}
\label{sec:introduction}

Due to the ever-increasing amount of textual data available online, document summarization has become crucial for efficient and accurate extraction of relevant information.
Large Language Models (LLMs) based on the transformer architecture \cite{vaswani2017attention} have shown outstanding abilities in many NLP tasks, including document summarization \cite{yadav2023state}.
Recent developments have demonstrated remarkable improvements in the relevancy and coherence of summaries generated by such LLMs.

However, long document summarization, which involves removing redundancies and makes reading long texts concise and efficient, remains a major challenge.
One of the significant limitations in the transformer architecture is limited context size, stemming from the quadratic memory and computational complexity of the attention mechanism \cite{du2023improving}.
This constraint hinders extracting relevant information from extensive texts where summarization is valuable to overcome the time, effort, and interpretive issues posed by complex and large documents.

We experiment with three novel approaches to address the input size limitations of transformers.
The methods introduced do not include any architectural modifications to the model used and can be incorporated into any existing pipeline.
We believe that these methods can effectively utilize the full potential of any existing LLM by capturing information from crucial aspects of the document.
Though our experiments only include the task of summarization, we hypothesize that our methods can be applied to NLP tasks that require processing long texts.

We start by stating the problem statement (\autoref{sec:problem}) and discussing related works (\autoref{sec:related-works}) to gain insights into the problem and the state-of-the-art solutions.
We then introduce the datasets (\autoref{sec:datasets}) and methodology (\autoref{sec:methodology}) used in our experiments.
For evaluating our results, we present some common metrics (\autoref{sec:metrics}) used in text summarization.
We end the report by discussing our experimental findings (\autoref{sec:findings}) and potential future work (\autoref{sec:future-work}), and concluding the study (\autoref{sec:conclusion}).
