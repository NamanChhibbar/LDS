@article{wang2024videoagent,
  title={VideoAgent: Long-form Video Understanding with Large Language Model as Agent},
  author={Wang, Xiaohan and Zhang, Yuhui and Zohar, Orr and Yeung-Levy, Serena},
  journal={arXiv preprint arXiv:2403.10517},
  year={2024},
  url="https://arxiv.org/abs/2403.10517"
}

@article{chen2022long,
  title={A long-text classification method of Chinese news based on BERT and CNN},
  author={Chen, Xinying and Cong, Peimin and Lv, Shuo},
  journal={IEEE Access},
  volume={10},
  pages={34046--34057},
  year={2022},
  publisher={IEEE},
	url="https://ieeexplore.ieee.org/abstract/document/9743465"
}

@article{yadav2023state,
  title={State-of-the-art approach to extractive text summarization: a comprehensive review},
  author={Yadav, Avaneesh Kumar and Ranvijay and Yadav, Rama Shankar and Maurya, Ashish Kumar},
  journal={Multimedia Tools and Applications},
  volume={82},
  number={19},
  pages={29135--29197},
  year={2023},
  publisher={Springer},
	url="https://link.springer.com/article/10.1007/s11042-023-14613-9"
}

@inproceedings{sun2019fine,
  title={How to fine-tune bert for text classification?},
  author={Sun, Chi and Qiu, Xipeng and Xu, Yige and Huang, Xuanjing},
  booktitle={Chinese computational linguistics: 18th China national conference, CCL 2019, Kunming, China, October 18--20, 2019, proceedings 18},
  pages={194--206},
  year={2019},
  organization={Springer},
	url="https://link.springer.com/chapter/10.1007/978-3-030-32381-3_16"
}

@article{beltagy2020longformer,
  title={Longformer: The long-document transformer},
  author={Beltagy, Iz and Peters, Matthew E and Cohan, Arman},
  journal={arXiv preprint arXiv:2004.05150},
  year={2020},
  url="https://arxiv.org/abs/2004.05150"
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017},
  url="https://proceedings.neurips.cc/paper/7181-attention-is-all"
}

@inproceedings{10.1145/3639233.3639253,
  author = {Golia, Logan and Kalita, Jugal},
  title = {Action-Item-Driven Summarization of Long Meeting Transcripts},
  year = {2024},
  isbn = {9798400709227},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/3639233.3639253},
  doi = {10.1145/3639233.3639253},
  abstract = {The increased prevalence of online meetings has significantly enhanced the practicality of a model that can automatically generate the summary of a given meeting. This paper introduces a novel and effective approach to automate the generation of meeting summaries. Current approaches to this problem generate general and basic summaries, considering the meeting simply as a long dialogue. However, our novel algorithms can generate abstractive meeting summaries that are driven by the action items contained in the meeting transcript. This is done by recursively generating summaries and employing our action-item extraction algorithm for each section of the meeting in parallel. All of these sectional summaries are then combined and summarized together to create a coherent and action-item-driven summary. In addition, this paper introduces three novel methods for dividing up long transcripts into topic-based sections to improve the time efficiency of our algorithm, as well as to resolve the issue of large language models (LLMs) forgetting long-term dependencies. Our pipeline achieved a BERTScore of 64.98 across the AMI corpus, which is an approximately 4.98\% increase from the current state-of-the-art result produced by a fine-tuned BART (Bidirectional and Auto-Regressive Transformers) model.1},
  booktitle = {Proceedings of the 2023 7th International Conference on Natural Language Processing and Information Retrieval},
  pages = {91â€“98},
  numpages = {8},
  keywords = {action item extraction, neural networks, text summarization, topic segmentation},
  location = {<conf-loc>, <city>Seoul</city>, <country>Republic of Korea</country>, </conf-loc>},
  series = {NLPIR '23}
}

@inproceedings{worsham-kalita-2018-genre,
  title = "Genre Identification and the Compositional Effect of Genre in Literature",
  author = "Worsham, Joseph  and
    Kalita, Jugal",
  editor = "Bender, Emily M.  and
    Derczynski, Leon  and
    Isabelle, Pierre",
  booktitle = "Proceedings of the 27th International Conference on Computational Linguistics",
  month = aug,
  year = "2018",
  address = "Santa Fe, New Mexico, USA",
  publisher = "Association for Computational Linguistics",
  url = "https://aclanthology.org/C18-1167",
  pages = "1963--1973",
  abstract = "Recent advances in Natural Language Processing are finding ways to place an emphasis on the hierarchical nature of text instead of representing language as a flat sequence or unordered collection of words or letters. A human reader must capture multiple levels of abstraction and meaning in order to formulate an understanding of a document. In this paper, we address the problem of developing approaches which are capable of working with extremely large and complex literary documents to perform Genre Identification. The task is to assign the literary classification to a full-length book belonging to a corpus of literature, where the works on average are well over 200,000 words long and genre is an abstract thematic concept. We introduce the Gutenberg Dataset for Genre Identification. Additionally, we present a study on how current deep learning models compare to traditional methods for this task. The results are presented as a baseline along with findings on how using an ensemble of chapters can significantly improve results in deep learning methods. The motivation behind the ensemble of chapters method is discussed as the compositionality of subtexts which make up a larger work and contribute to the overall genre.",
}

@inproceedings{lin-2004-rouge,
  title = "{ROUGE}: A Package for Automatic Evaluation of Summaries",
  author = "Lin, Chin-Yew",
  booktitle = "Text Summarization Branches Out",
  month = jul,
  year = "2004",
  address = "Barcelona, Spain",
  publisher = "Association for Computational Linguistics",
  url = "https://aclanthology.org/W04-1013",
  pages = "74--81",
}

@inproceedings{papineni-etal-2002-bleu,
  title = "{B}leu: a Method for Automatic Evaluation of Machine Translation",
  author = "Papineni, Kishore  and
    Roukos, Salim  and
    Ward, Todd  and
    Zhu, Wei-Jing",
  editor = "Isabelle, Pierre  and
    Charniak, Eugene  and
    Lin, Dekang",
  booktitle = "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics",
  month = jul,
  year = "2002",
  address = "Philadelphia, Pennsylvania, USA",
  publisher = "Association for Computational Linguistics",
  url = "https://aclanthology.org/P02-1040",
  doi = "10.3115/1073083.1073135",
  pages = "311--318",
}

@inproceedings{banerjee-lavie-2005-meteor,
  title = "{METEOR}: An Automatic Metric for {MT} Evaluation with Improved Correlation with Human Judgments",
  author = "Banerjee, Satanjeev  and
    Lavie, Alon",
  editor = "Goldstein, Jade  and
    Lavie, Alon  and
    Lin, Chin-Yew  and
    Voss, Clare",
  booktitle = "Proceedings of the {ACL} Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization",
  month = jun,
  year = "2005",
  address = "Ann Arbor, Michigan",
  publisher = "Association for Computational Linguistics",
  url = "https://aclanthology.org/W05-0909",
  pages = "65--72",
}

@inproceedings{mihalcea-tarau-2004-textrank,
  title = "{T}ext{R}ank: Bringing Order into Text",
  author = "Mihalcea, Rada  and
    Tarau, Paul",
  editor = "Lin, Dekang  and
    Wu, Dekai",
  booktitle = "Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing",
  month = jul,
  year = "2004",
  address = "Barcelona, Spain",
  publisher = "Association for Computational Linguistics",
  url = "https://aclanthology.org/W04-3252",
  pages = "404--411",
}

@article{erkan2004lexrank,
  title={Lexrank: Graph-based lexical centrality as salience in text summarization},
  author={Erkan, G{\"u}nes and Radev, Dragomir R},
  journal={Journal of artificial intelligence research},
  volume={22},
  pages={457--479},
  year={2004},
  url="https://www.jair.org/index.php/jair/article/view/10396"
}

@INPROCEEDINGS{10188527,
  author={Siddika, Saba and Hossen, Md. Sharif},
  booktitle={2022 International Conference on Recent Progresses in Science, Engineering and Technology (ICRPSET)}, 
  title={Automatic Text Summarization Using Term Frequency, Luhn's Heuristic, and Cosine Similarity Approaches}, 
  year={2022},
  volume={},
  number={},
  pages={1-6},
  keywords={Time-frequency analysis;Semantics;Frequency measurement;Text mining;Size measurement;Runtime;Flowcharts;Python;NLTK;WordNet;Corpus;Cosine Similarity;Automatic Text Summarizer},
  doi={10.1109/ICRPSET57982.2022.10188527},
  url="https://ieeexplore.ieee.org/document/10188527"
}

@inproceedings{zheng-lapata-2019-sentence,
  title = "Sentence Centrality Revisited for Unsupervised Summarization",
  author = "Zheng, Hao  and
    Lapata, Mirella",
  editor = "Korhonen, Anna  and
    Traum, David  and
    M{\`a}rquez, Llu{\'\i}s",
  booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
  month = jul,
  year = "2019",
  address = "Florence, Italy",
  publisher = "Association for Computational Linguistics",
  url = "https://aclanthology.org/P19-1628",
  doi = "10.18653/v1/P19-1628",
  pages = "6236--6247",
  abstract = "Single document summarization has enjoyed renewed interest in recent years thanks to the popularity of neural network models and the availability of large-scale datasets. In this paper we develop an unsupervised approach arguing that it is unrealistic to expect large-scale and high-quality training data to be available or created for different types of summaries, domains, or languages. We revisit a popular graph-based ranking algorithm and modify how node (aka sentence) centrality is computed in two ways: (a) we employ BERT, a state-of-the-art neural representation learning model to better capture sentential meaning and (b) we build graphs with directed edges arguing that the contribution of any two nodes to their respective centrality is influenced by their relative position in a document. Experimental results on three news summarization datasets representative of different languages and writing styles show that our approach outperforms strong baselines by a wide margin.",
}

@article{Nallapati_Zhai_Zhou_2017,
  title={SummaRuNNer: A Recurrent Neural Network Based Sequence Model for Extractive Summarization of Documents},
  volume={31},
  url={https://ojs.aaai.org/index.php/AAAI/article/view/10958},
  DOI={10.1609/aaai.v31i1.10958},
  abstractNote={ &lt;p&gt; We present SummaRuNNer, a Recurrent Neural Network (RNN) based sequence model for extractive summarization of documents and show that it achieves performance better than or comparable to state-of-the-art. Our model has the additional advantage of being very interpretable, since it allows visualization of its predictions broken up by abstract features such as information content, salience and novelty. Another novel contribution of our work is abstractive training of our extractive model that can train on human generated reference summaries alone, eliminating the need for sentence-level extractive labels. &lt;/p&gt; },
  number={1},
  journal={Proceedings of the AAAI Conference on Artificial Intelligence},
  author={Nallapati, Ramesh and Zhai, Feifei and Zhou, Bowen},
  year={2017},
  month={Feb.}
}
