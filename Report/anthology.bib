% Literature

@article{wang2024videoagent,
  title={VideoAgent: Long-form Video Understanding with Large Language Model as Agent},
  author={Wang, Xiaohan and Zhang, Yuhui and Zohar, Orr and Yeung-Levy, Serena},
  journal={arXiv preprint arXiv:2403.10517},
  year={2024},
  url="https://arxiv.org/abs/2403.10517"
}

@article{chen2022long,
  title={A long-text classification method of Chinese news based on BERT and CNN},
  author={Chen, Xinying and Cong, Peimin and Lv, Shuo},
  journal={IEEE Access},
  volume={10},
  pages={34046--34057},
  year={2022},
  publisher={IEEE},
	url="https://ieeexplore.ieee.org/abstract/document/9743465"
}

@article{yadav2023state,
  title={State-of-the-art approach to extractive text summarization: a comprehensive review},
  author={Yadav, Avaneesh Kumar and Ranvijay and Yadav, Rama Shankar and Maurya, Ashish Kumar},
  journal={Multimedia Tools and Applications},
  volume={82},
  number={19},
  pages={29135--29197},
  year={2023},
  publisher={Springer},
	url="https://link.springer.com/article/10.1007/s11042-023-14613-9"
}

@inproceedings{sun2019fine,
  title={How to fine-tune bert for text classification?},
  author={Sun, Chi and Qiu, Xipeng and Xu, Yige and Huang, Xuanjing},
  booktitle={Chinese computational linguistics: 18th China national conference, CCL 2019, Kunming, China, October 18--20, 2019, proceedings 18},
  pages={194--206},
  year={2019},
  organization={Springer},
	url="https://link.springer.com/chapter/10.1007/978-3-030-32381-3_16"
}

@article{beltagy2020longformer,
  title={Longformer: The long-document transformer},
  author={Beltagy, Iz and Peters, Matthew E and Cohan, Arman},
  journal={arXiv preprint arXiv:2004.05150},
  year={2020},
  url="https://arxiv.org/abs/2004.05150"
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017},
  url="https://proceedings.neurips.cc/paper/7181-attention-is-all"
}

@inproceedings{10.1145/3639233.3639253,
  author = {Golia, Logan and Kalita, Jugal},
  title = {Action-Item-Driven Summarization of Long Meeting Transcripts},
  year = {2024},
  isbn = {9798400709227},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/3639233.3639253},
  doi = {10.1145/3639233.3639253},
  abstract = {The increased prevalence of online meetings has significantly enhanced the practicality of a model that can automatically generate the summary of a given meeting. This paper introduces a novel and effective approach to automate the generation of meeting summaries. Current approaches to this problem generate general and basic summaries, considering the meeting simply as a long dialogue. However, our novel algorithms can generate abstractive meeting summaries that are driven by the action items contained in the meeting transcript. This is done by recursively generating summaries and employing our action-item extraction algorithm for each section of the meeting in parallel. All of these sectional summaries are then combined and summarized together to create a coherent and action-item-driven summary. In addition, this paper introduces three novel methods for dividing up long transcripts into topic-based sections to improve the time efficiency of our algorithm, as well as to resolve the issue of large language models (LLMs) forgetting long-term dependencies. Our pipeline achieved a BERTScore of 64.98 across the AMI corpus, which is an approximately 4.98\% increase from the current state-of-the-art result produced by a fine-tuned BART (Bidirectional and Auto-Regressive Transformers) model.1},
  booktitle = {Proceedings of the 2023 7th International Conference on Natural Language Processing and Information Retrieval},
  pages = {91â€“98},
  numpages = {8},
  keywords = {action item extraction, neural networks, text summarization, topic segmentation},
  location = {<conf-loc>, <city>Seoul</city>, <country>Republic of Korea</country>, </conf-loc>},
  series = {NLPIR '23}
}

@inproceedings{worsham-kalita-2018-genre,
  title = "Genre Identification and the Compositional Effect of Genre in Literature",
  author = "Worsham, Joseph  and
    Kalita, Jugal",
  editor = "Bender, Emily M.  and
    Derczynski, Leon  and
    Isabelle, Pierre",
  booktitle = "Proceedings of the 27th International Conference on Computational Linguistics",
  month = aug,
  year = "2018",
  address = "Santa Fe, New Mexico, USA",
  publisher = "Association for Computational Linguistics",
  url = "https://aclanthology.org/C18-1167",
  pages = "1963--1973",
  abstract = "Recent advances in Natural Language Processing are finding ways to place an emphasis on the hierarchical nature of text instead of representing language as a flat sequence or unordered collection of words or letters. A human reader must capture multiple levels of abstraction and meaning in order to formulate an understanding of a document. In this paper, we address the problem of developing approaches which are capable of working with extremely large and complex literary documents to perform Genre Identification. The task is to assign the literary classification to a full-length book belonging to a corpus of literature, where the works on average are well over 200,000 words long and genre is an abstract thematic concept. We introduce the Gutenberg Dataset for Genre Identification. Additionally, we present a study on how current deep learning models compare to traditional methods for this task. The results are presented as a baseline along with findings on how using an ensemble of chapters can significantly improve results in deep learning methods. The motivation behind the ensemble of chapters method is discussed as the compositionality of subtexts which make up a larger work and contribute to the overall genre.",
}

@inproceedings{mihalcea-tarau-2004-textrank,
  title = "{T}ext{R}ank: Bringing Order into Text",
  author = "Mihalcea, Rada  and
    Tarau, Paul",
  editor = "Lin, Dekang  and
    Wu, Dekai",
  booktitle = "Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing",
  month = jul,
  year = "2004",
  address = "Barcelona, Spain",
  publisher = "Association for Computational Linguistics",
  url = "https://aclanthology.org/W04-3252",
  pages = "404--411",
}

@article{erkan2004lexrank,
  title={Lexrank: Graph-based lexical centrality as salience in text summarization},
  author={Erkan, G{\"u}nes and Radev, Dragomir R},
  journal={Journal of artificial intelligence research},
  volume={22},
  pages={457--479},
  year={2004},
  url="https://www.jair.org/index.php/jair/article/view/10396"
}

@INPROCEEDINGS{10188527,
  author={Siddika, Saba and Hossen, Md. Sharif},
  booktitle={2022 International Conference on Recent Progresses in Science, Engineering and Technology (ICRPSET)}, 
  title={Automatic Text Summarization Using Term Frequency, Luhn's Heuristic, and Cosine Similarity Approaches}, 
  year={2022},
  volume={},
  number={},
  pages={1-6},
  keywords={Time-frequency analysis;Semantics;Frequency measurement;Text mining;Size measurement;Runtime;Flowcharts;Python;NLTK;WordNet;Corpus;Cosine Similarity;Automatic Text Summarizer},
  doi={10.1109/ICRPSET57982.2022.10188527},
  url="https://ieeexplore.ieee.org/document/10188527"
}

@inproceedings{zheng-lapata-2019-sentence,
  title = "Sentence Centrality Revisited for Unsupervised Summarization",
  author = "Zheng, Hao  and
    Lapata, Mirella",
  editor = "Korhonen, Anna  and
    Traum, David  and
    M{\`a}rquez, Llu{\'\i}s",
  booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
  month = jul,
  year = "2019",
  address = "Florence, Italy",
  publisher = "Association for Computational Linguistics",
  url = "https://aclanthology.org/P19-1628",
  doi = "10.18653/v1/P19-1628",
  pages = "6236--6247",
  abstract = "Single document summarization has enjoyed renewed interest in recent years thanks to the popularity of neural network models and the availability of large-scale datasets. In this paper we develop an unsupervised approach arguing that it is unrealistic to expect large-scale and high-quality training data to be available or created for different types of summaries, domains, or languages. We revisit a popular graph-based ranking algorithm and modify how node (aka sentence) centrality is computed in two ways: (a) we employ BERT, a state-of-the-art neural representation learning model to better capture sentential meaning and (b) we build graphs with directed edges arguing that the contribution of any two nodes to their respective centrality is influenced by their relative position in a document. Experimental results on three news summarization datasets representative of different languages and writing styles show that our approach outperforms strong baselines by a wide margin.",
}

@article{Nallapati_Zhai_Zhou_2017,
  title={SummaRuNNer: A Recurrent Neural Network Based Sequence Model for Extractive Summarization of Documents},
  volume={31},
  url={https://ojs.aaai.org/index.php/AAAI/article/view/10958},
  DOI={10.1609/aaai.v31i1.10958},
  abstractNote={ &lt;p&gt; We present SummaRuNNer, a Recurrent Neural Network (RNN) based sequence model for extractive summarization of documents and show that it achieves performance better than or comparable to state-of-the-art. Our model has the additional advantage of being very interpretable, since it allows visualization of its predictions broken up by abstract features such as information content, salience and novelty. Another novel contribution of our work is abstractive training of our extractive model that can train on human generated reference summaries alone, eliminating the need for sentence-level extractive labels. &lt;/p&gt; },
  number={1},
  journal={Proceedings of the AAAI Conference on Artificial Intelligence},
  author={Nallapati, Ramesh and Zhai, Feifei and Zhou, Bowen},
  year={2017},
  month={Feb.}
}


% Datasets

%% GovReport
@inproceedings{huang-etal-2021-efficient,
  title = "Efficient Attentions for Long Document Summarization",
  author = "Huang, Luyang  and
    Cao, Shuyang  and
    Parulian, Nikolaus  and
    Ji, Heng  and
    Wang, Lu",
  booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
  month = jun,
  year = "2021",
  address = "Online",
  publisher = "Association for Computational Linguistics",
  url = "https://aclanthology.org/2021.naacl-main.112",
  doi = "10.18653/v1/2021.naacl-main.112",
  pages = "1419--1436",
  abstract = "The quadratic computational and memory complexities of large Transformers have limited their scalability for long document summarization. In this paper, we propose Hepos, a novel efficient encoder-decoder attention with head-wise positional strides to effectively pinpoint salient information from the source. We further conduct a systematic study of existing efficient self-attentions. Combined with Hepos, we are able to process ten times more tokens than existing models that use full attentions. For evaluation, we present a new dataset, GovReport, with significantly longer documents and summaries. Results show that our models produce significantly higher ROUGE scores than competitive comparisons, including new state-of-the-art results on PubMed. Human evaluation also shows that our models generate more informative summaries with fewer unfaithful errors.",
}

%% BookSum
@article{kryscinski2021booksum,
  title={Booksum: A collection of datasets for long-form narrative summarization},
  author={Kry{\'s}ci{\'n}ski, Wojciech and Rajani, Nazneen and Agarwal, Divyansh and Xiong, Caiming and Radev, Dragomir},
  journal={arXiv preprint arXiv:2105.08209},
  year={2021},
  url="https://arxiv.org/abs/2105.08209"
}

%% BigPatent
@inproceedings{sharma-etal-2019-bigpatent,
  title = "{BIGPATENT}: A Large-Scale Dataset for Abstractive and Coherent Summarization",
  author = "Sharma, Eva  and
    Li, Chen  and
    Wang, Lu",
  editor = "Korhonen, Anna  and
    Traum, David  and
    M{\`a}rquez, Llu{\'\i}s",
  booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
  month = jul,
  year = "2019",
  address = "Florence, Italy",
  publisher = "Association for Computational Linguistics",
  url = "https://aclanthology.org/P19-1212",
  doi = "10.18653/v1/P19-1212",
  pages = "2204--2213",
  abstract = "Most existing text summarization datasets are compiled from the news domain, where summaries have a flattened discourse structure. In such datasets, summary-worthy content often appears in the beginning of input articles. Moreover, large segments from input articles are present verbatim in their respective summaries. These issues impede the learning and evaluation of systems that can understand an article{'}s global content structure as well as produce abstractive summaries with high compression ratio. In this work, we present a novel dataset, BIGPATENT, consisting of 1.3 million records of U.S. patent documents along with human written abstractive summaries. Compared to existing summarization datasets, BIGPATENT has the following properties: i) summaries contain a richer discourse structure with more recurring entities, ii) salient content is evenly distributed in the input, and iii) lesser and shorter extractive fragments are present in the summaries. Finally, we train and evaluate baselines and popular learning models on BIGPATENT to shed light on new challenges and motivate future directions for summarization research.",
}


% Metrics

%% ROUGE
@inproceedings{lin-2004-rouge,
  title = "{ROUGE}: A Package for Automatic Evaluation of Summaries",
  author = "Lin, Chin-Yew",
  booktitle = "Text Summarization Branches Out",
  month = jul,
  year = "2004",
  address = "Barcelona, Spain",
  publisher = "Association for Computational Linguistics",
  url = "https://aclanthology.org/W04-1013",
  pages = "74--81",
}

%% BLEU
@inproceedings{papineni-etal-2002-bleu,
  title = "{B}leu: a Method for Automatic Evaluation of Machine Translation",
  author = "Papineni, Kishore  and
    Roukos, Salim  and
    Ward, Todd  and
    Zhu, Wei-Jing",
  editor = "Isabelle, Pierre  and
    Charniak, Eugene  and
    Lin, Dekang",
  booktitle = "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics",
  month = jul,
  year = "2002",
  address = "Philadelphia, Pennsylvania, USA",
  publisher = "Association for Computational Linguistics",
  url = "https://aclanthology.org/P02-1040",
  doi = "10.3115/1073083.1073135",
  pages = "311--318",
}

%% METEOR
@inproceedings{banerjee-lavie-2005-meteor,
  title = "{METEOR}: An Automatic Metric for {MT} Evaluation with Improved Correlation with Human Judgments",
  author = "Banerjee, Satanjeev  and
    Lavie, Alon",
  editor = "Goldstein, Jade  and
    Lavie, Alon  and
    Lin, Chin-Yew  and
    Voss, Clare",
  booktitle = "Proceedings of the {ACL} Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization",
  month = jun,
  year = "2005",
  address = "Ann Arbor, Michigan",
  publisher = "Association for Computational Linguistics",
  url = "https://aclanthology.org/W05-0909",
  pages = "65--72",
}

%% BERTScore
@inproceedings{hanna-bojar-2021-fine,
  title = "A Fine-Grained Analysis of {BERTS}core",
  author = "Hanna, Michael  and
    Bojar, Ond{\v{r}}ej",
  editor = "Barrault, Loic  and
    Bojar, Ondrej  and
    Bougares, Fethi  and
    Chatterjee, Rajen  and
    Costa-jussa, Marta R.  and
    Federmann, Christian  and
    Fishel, Mark  and
    Fraser, Alexander  and
    Freitag, Markus  and
    Graham, Yvette  and
    Grundkiewicz, Roman  and
    Guzman, Paco  and
    Haddow, Barry  and
    Huck, Matthias  and
    Yepes, Antonio Jimeno  and
    Koehn, Philipp  and
    Kocmi, Tom  and
    Martins, Andre  and
    Morishita, Makoto  and
    Monz, Christof",
  booktitle = "Proceedings of the Sixth Conference on Machine Translation",
  month = nov,
  year = "2021",
  address = "Online",
  publisher = "Association for Computational Linguistics",
  url = "https://aclanthology.org/2021.wmt-1.59",
  pages = "507--517",
  abstract = "BERTScore, a recently proposed automatic metric for machine translation quality, uses BERT, a large pre-trained language model to evaluate candidate translations with respect to a gold translation. Taking advantage of BERT{'}s semantic and syntactic abilities, BERTScore seeks to avoid the flaws of earlier approaches like BLEU, instead scoring candidate translations based on their semantic similarity to the gold sentence. However, BERT is not infallible; while its performance on NLP tasks set a new state of the art in general, studies of specific syntactic and semantic phenomena have shown where BERT{'}s performance deviates from that of humans more generally. This naturally raises the questions we address in this paper: what are the strengths and weaknesses of BERTScore? Do they relate to known weaknesses on the part of BERT? We find that while BERTScore can detect when a candidate differs from a reference in important content words, it is less sensitive to smaller errors, especially if the candidate is lexically or stylistically similar to the reference.",
}
