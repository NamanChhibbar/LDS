% Literature

@article{wang2024videoagent,
  title={VideoAgent: Long-form Video Understanding with Large Language Model as Agent},
  author={Wang, Xiaohan and Zhang, Yuhui and Zohar, Orr and Yeung-Levy, Serena},
  journal={arXiv preprint arXiv:2403.10517},
  year={2024},
  url="https://arxiv.org/abs/2403.10517"
}

@article{chen2022long,
  title={A long-text classification method of Chinese news based on BERT and CNN},
  author={Chen, Xinying and Cong, Peimin and Lv, Shuo},
  journal={IEEE Access},
  volume={10},
  pages={34046--34057},
  year={2022},
  publisher={IEEE},
	url="https://ieeexplore.ieee.org/abstract/document/9743465"
}

@article{yadav2023state,
  title={State-of-the-art approach to extractive text summarization: a comprehensive review},
  author={Yadav, Avaneesh Kumar and Ranvijay and Yadav, Rama Shankar and Maurya, Ashish Kumar},
  journal={Multimedia Tools and Applications},
  volume={82},
  number={19},
  pages={29135--29197},
  year={2023},
  publisher={Springer},
	url="https://link.springer.com/article/10.1007/s11042-023-14613-9"
}

@inproceedings{sun2019fine,
  title={How to fine-tune bert for text classification?},
  author={Sun, Chi and Qiu, Xipeng and Xu, Yige and Huang, Xuanjing},
  booktitle={Chinese computational linguistics: 18th China national conference, CCL 2019, Kunming, China, October 18--20, 2019, proceedings 18},
  pages={194--206},
  year={2019},
  organization={Springer},
	url="https://link.springer.com/chapter/10.1007/978-3-030-32381-3_16"
}

@article{beltagy2020longformer,
  title={Longformer: The long-document transformer},
  author={Beltagy, Iz and Peters, Matthew E and Cohan, Arman},
  journal={arXiv preprint arXiv:2004.05150},
  year={2020},
  url="https://arxiv.org/abs/2004.05150"
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017},
  url="https://proceedings.neurips.cc/paper/7181-attention-is-all"
}

@inproceedings{golia2024action,
  author = {Golia, Logan and Kalita, Jugal},
  title = {Action-Item-Driven Summarization of Long Meeting Transcripts},
  year = {2024},
  isbn = {9798400709227},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/3639233.3639253},
  doi = {10.1145/3639233.3639253},
  abstract = {The increased prevalence of online meetings has significantly enhanced the practicality of a model that can automatically generate the summary of a given meeting. This paper introduces a novel and effective approach to automate the generation of meeting summaries. Current approaches to this problem generate general and basic summaries, considering the meeting simply as a long dialogue. However, our novel algorithms can generate abstractive meeting summaries that are driven by the action items contained in the meeting transcript. This is done by recursively generating summaries and employing our action-item extraction algorithm for each section of the meeting in parallel. All of these sectional summaries are then combined and summarized together to create a coherent and action-item-driven summary. In addition, this paper introduces three novel methods for dividing up long transcripts into topic-based sections to improve the time efficiency of our algorithm, as well as to resolve the issue of large language models (LLMs) forgetting long-term dependencies. Our pipeline achieved a BERTScore of 64.98 across the AMI corpus, which is an approximately 4.98\% increase from the current state-of-the-art result produced by a fine-tuned BART (Bidirectional and Auto-Regressive Transformers) model.1},
  booktitle = {Proceedings of the 2023 7th International Conference on Natural Language Processing and Information Retrieval},
  pages = {91–98},
  numpages = {8},
  keywords = {action item extraction, neural networks, text summarization, topic segmentation},
  location = {<conf-loc>, <city>Seoul</city>, <country>Republic of Korea</country>, </conf-loc>},
  series = {NLPIR '23}
}

@inproceedings{worsham-kalita-2018-genre,
  title = "Genre Identification and the Compositional Effect of Genre in Literature",
  author = "Worsham, Joseph  and
    Kalita, Jugal",
  editor = "Bender, Emily M.  and
    Derczynski, Leon  and
    Isabelle, Pierre",
  booktitle = "Proceedings of the 27th International Conference on Computational Linguistics",
  month = aug,
  year = "2018",
  address = "Santa Fe, New Mexico, USA",
  publisher = "Association for Computational Linguistics",
  url = "https://aclanthology.org/C18-1167",
  pages = "1963--1973",
  abstract = "Recent advances in Natural Language Processing are finding ways to place an emphasis on the hierarchical nature of text instead of representing language as a flat sequence or unordered collection of words or letters. A human reader must capture multiple levels of abstraction and meaning in order to formulate an understanding of a document. In this paper, we address the problem of developing approaches which are capable of working with extremely large and complex literary documents to perform Genre Identification. The task is to assign the literary classification to a full-length book belonging to a corpus of literature, where the works on average are well over 200,000 words long and genre is an abstract thematic concept. We introduce the Gutenberg Dataset for Genre Identification. Additionally, we present a study on how current deep learning models compare to traditional methods for this task. The results are presented as a baseline along with findings on how using an ensemble of chapters can significantly improve results in deep learning methods. The motivation behind the ensemble of chapters method is discussed as the compositionality of subtexts which make up a larger work and contribute to the overall genre.",
}

@inproceedings{bertsch2023unlimiformer,
  author = {Bertsch, Amanda and Alon, Uri and Neubig, Graham and Gormley, Matthew},
  booktitle = {Advances in Neural Information Processing Systems},
  editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
  pages = {35522--35543},
  publisher = {Curran Associates, Inc.},
  title = {Unlimiformer: Long-Range Transformers with Unlimited Length Input},
  url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/6f9806a5adc72b5b834b27e4c7c0df9b-Paper-Conference.pdf},
  volume = {36},
  year = {2023}
}

@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018},
  url="https://arxiv.org/abs/1810.04805?amp=1"
}

@article{wang2020minilm,
  title={Minilm: Deep self-attention distillation for task-agnostic compression of pre-trained transformers},
  author={Wang, Wenhui and Wei, Furu and Dong, Li and Bao, Hangbo and Yang, Nan and Zhou, Ming},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={5776--5788},
  year={2020},
  url="https://proceedings.neurips.cc/paper/2020/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html"
}

@inproceedings{lewis-etal-2020-bart,
  title = "{BART}: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension",
  author = "Lewis, Mike  and
    Liu, Yinhan  and
    Goyal, Naman  and
    Ghazvininejad, Marjan  and
    Mohamed, Abdelrahman  and
    Levy, Omer  and
    Stoyanov, Veselin  and
    Zettlemoyer, Luke",
  editor = "Jurafsky, Dan  and
    Chai, Joyce  and
    Schluter, Natalie  and
    Tetreault, Joel",
  booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
  month = jul,
  year = "2020",
  address = "Online",
  publisher = "Association for Computational Linguistics",
  url = "https://aclanthology.org/2020.acl-main.703",
  doi = "10.18653/v1/2020.acl-main.703",
  pages = "7871--7880",
  abstract = "We present BART, a denoising autoencoder for pretraining sequence-to-sequence models. BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and other recent pretraining schemes. We evaluate a number of noising approaches, finding the best performance by both randomly shuffling the order of sentences and using a novel in-filling scheme, where spans of text are replaced with a single mask token. BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa on GLUE and SQuAD, and achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 3.5 ROUGE. BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining. We also replicate other pretraining schemes within the BART framework, to understand their effect on end-task performance."
}

@article{guo2021longt5,
  title={LongT5: Efficient text-to-text transformer for long sequences},
  author={Guo, Mandy and Ainslie, Joshua and Uthus, David and Ontanon, Santiago and Ni, Jianmo and Sung, Yun-Hsuan and Yang, Yinfei},
  journal={arXiv preprint arXiv:2112.07916},
  year={2021},
  url="https://arxiv.org/abs/2112.07916"
}

@article{raffel2020exploring,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={Journal of machine learning research},
  volume={21},
  number={140},
  pages={1--67},
  year={2020},
  url="https://www.jmlr.org/papers/v21/20-074.html"
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020},
  url="https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html"
}

@article{fabbri2021summeval,
  title={Summeval: Re-evaluating summarization evaluation},
  author={Fabbri, Alexander R and Kry{\'s}ci{\'n}ski, Wojciech and McCann, Bryan and Xiong, Caiming and Socher, Richard and Radev, Dragomir},
  journal={Transactions of the Association for Computational Linguistics},
  volume={9},
  pages={391--409},
  year={2021},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…},
  url="https://direct.mit.edu/tacl/article-abstract/doi/10.1162/tacl_a_00373/100686"
}

@article{phang2022investigating,
  title={Investigating efficiently extending transformers for long input summarization},
  author={Phang, Jason and Zhao, Yao and Liu, Peter J},
  journal={arXiv preprint arXiv:2208.04347},
  year={2022},
  url="https://arxiv.org/abs/2208.04347"
}

@article{du2023improving,
  author = {Du, Jiangsu and Jiang, Jiazhi and Zheng, Jiang and Zhang, Hongbin and Huang, Dan and Lu, Yutong},
  title = {Improving Computation and Memory Efficiency for Real-world Transformer Inference on GPUs},
  year = {2023},
  issue_date = {December 2023},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  volume = {20},
  number = {4},
  issn = {1544-3566},
  url = {https://doi.org/10.1145/3617689},
  doi = {10.1145/3617689},
  abstract = {Transformer models have emerged as a leading approach in the field of natural language processing (NLP) and are increasingly being deployed in production environments. Graphic processing units (GPUs) have become a popular choice for the transformer deployment and often rely on the batch processing technique to ensure high hardware performance. Nonetheless, the current practice for transformer inference encounters computational and memory redundancy due to the heavy-tailed distribution of sequence lengths in NLP scenarios, resulting in low practical performance. In this article, we propose a unified solution for improving both computation and memory efficiency of the real-world transformer inference on GPUs. The solution eliminates the redundant computation and memory footprint across a transformer model. At first, a GPU-oriented computation approach is proposed to process the self-attention module in a fine-grained manner, eliminating its redundant computation. Next, the multi-layer perceptron module continues to use the word-accumulation approach to eliminate its redundant computation. Then, to better unify the fine-grained approach and the word-accumulation approach, it organizes the data layout of the self-attention module in block granularity. Since aforementioned approaches make the required memory size largely reduce and constantly fluctuate, we propose the chunk-based approach to enable a better balance between memory footprint and allocation/free efficiency. Our experimental results show that our unified solution achieves a decrease of average latency by 28\% on the entire transformer model, 63.8\% on the self-attention module, and reduces memory footprint of intermediate results by 7.8\texttimes{}, compared with prevailing frameworks.},
  journal = {ACM Trans. Archit. Code Optim.},
  month = {oct},
  articleno = {46},
  numpages = {22},
  keywords = {Parallel computing, graphics processing unit, natural language processing, transformer inference}
}

@article{blei2003latent,
author = {Blei, David M. and Ng, Andrew Y. and Jordan, Michael I.},
title = {Latent dirichlet allocation},
year = {2003},
issue_date = {3/1/2003},
publisher = {JMLR.org},
volume = {3},
number = {null},
issn = {1532-4435},
abstract = {We describe latent Dirichlet allocation (LDA), a generative probabilistic model for collections of discrete data such as text corpora. LDA is a three-level hierarchical Bayesian model, in which each item of a collection is modeled as a finite mixture over an underlying set of topics. Each topic is, in turn, modeled as an infinite mixture over an underlying set of topic probabilities. In the context of text modeling, the topic probabilities provide an explicit representation of a document. We present efficient approximate inference techniques based on variational methods and an EM algorithm for empirical Bayes parameter estimation. We report results in document modeling, text classification, and collaborative filtering, comparing to a mixture of unigrams model and the probabilistic LSI model.},
journal = {J. Mach. Learn. Res.},
month = {mar},
pages = {993–1022},
numpages = {30},
url = "https://dl.acm.org/doi/10.5555/944919.944937"
}

@article{chen2023extending,
  title={Extending context window of large language models via positional interpolation},
  author={Chen, Shouyuan and Wong, Sherman and Chen, Liangjian and Tian, Yuandong},
  journal={arXiv preprint arXiv:2306.15595},
  year={2023},
  url="https://arxiv.org/abs/2306.15595"
}

@article{zaheer2020big,
  title={Big bird: Transformers for longer sequences},
  author={Zaheer, Manzil and Guruganesh, Guru and Dubey, Kumar Avinava and Ainslie, Joshua and Alberti, Chris and Ontanon, Santiago and Pham, Philip and Ravula, Anirudh and Wang, Qifan and Yang, Li and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={17283--17297},
  year={2020},
  url="https://proceedings.neurips.cc/paper/2020/hash/c8512d142a2d849725f31a9a7a361ab9-Abstract.html"
}


% Datasets

%% GovReport
@inproceedings{huang-etal-2021-efficient,
  title = "Efficient Attentions for Long Document Summarization",
  author = "Huang, Luyang  and
    Cao, Shuyang  and
    Parulian, Nikolaus  and
    Ji, Heng  and
    Wang, Lu",
  booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
  month = jun,
  year = "2021",
  address = "Online",
  publisher = "Association for Computational Linguistics",
  url = "https://aclanthology.org/2021.naacl-main.112",
  doi = "10.18653/v1/2021.naacl-main.112",
  pages = "1419--1436",
  abstract = "The quadratic computational and memory complexities of large Transformers have limited their scalability for long document summarization. In this paper, we propose Hepos, a novel efficient encoder-decoder attention with head-wise positional strides to effectively pinpoint salient information from the source. We further conduct a systematic study of existing efficient self-attentions. Combined with Hepos, we are able to process ten times more tokens than existing models that use full attentions. For evaluation, we present a new dataset, GovReport, with significantly longer documents and summaries. Results show that our models produce significantly higher ROUGE scores than competitive comparisons, including new state-of-the-art results on PubMed. Human evaluation also shows that our models generate more informative summaries with fewer unfaithful errors.",
}

@inproceedings{sharma-etal-2019-bigpatent,
  title = "{BIGPATENT}: A Large-Scale Dataset for Abstractive and Coherent Summarization",
  author = "Sharma, Eva  and
    Li, Chen  and
    Wang, Lu",
  editor = "Korhonen, Anna  and
    Traum, David  and
    M{\`a}rquez, Llu{\'\i}s",
  booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
  month = jul,
  year = "2019",
  address = "Florence, Italy",
  publisher = "Association for Computational Linguistics",
  url = "https://aclanthology.org/P19-1212",
  doi = "10.18653/v1/P19-1212",
  pages = "2204--2213",
  abstract = "Most existing text summarization datasets are compiled from the news domain, where summaries have a flattened discourse structure. In such datasets, summary-worthy content often appears in the beginning of input articles. Moreover, large segments from input articles are present verbatim in their respective summaries. These issues impede the learning and evaluation of systems that can understand an article{'}s global content structure as well as produce abstractive summaries with high compression ratio. In this work, we present a novel dataset, BIGPATENT, consisting of 1.3 million records of U.S. patent documents along with human written abstractive summaries. Compared to existing summarization datasets, BIGPATENT has the following properties: i) summaries contain a richer discourse structure with more recurring entities, ii) salient content is evenly distributed in the input, and iii) lesser and shorter extractive fragments are present in the summaries. Finally, we train and evaluate baselines and popular learning models on BIGPATENT to shed light on new challenges and motivate future directions for summarization research.",
}

@article{kryscinski2021booksum,
  title={Booksum: A collection of datasets for long-form narrative summarization},
  author={Kry{\'s}ci{\'n}ski, Wojciech and Rajani, Nazneen and Agarwal, Divyansh and Xiong, Caiming and Radev, Dragomir},
  journal={arXiv preprint arXiv:2105.08209},
  year={2021},
  url="https://arxiv.org/abs/2105.08209"
}

%% CNN/Daily Mail
@article{nallapati2016abstractive,
  title={Abstractive text summarization using sequence-to-sequence rnns and beyond},
  author={Nallapati, Ramesh and Zhou, Bowen and Gulcehre, Caglar and Xiang, Bing and others},
  journal={arXiv preprint arXiv:1602.06023},
  year={2016},
  url="https://arxiv.org/abs/1602.06023"
}


% Metrics

%% ROUGE
@inproceedings{lin-2004-rouge,
  title = "{ROUGE}: A Package for Automatic Evaluation of Summaries",
  author = "Lin, Chin-Yew",
  booktitle = "Text Summarization Branches Out",
  month = jul,
  year = "2004",
  address = "Barcelona, Spain",
  publisher = "Association for Computational Linguistics",
  url = "https://aclanthology.org/W04-1013",
  pages = "74--81",
}

%% BERTScore
@article{zhang2019bertscore,
  title={Bertscore: Evaluating text generation with bert},
  author={Zhang, Tianyi and Kishore, Varsha and Wu, Felix and Weinberger, Kilian Q and Artzi, Yoav},
  journal={arXiv preprint arXiv:1904.09675},
  year={2019},
  url="https://arxiv.org/abs/1904.09675"
}

%% BLEU
@inproceedings{papineni-etal-2002-bleu,
  title = "{B}leu: a Method for Automatic Evaluation of Machine Translation",
  author = "Papineni, Kishore  and
    Roukos, Salim  and
    Ward, Todd  and
    Zhu, Wei-Jing",
  editor = "Isabelle, Pierre  and
    Charniak, Eugene  and
    Lin, Dekang",
  booktitle = "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics",
  month = jul,
  year = "2002",
  address = "Philadelphia, Pennsylvania, USA",
  publisher = "Association for Computational Linguistics",
  url = "https://aclanthology.org/P02-1040",
  doi = "10.3115/1073083.1073135",
  pages = "311--318",
}

%% METEOR
@inproceedings{banerjee-lavie-2005-meteor,
  title = "{METEOR}: An Automatic Metric for {MT} Evaluation with Improved Correlation with Human Judgments",
  author = "Banerjee, Satanjeev  and
    Lavie, Alon",
  editor = "Goldstein, Jade  and
    Lavie, Alon  and
    Lin, Chin-Yew  and
    Voss, Clare",
  booktitle = "Proceedings of the {ACL} Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization",
  month = jun,
  year = "2005",
  address = "Ann Arbor, Michigan",
  publisher = "Association for Computational Linguistics",
  url = "https://aclanthology.org/W05-0909",
  pages = "65--72",
}
