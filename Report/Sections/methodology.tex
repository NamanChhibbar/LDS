\section{Proposed Methods}
% \section{Methodology}


\subsection{Truncation}
\label{method:truncation}

Truncation is the most common and straightforward approach used to handle long texts
that exceed the context size of an LLM.
It is done in three main ways:

\begin{itemize}
	\item \textbf{Retaining Head}: Keeping tokens from the start.
	\item \textbf{Retaining Tail}: Keeping tokens from the end.
	\item \textbf{Head and Tail}: Keeping tokens from both start and end.
\end{itemize}

\citet{worsham-kalita-2018-genre} also employ "retaining head" and "retaining tail"
strategies on long texts and find promising results.
Though the "retaining head" method is often used, keeping the initial tokens allowed
by the LLM, \citet{sun2019fine} have found that keeping both head and tail produces
better results than both the "retaining head" and the "retaining tail" methods.
Their research also shows that truncating the middle is even better than the more
complicated hierarchical methods, displaying superiority with simplicity.


\subsection{Document Skimming}
\label{method:skimming}

One way to process long texts is by employing a speed reading strategy known as
"skimming".
Skimming is done by reading the whole text in a go while selectively skipping some
parts of the text for quicker reading.
The reader usually omits the portions that seem redundant or irrelevant in the text,
minimizing information loss.

Since we do not know which parts of a text are relevant, we suggest uniformly sampling
sentences or segments of the text such that they fit in the context size of the LLM.
This approach ensures we sample a segment from each part of the text.
This approch is a modification to the methodology \citet{wang2024videoagent} use for
QA on long videos. \citet{worsham-kalita-2018-genre} also use random sampling for genre
identification.


\subsection{Document Skimming with Extraction}

In addition to skimming, we can use keywords from an extractive summarization
algorithm to obtain similarity scores for keywords and text segments.
We can then create a probability distribution using the scores from which we can sample
text segments to include.
This will help us choose segemnts intelligently instead of randomly sampling them.

Some extractive summarization algorithms that can be used are:

\begin{itemize}
	\item \textbf{TextRank}: An unsupervised graph-based ranking algorithm developed for
	keyword and sentence extraction. It is introduced by
	\citet{mihalcea-tarau-2004-textrank}.
	\item \textbf{LexRank}: An algorithm similar to TextRank, but uses cosine similarity
	for extraction. It is developed by \citet{erkan2004lexrank}.
	\item \textbf{PacSum}: An algorithm also similar to TextRank, but also uses
	positional information for extraction. It is developed by
	\citet{zheng-lapata-2019-sentence}.
	\item \textbf{ATS using Luhn's Heuristic}: An algorithm introduced by
	\citet{10188527} that extracts sentences based on term frequencies.
	\item \textbf{SummaRuNNer}: An RNN based sequence model for extractive summarization.
	Developed by \citet{Nallapati_Zhai_Zhou_2017}.
\end{itemize}

This approach is similar to the way \citet{10.1145/3639233.3639253} use action-item
pairs to summarize.


\subsection{Summarization using Convolutions}

Another way to approach the problem is to apply convolution to encoded segments of
the document.
The convolution operation multiplies a sliding window of weights element-wise by the
embeddings and then sums the result.
This allows the model to learn cross-segment relationships and acts as a
limited-range attention mechanism, an algorithm similar to \citet{chen2022long}.

This method begins with separating sentences from the document and grouping them
based on a fixed maximum number of sentences per group.
These groups are then encoded and passed through a 1D convolution layer of size $k$ with
$f$ filters, where $k$ and $f$ are hyperparameters in the model.
A max-pooling layer can also be applied for feature enhancement.
A decoder-only transformer then uses these processed embeddings as keys and values
for the encoder-decoder attention mechanism to generate the summary.

Longformer model \cite{beltagy2020longformer} uses a windowed attention mechanism with
linear complexity to process long texts.
Our 1D convolution operation outperforms the windowed attention mechanism since it has
linear complexity and is more efficient to calculate than attention.


\section{Implemented Pipelines}


\subsection*{Pipeline 1}

This pipeline is based on the Truncation method proposed in Section \ref{method:truncation}
and truncates the middle of the text.
It includes a hyperparameter $\mathrm{head\_size} \in [0, 1]$ which determines the fraction
of tokens to be taken from the head.
Setting $\mathrm{head\_size} = 1$ results in taking tokens only from the head, whereas
setting $\mathrm{head\_size} = 0$ results in taking tokens only from the tail.


\subsection*{Pipeline 2}

This pipeline is based on the Document Skimming method proposed in Section \ref{method:skimming}
and uniformly samples segments of the text.
This method starts by segmenting the text using a segmenter.
It inlcudes a hyperparameter $\mathrm{min\_words}$, which controls the minimum number of
words required in a segment.
The segmenter uses a sentence tokenizer to separate sentences from the text, and then
merges the sentences which have fewer words than $\mathrm{min\_words}$.
We then choose segments uniformly, with each segment having probability $p$ to be picked.
The calculation for the optimal value is given below.

Let $X$ be denote the total number of tokens in the picked segments.
Since segments are picked randomly, $X$ is a random variable.
If the context size of the model is $c$, we want $\mathrm{E}[X] = c$, where $\mathrm{E}[X]$
denotes the expectation of $X$.

Suppose we have $n \in \mathbb{N}$ segments and $X_i$ denotes if segment $i$ is chosen,
$i \in \{1, 2, \dots, n\}$.
Therefore, $X_i \sim \mathrm{Bernoulli}(p), i \in \{1, 2, \dots, n\}$ are independent and
identically distributed (IID) random variables.

If $\mathrm{len}_i$ denotes the number of tokens in segment $i$, we can write:

\[ X = \sum_{i = 1}^{n} X_i \cdot \mathrm{len}_i \]
\[ \Rightarrow \mathrm{E}[X] = \mathrm{E}[\sum_{i = 1}^{n} X_i \cdot \mathrm{len}_i] \]
\[ \Rightarrow \mathrm{E}[X] = \sum_{i = 1}^{n} \mathrm{E}[X_i \cdot \mathrm{len}_i] \]
\[ \Rightarrow \mathrm{E}[X] = \sum_{i = 1}^{n} \mathrm{E}[X_i] \cdot \mathrm{len}_i \]

Since $X_i \sim \mathrm{Bernoulli}(p), i \in \{1, 2, \dots, n\}$, we have
$\mathrm{E}[X_i] = p, i \in \{1, 2, \dots, n\}$.

\[ \Rightarrow \mathrm{E}[X] = \sum_{i = 1}^{n} p \cdot \mathrm{len}_i \]
\[ \Rightarrow \mathrm{E}[X] = p \cdot \sum_{i = 1}^{n} \mathrm{len}_i \]

Let $\mathrm{total\_len}$ be the total number of tokens in the text, then
$\mathrm{total\_len} = \sum_{i = 1}^{n} \mathrm{len}_i$.

\[ \therefore \mathrm{E}[X] = p \cdot \mathrm{total\_len} \]
\[ \mathrm{E}[X] = c \Rightarrow p \cdot \mathrm{total\_len} = c \]
\[ \therefore p = c / \mathrm{total\_len} \]


\subsection*{Pipeline 3}
