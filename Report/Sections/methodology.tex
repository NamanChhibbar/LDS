\section{Methodology}
\label{sec:methodology}

We discuss the methodology used in our experiments in this section.
Most of the methods start by segmenting the document into smaller, contiguous, and
exhaustive parts.
The segmentation algorithm is controlled by a threshold on the minimum number of words
in a segment, which is a hyperparameter in each of the respective methods.


\subsection{Central Truncation}
\label{method:truncation}

Truncation is the most common and straightforward approach used to handle long texts
that exceed the context size of an LLM.
It is done in three main ways:

\begin{itemize}
	\item \textbf{Retaining Head}: Keeping tokens from the start.
	\item \textbf{Retaining Tail}: Keeping tokens from the end.
	\item \textbf{Head and Tail}: Keeping tokens from both start and end.
\end{itemize}

\citet{worsham-kalita-2018-genre} also employ "retaining head" and "retaining tail"
strategies on long texts and find promising results.
Though the "retaining head" method is often used, keeping the initial tokens allowed
by the LLM, \citet{sun2019fine} have found that keeping both head and tail produces
better results than both the "retaining head" and the "retaining tail" methods.
Their research also shows that truncating the middle is even better than the more
complicated hierarchical methods, displaying superiority with simplicity.

The fraction of tokens to be taken from the head is controlled by the hyperparameter
$head\_size \in [0, 1]$.
Setting $head\_size = 1$ results in taking tokens only from the head, whereas
setting $head\_size = 0$ results in taking tokens only from the tail.
The truncated text is then sent to the summarizer.


\subsection{Document Skimming}
\label{method:skimming}

One way to process long texts is by employing a speed reading strategy known as "skimming".
Skimming is done by reading the whole text in a go while selectively skipping some
parts of the text for quicker reading.
The reader usually omits the portions that seem redundant or irrelevant in the text,
minimizing information loss.

This method starts by segmenting the text using a segmenter with the hyperparameter $min\_words$.
The segmenter uses a sentence tokenizer to separate sentences from the text, and then
merges the sentences which have fewer words than $min\_words$.
We then sample segments uniformly, with each segment having probability $p$ to be picked.
The sampled segments are then merged to form a single text and sent to the summarizer.
This ensures we sample a segment from each part of the text.

To address the removal of redundancy in the document, we experiment with and without removing
redundant segments before and after sampling.
This is done by maintaining the average of segment embeddings.
\href{https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2}{this} sentence transformer
is used to generate the segment embeddings.
If the cosine similarity between the average of segment embeddings and the current segment
embedding is greater than a threshold, the segment is removed.
Though removing redundant segments before sampling is less efficient due to the whole document
being processed, it ensures better utilization of the LLM's context size.
The mean embedding is initialised as a zero vector and updated after each segment is processed.
Doing so introduces a hyperparameter $threshold$ to control the similarity threshold between the
mean embedding and the current segment embedding.

This approch is a modification to the methodology \citet{wang2024videoagent} use for
question-answering on long videos. \citet{worsham-kalita-2018-genre} also use random sampling
for genre identification.

We will now discuss the calculation of the optimal value of $p$ here.
Let $X$ denote the total number of tokens in the sampled segments.
Since segments are sampled randomly, $X$ is a random variable.
If the context size of the model is $model\_size$, we want $\mathrm{E}[X] = model\_size$,
where $\mathrm{E}[X]$ denotes the expectation of $X$.

Suppose we have $n \in \mathbb{N}$ segments and $X_i \sim \mathrm{Bernoulli}(p)$ denotes
if segment $i$ is chosen, $i \in \{1, 2, \dots, n\}$.
If $len_i$ denotes the number of tokens in segment $i$, we can write:

\[ X = \sum_{i = 1}^{n} X_i \cdot len_i \]
\[ \Rightarrow \mathrm{E}[X] = \mathrm{E}[\sum_{i = 1}^{n} X_i \cdot len_i] \]
\[ \Rightarrow \mathrm{E}[X] = \sum_{i = 1}^{n} \mathrm{E}[X_i \cdot len_i] \]
\[ \Rightarrow \mathrm{E}[X] = \sum_{i = 1}^{n} \mathrm{E}[X_i] \cdot len_i \]

Since $X_i \sim \mathrm{Bernoulli}(p)$ $\forall i \in \{1, 2, \dots, n\}$, we
have $\mathrm{E}[X_i] = p$ $\forall i \in \{1, 2, \dots, n\}$.

\[ \Rightarrow \mathrm{E}[X] = \sum_{i = 1}^{n} p \cdot len_i \]
\[ \Rightarrow \mathrm{E}[X] = p \cdot \sum_{i = 1}^{n} len_i \]

Let $total\_len$ be the total number of tokens in the text, then
$total\_len = \sum_{i = 1}^{n} len_i$.

\[ \therefore \mathrm{E}[X] = p \cdot total\_len = model\_size \]
\[ \Rightarrow p \cdot total\_len = model\_size \]
\[ \therefore p = model\_size / total\_len \]


\subsection{Document Skimming with Keyword Extraction}

In addition to skimming, we can use keywords from an extractive algorithm to obtain
similarity scores for keywords and text segments.
We can then create a probability distribution using the scores from which we can sample
text segments to include.
This will help us choose segemnts intelligently instead of randomly sampling them.

Some extractive summarization algorithms that can be used are:

This approach is similar to the way \citet{golia2024action} use action-item
pairs to summarize.


% \subsection{Summarization using Convolutions}

% Another way to approach the problem is to apply convolution to encoded segments of
% the document.
% The convolution operation multiplies a sliding window of weights element-wise by the
% embeddings and then sums the result.
% This allows the model to learn cross-segment relationships and acts as a
% limited-range attention mechanism, an algorithm similar to \citet{chen2022long}.

% This method begins with separating sentences from the document and grouping them
% based on a fixed maximum number of sentences per group.
% These groups are then encoded and passed through a 1D convolution layer of size $k$ with
% $f$ filters, where $k$ and $f$ are hyperparameters in the model.
% A max-pooling layer can also be applied for feature enhancement.
% A decoder-only transformer then uses these processed embeddings as keys and values
% for the encoder-decoder attention mechanism to generate the summary.

% Longformer model \cite{beltagy2020longformer} uses a windowed attention mechanism with
% linear complexity to process long texts.
% Our 1D convolution operation outperforms the windowed attention mechanism since it has
% linear complexity and is more efficient to calculate than attention.
