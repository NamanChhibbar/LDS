\section{Methodology}
	\label{sec:methodology}

	In this section, we discuss the algorithms used in our experiments.
	Most of our algorithms start by segmenting the document into smaller, contiguous, and exhaustive
	parts.
	We do so by using a sentence tokenizer to separate sentences from the text and merging them such that
	have fewer words than the threshold $min\_words$, which is a hyperparameter is each of the respective
	methods.


	\subsection{Central Truncation}
		\label{method:truncation}

		Truncation is the most common and straightforward approach used to handle long texts
		that exceed the context size of an LLM.
		It is done in three main ways:

		\begin{itemize}
			\item \textbf{Retaining Head}: Keeping tokens from the start.
			\item \textbf{Retaining Tail}: Keeping tokens from the end.
			\item \textbf{Head and Tail}: Keeping tokens from both start and end.
		\end{itemize}

		\citet{worsham-kalita-2018-genre} also employ "retaining head" and "retaining tail"
		strategies on long texts and find promising results.
		Though the "retaining head" method is often used, keeping the initial tokens allowed by
		the LLM, \citet{sun2019fine} find that keeping both head and tail produces better results
		than both the "retaining head" and the "retaining tail" methods.
		Their research also shows that truncating the middle is even better than the more
		complicated hierarchical methods, displaying superiority with simplicity.

		The fraction of tokens to be taken from the head is controlled by the hyperparameter $head\_size
		\in [0, 1]$.
		Setting $head\_size = 1$ results in taking tokens only from the head, whereas setting $head\_size
		= 0$ results in taking tokens only from the tail.
		The truncated text is then sent to the model.


	\subsection{Document Skimming}
		\label{method:skimming}

		One way to process long texts is by employing a speed reading strategy called "skimming".
		Skimming is done by reading the whole text in a go while selectively skipping some parts of the
		text for quicker reading.
		The reader usually omits the portions that seem redundant or irrelevant in the text, minimizing
		information loss.
		This method is inspired by the way \cite{wang2024videoagent} randomly sample video frames to
		generate captions.
		\citet{worsham-kalita-2018-genre} also use random sampling for genre identification.

		This method starts by segmenting the document with the hyperparameter $min\_words$.
		We then sample segments uniformly, with each segment having probability $p$ to be picked.
		The sampled segments are then merged to form a single text and sent to the model.
		This ensures we sample a segment from each part of the text.

		To address the removal of redundancy in the document, we experiment with and without removing
		redundant segments before and after sampling.
		This is done by computing the cosine similarity between the mean segment embeddings and the
		current segment embedding.
		\href{https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2}{This} sentence transformer
		is used to generate the segment embeddings.
		The sentence transformer is based on MiniLM \cite{wang2020minilm}, which is a distilled version
		of larger encoder-only transformer models.
		If the cosine similarity is greater than a threshold, the current segment is removed.
		The mean embedding is updated if the current segment is retained.

		When removing segments after sampling, we increase the probability of choosing a segment during
		sampling to compensate for the removed segments.
		This fraction is controlled by the hyperparameter $prob\_boost$.
		The updated probability is calculated as $p_{new} = (1 + prob\_boost) \cdot p$.
		Even though removing redundant segments before sampling is less efficient due to the whole document
		being processed, it ensures better utilization of the LLM's context size.
		The mean embedding is initialised as a zero vector and updated after each segment is processed.
		Doing so introduces a hyperparameter $threshold$ to control the similarity threshold between the
		mean embedding and the current segment embedding.

		We will now discuss the calculation of the optimal value of $p$ here.
		Let $X$ denote the total number of tokens in the sampled segments.
		Since segments are sampled randomly, $X$ is a random variable.
		If the context size of the model is $model\_size$, we want $\mathrm{E}[X] = model\_size$,
		where $\mathrm{E}[X]$ denotes the expectation of $X$.

		Suppose we have $n \in \mathbb{N}$ segments and $X_i \sim \mathrm{Bernoulli}(p)$ denotes
		if segment $i$ is chosen, $i \in \{1, 2, \dots, n\}$.
		If $len_i$ denotes the number of tokens in segment $i$, we can write:

		\[ X = \sum_{i = 1}^{n} X_i \cdot len_i \]
		\[ \Rightarrow \mathrm{E}[X] = \mathrm{E}[\sum_{i = 1}^{n} X_i \cdot len_i] \]
		\[ \Rightarrow \mathrm{E}[X] = \sum_{i = 1}^{n} \mathrm{E}[X_i \cdot len_i] \]
		\[ \Rightarrow \mathrm{E}[X] = \sum_{i = 1}^{n} \mathrm{E}[X_i] \cdot len_i \]

		Since $X_i \sim \mathrm{Bernoulli}(p)$ $\forall i \in \{1, 2, \dots, n\}$, we
		have $\mathrm{E}[X_i] = p$ $\forall i \in \{1, 2, \dots, n\}$.

		\[ \Rightarrow \mathrm{E}[X] = \sum_{i = 1}^{n} p \cdot len_i \]
		\[ \Rightarrow \mathrm{E}[X] = p \cdot \sum_{i = 1}^{n} len_i \]

		Let $total\_len$ be the total number of tokens in the text, then
		$total\_len = \sum_{i = 1}^{n} len_i$.

		\[ \therefore \mathrm{E}[X] = p \cdot total\_len = model\_size \]
		\[ \Rightarrow p \cdot total\_len = model\_size \]
		\[ \therefore p = model\_size / total\_len \]


	\subsection{Summarization with Keyword Extraction}
		\label{method:keyword}

		Document skimming (\ref{method:skimming}) involves a very intuitive and simple approach of
		sampling segments randomly.
		Instead of that, we can use an efficient keyword extraction algorithm to get important keywords
		from the document.
		These keywords help us sample segments intelligently, ensuring we get the most important
		segments from the document.

		We use Latent Dirichlet Allocation (LDA) \cite{blei2003latent} with a single topic to get topic
		words (keywords) from the document.
		These keywords are concatenated using a delimiter (a space is used in our experiments) to form a
		single sentence.
		This sentence is then embedded and compared with other segment embeddings using cosine similarity.
		Maximum number of segments with the highest similarity are retained.
		The keyword sentence and document segments are embedded using the same
		\href{https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2}{sentence transformer}
		used in the previous method.

		This approach is similar to the way \citet{golia2024action} use action items to pick segments
		of text (a neighbourhood of 2 sentences around the action item) for summarization.
