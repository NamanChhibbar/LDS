\section{Experimental Results}

We test our pipelines with the following summarizers: Bidirectional and
Autoregressive Transformer (BART) \cite{lewis-etal-2020-bart} fine-tuned on the
CNN/Daily Mail dataset \cite{nallapati2016abstractive} with a context size of 1024,
LongT5 \cite{guo2021longt5}, a variant of Text-to-Text Transfer Transformer (T5)
\cite{raffel2020exploring}, fine-tuned on the BookSum dataset with a context size
of 4096, GPT-3.5 Turbo \cite{brown2020language} with a context size of 4096.
