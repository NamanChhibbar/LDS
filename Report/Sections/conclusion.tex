\section{Future Work}

We have used a very basic sentence tokenizer
(\href{https://www.nltk.org/api/nltk.tokenize.sent_tokenize.html}{nltk.sent\_tokenize}),
with some modifications to control the minimum number of words in a segment,
to segment the document.
In out experiments, we find that segmentation is a crucial step in the pipeline, and can
influence the output summary significantly.
Ensuring the uniformity of the length of the segments while preserving coherence is
important for better utilization of the context size of the summarizer while sampling.
We encourage future work to experiment with more complex segmenters.


\section{Conclusion}
\label{sec:conclusion}

Our experiments shows a huge jump in BERTScore compared to Unlimiformer on
documents with word counts upto 100,000.
This shows that our pipelines are able utilize details of long text efficiently.
Even though our ROUGE-2 scores are lower than the baselines, ROUGE-1 and ROUGE-L
scores are competitive.
Since BERTScore is a better metric for capturing semantic similarity, we hypothesize
our pipelines are able to generate better summaries than baselines with higher
ROUGE scores.
