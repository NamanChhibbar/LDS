\section{Evaluation Metrics}
	\label{sec:metrics}

	The best way to evaluate generated natural language is by humans, but conducting
	human trials are expensive and time-consuming.
	Hence, we use automatic evaluation metrics to evaluate the quality of the generated
	summary, given reference summaries.
	\citet{fabbri2021summeval} review many such open-source and state-of-the-art metrics.
	The two that we use in our experiments are discussed below.
	These metrics are commonly used in published literature.

	\textbf{ROUGE metrics:} \citet{lin-2004-rouge} introduces the Recall-Oriented Understudy
	for Gisting Evaluation (ROUGE) metrics.
	The basic ROUGE-N metric is based on the fraction of overlaps of ideal or reference summaries
	with the candidate summary, hence being recall-oriented.
	His study concludes that ROUGE-N with $\text{N} = 2$, ROUGE-L, ROUGE-W, and ROUGE-S work
	well for the summarization task.

	\textbf{BERTScore:} \citet{zhang2019bertscore} introduce the BERTScore, an automatic
	evaluation metric for text generation.
	BERTScore is calculated by comparing the contextual embeddings of tokens in the candidate
	and reference summaries, which are generated using BERT \cite{devlin2018bert}.
	BERTScore excels at capturing semantic similarities between sentences since it uses
	contextual embeddings of tokens instead of using N-gram frequencies to calculate similarity.
