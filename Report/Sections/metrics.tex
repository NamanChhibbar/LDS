\section{Evaluation Metrics}

The best way to evaluate generated natural language is by humans.
Conducting human trials to evaluate summaries may give valuable results.
We can also use well-established and open-source metrics for evaluating a candidate
summary, given some ideal summaries.


\subsection{ROUGE metrics}

\citet{lin-2004-rouge} introduces the Recall-Oriented Understudy for Gisting
Evaluation (ROUGE) metrics.
The basic ROUGE-N metric is based on the fraction of overlaps of some ideal summaries
with the candidate summary, hence being recall-oriented.
His study concludes that ROUGE-N with $\text{N}=2$, ROUGE-L, ROUGE-W, and ROUGE-S work
well for the summarization task.


\subsection{BERTScore}

\citet{zhang2019bertscore} introduce the BERTScore, an automatic evaluation metric for
text generation.
BERTScore is calculated by comparing the contextual embeddings of tokens in the candidate
and reference summaries, which are generated using BERT \cite{devlin2018bert}.
BERTScore excels at capturing semantic similarities between sentences since it uses
contextual embeddings of tokens instead of using N-gram frequencies to calculate similarity.


\subsection{BLEU metric}

Bilingual Evaluation Understudy (BLEU), developed by \citet{papineni-etal-2002-bleu},
works by calculating the fraction of overlaps of candidate summary with some ideal
summaries, hence being precision-oriented.
Their study claims that BLEU can be adapted for all NLG tasks!


\subsection{METEOR metric}

Metric for Evaluation of Translation with Explicit Ordering (METEOR), introduced by
\citet{banerjee-lavie-2005-meteor}, is evaluated by a complex algorithm involving
unigram precision and recall with a measure of fragmentation, i.e. how well the matched
words are ordered.
This metric is also well suited for NLG tasks like machine translation and
summarization.
