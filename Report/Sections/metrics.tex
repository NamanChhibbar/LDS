\section{Evaluation Metrics}

The best way to evaluate generated natural language is by humans.
Conducting human trials to evaluate summaries may give valuable results.
We can also use well-established and open-source metrics for evaluating a candidate
summary, given some ideal summaries.


\subsection{ROUGE metrics}

\citet{lin-2004-rouge} introduces the Recall-Oriented Understudy for Gisting
Evaluation (ROUGE) metrics.
The basic ROUGE-N metric is based on the fraction of overlaps of some ideal summaries
with the candidate summary, hence being recall-oriented.
His study concludes that ROUGE-N with $\text{N}=2$, ROUGE-L, ROUGE-W, and ROUGE-S work
well for the summarization task.

\subsection{BLEU metric}

Bilingual Evaluation Understudy (BLEU), developed by \citet{papineni-etal-2002-bleu},
works by calculating the fraction of overlaps of candidate summary with some ideal
summaries, hence being precision-oriented.
Their study claims that BLEU can be adapted for all NLG tasks!


\subsection{METEOR metric}

Metric for Evaluation of Translation with Explicit Ordering (METEOR), introduced in
\citet{banerjee-lavie-2005-meteor}, is evaluated by a complex algorithm involving
unigram precision and recall with a measure of fragmentation, i.e. how well the matched
words are ordered.
This metric is also well suited for NLG tasks like machine translation and
summarization.
