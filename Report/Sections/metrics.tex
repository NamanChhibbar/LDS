\section{Evaluation Metrics}
	\label{sec:metrics}

	The best way to evaluate generated natural language is by humans, but conducting
	human trials is expensive and time-consuming.
	Hence, we use automatic evaluation metrics to evaluate the quality of the generated
	summary, given some reference summaries.
	\citet{fabbri2021summeval} review many such open-source and state-of-the-art metrics.
	Some of them that we use in our experiments are discussed below.

	\textbf{ROUGE metrics:} \citet{lin-2004-rouge} introduces the Recall-Oriented Understudy
	for Gisting Evaluation (ROUGE) metrics.
	The basic ROUGE-N metric is based on the fraction of overlaps of some ideal summaries
	with the candidate summary, hence being recall-oriented.
	His study concludes that ROUGE-N with $\text{N}=2$, ROUGE-L, ROUGE-W, and ROUGE-S work
	well for the summarization task.

	\textbf{BERTScore:} \citet{zhang2019bertscore} introduce the BERTScore, an automatic
	evaluation metric for text generation.
	BERTScore is calculated by comparing the contextual embeddings of tokens in the candidate
	and reference summaries, which are generated using BERT \cite{devlin2018bert}.
	BERTScore excels at capturing semantic similarities between sentences since it uses
	contextual embeddings of tokens instead of using N-gram frequencies to calculate similarity.
