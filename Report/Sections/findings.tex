\section{Experimental Findings}

We test our pipelines with the following summarizers: \textbf{BART} (Bidirectional
and Autoregressive Transformer) \cite{lewis-etal-2020-bart} fine-tuned on the
CNN/Daily Mail dataset \cite{nallapati2016abstractive} with a context size of 1024,
\textbf{LongT5} \cite{guo2021longt5}, a variant of T5 (Text-to-Text Transfer
Transformer) \cite{raffel2020exploring}, fine-tuned on the BookSum dataset with
a context size of 4096, and \textbf{GPT-3.5 Turbo} \cite{brown2020language} with a
context size of 4096.

We compare our results with the state-of-the-art summarization models, including
Unlimiformer \cite{bertsch2023unlimiformer}, Hepos \cite{huang-etal-2021-efficient},
staggered block-local attention \cite{phang2022investigating}.
Refer to Table \ref{tab:results} for baseline results and our results.

\begin{table*}[!ht]
	\centering

	\begin{tabular}{c c c c c}
		\hline
		\textbf{Model} & \textbf{ROUGE-1} & \textbf{ROUGE-2} & \textbf{ROUGE-L} &
		\textbf{BERTScore} \\
		\hline
		BART + Unlimiformer & 53.4 & 22.5 & 22.5 & 66.0 \\
		PRIMERA + Unlimiformer & 56.5 & 24.8 & 26.3 & 67.7 \\
		Hepos & 51.34 & 19.09 & \textbf{48.73} & - \\
		PEGASUS-X \textbackslash w & \textbf{60.3} & 30.0 & 31.5 & - \\
		Staggered Block-Local Attention & & & & \\
		\hline
		Pipeline 3 \textbackslash w GPT-3.5 Turbo & 58.71 & 13.15 & 35.04 & \textbf{85.37} \\
		Pipeline 1 \textbackslash w LongT5 & 46.20 & 4.38 & 38.27 & 82.19 \\
		Pipeline 3 \textbackslash w LongT5 & 46.76 & 4.56 & 39.61 & 81.96 \\
		\hline
	\end{tabular}

	\caption{Automatic evaluation results on GovReport dataset}
	\label{tab:results}
\end{table*}
