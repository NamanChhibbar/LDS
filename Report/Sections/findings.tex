\section{Experimental Findings}
\label{sec:findings}

We test our pipelines with the following summarizers: \textbf{BART} (Bidirectional
and Autoregressive Transformer) \cite{lewis-etal-2020-bart} fine-tuned on the
CNN/Daily Mail dataset \cite{nallapati2016abstractive} with a context size of 1024,
\textbf{LongT5} \cite{guo2021longt5}, a variant of T5 (Text-to-Text Transfer
Transformer) \cite{raffel2020exploring}, fine-tuned on the BookSum dataset with
a context size of 4096, and \textbf{GPT-3.5 Turbo} \cite{brown2020language} with a
context size of 4096.

We compare our results with the state-of-the-art summarization models, including
Unlimiformer \cite{bertsch2023unlimiformer}, Hepos \cite{huang-etal-2021-efficient},
and PEGASUS-X with staggered block-local attention \cite{phang2022investigating}.
Refer to Table \ref{tab:results} for baseline results and our results.

\begin{table*}[!ht]
	\centering

	\begin{tabular}{c c c c c}
		\hline
		\textbf{Model} & \textbf{ROUGE-1} & \textbf{ROUGE-2} & \textbf{ROUGE-L} &
		\textbf{BERTScore} \\
		\hline
		BART w/ Unlimiformer & 53.4 & 22.5 & 22.5 & 66.0 \\
		PRIMERA w/ Unlimiformer & 56.5 & 24.8 & 26.3 & 67.7 \\
		Hepos & 51.34 & 19.09 & \textbf{48.73} & - \\
		PEGASUS-X w/ Staggered & 60.3 & \textbf{30.0} & 31.5 & - \\
		Block-Local Attention & & & & \\
		\hline
		Skimming w/ Extraction & \textbf{61.99} & 18.52 & 38.46 & \textbf{86.20} \\
		+ GPT-3.5 Turbo & & & & \\
		Pipeline 1 w/ LongT5 & 46.20 & 4.38 & 38.27 & 82.19 \\
		Pipeline 3 w/ LongT5 & 46.76 & 4.56 & 39.61 & 81.96 \\
		\hline
	\end{tabular}

	\caption{Automatic evaluation results on GovReport dataset. The best scores are
	highlighted in \textbf{bold}.}
	\label{tab:results}
\end{table*}
