\section{Introduction}
	\label{sec:introduction}

	Due to ever increasing amount of textual data available online, document summarization
	has become crucial for efficient and accurate extraction of relevant information from a
	piece of text.
	Over the past few years, Large Language Models (LLMs) based on the Transformer model
	\cite{vaswani2017attention} have shown ground-breaking abilities for several NLP tasks,
	including document summarization \cite{yadav2023state}.
	Recent developments have demonstrated remarkable improvements in the relevancy and
	coherence of summaries generated by such LLMs.

	Long document summarization has become a major challenge since it involves removing
	redundancies, making reading vast texts concise and efficient.
	One of the major limitations in the transformer architecture is limited context size,
	stemming from the quadratic memory and computational complexity of the attention mechanism
	\cite{du2023improving}.
	This constraint hinders the extraction of relevant information from extensive texts where
	summarization is valuable to overcome the interpretative barriers posed by complex relations
	on such a large scale.

	We experiment with various novel approaches to address the input size limitations of transformers.
	The methods introduced do not include any architectural modifications to the model used for
	summarization and can be incorporated into any existing summarization pipeline.
	We believe that these methods can effectively utilize the full potential of any existing LLM.
	Though our experiments only include the task of summarization, our methods can be applied
	to almost any NLP task (sequence classification, question-answering, NLI, etc.) that requires
	processing long texts.

	We start by stating the problem statement (\ref{sec:problem}) and discussing some related works
	(\ref{sec:related-works}) to gain insights into the problem and the state-of-the-art solutions.
	We then introduce the datasets (\ref{sec:datasets}) and methodology (\ref{sec:methodology}) used
	in our experiments.
	For evaluating our results, we present some common metrics (\ref{sec:metrics}) used in text
	summarization.
	We end the report by discussing our experimental findings (\ref{sec:findings}) and potential
	future work (\ref{sec:future-work}), and concluding the study (\ref{sec:conclusion}).
