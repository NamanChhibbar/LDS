\section{Introduction}
	\label{sec:introduction}

	Due to the ever-increasing amount of textual data available online, document summarization
	has become crucial for efficient and accurate extraction of relevant information.
	Over the past few years, Large Language Models (LLMs) based on the transformer architecture
	\cite{vaswani2017attention} have shown ground-breaking abilities for NLP tasks,
	including document summarization \cite{yadav2023state}.
	Recent developments have demonstrated remarkable improvements in the relevancy and
	coherence of summaries generated by such LLMs.

	However, long document summarization, which makes reading, interpreting, and extracting information
	from vast texts accurate and efficient, remains a major challenge.
	One of the major limitations in the transformer architecture is limited context size,
	stemming from the quadratic memory and computational complexity of the attention mechanism
	\cite{du2023improving}.
	This limitation hampers the extraction of relevant information from lengthy texts, where
	summarization is essential to overcome the time, effort, and interpretive challenges posed by
	the complexity of such documents.

	We experiment with three novel approaches to address the input size limitations of transformers.
	The methods introduced do not include any architectural modifications and can be incorporated
	into any existing pipeline.
	We believe that these methods can effectively utilize the full potential of any existing LLM.
	Though our experiments only include the task of summarization, our methods can be applied
	to any NLP task (such as sequence classification, question-answering, and NLI) which requires
	processing long texts.

	We start by stating the problem statement (\autoref{sec:problem}) and discussing related works
	(\autoref{sec:related-works}) to gain insights into the problem and the state-of-the-art solutions.
	We then introduce the datasets (\autoref{sec:datasets}) and methodology (\autoref{sec:methodology})
	used in our experiments.
	For evaluating our results, we present some common metrics (\autoref{sec:metrics}) used in text
	summarization.
	We end the report by discussing our experimental findings (\autoref{sec:findings}) and potential
	future work (\autoref{sec:future-work}), and concluding the study (\autoref{sec:conclusion}).
