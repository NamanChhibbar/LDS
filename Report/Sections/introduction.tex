\section{Introduction}
	\label{sec:introduction}

	Due to ever increasing amount of textual data available online, document summarization
	has become crucial for efficient and accurate extraction of relevant information from a
	piece of text.
	Over the past few years, Large Language Models (LLMs) based on the transformer architecture
	\cite{vaswani2017attention} have shown ground-breaking abilities for NLP tasks,
	including document summarization \cite{yadav2023state}.
	Recent developments have demonstrated remarkable improvements in the relevancy and
	coherence of summaries generated by such LLMs.

	However, long document summarization, which involves removing redundancies and makes reading
	vast texts concise and efficient, remains a major challenge.
	One of the major limitations in the transformer architecture is limited context size,
	stemming from the quadratic memory and computational complexity of the attention mechanism
	\cite{du2023improving}.
	This limitation hampers the extraction of relevant information from lengthy texts, where
	summarization is essential to overcome the time, effort, and interpretive challenges posed by
	the complexity of large documents.

	We experiment with three novel approaches to address the input size limitations of transformers.
	The methods introduced do not include any architectural modifications and can be incorporated
	into any existing pipeline.
	We believe that these methods can effectively utilize the full potential of any existing LLM.
	Though our experiments only include the task of summarization, our methods can be applied
	to any NLP task (such as sequence classification, question-answering, and NLI) which requires
	processing long texts.

	We start by stating the problem statement (\ref{sec:problem}) and discussing some related works
	(\ref{sec:related-works}) to gain insights into the problem and the state-of-the-art solutions.
	We then introduce the datasets (\ref{sec:datasets}) and methodology (\ref{sec:methodology}) used
	in our experiments.
	For evaluating our results, we present some common metrics (\ref{sec:metrics}) used in text
	summarization.
	We end the report by discussing our experimental findings (\ref{sec:findings}) and potential
	future work (\ref{sec:future-work}), and concluding the study (\ref{sec:conclusion}).
