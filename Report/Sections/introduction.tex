\section{Introduction}
\label{sec:introduction}

Due to the abundance of online text data, document summarization has become crucial
for efficiently and accurately extracting relevant information from a piece of text.
Over the past few years, Large Language Models (LLMs) based on the Transformer model
\cite{vaswani2017attention} have shown unique abilities for many NLP tasks, including
document summarization \cite{yadav2023state}.
Recent developments have demonstrated remarkable improvements in the relevancy and
coherence of summaries generated by such LLMs.

Long document summarization is more crucial than ever since it involves removing redundancies
from a piece of text, making reading the text concise and efficient.
A significant limitation of Transformers is their context size, stemming from the quadratic
memory and computational complexity of the attention mechanism in their architecture
\cite{du2023improving}.
This constraint hinders the extraction of relevant information from extensive texts, where
summarization is valuable to overcome the interpretative challenges posed by complex
relations on such a large scale.

We experiment with various novel approaches to address the input size limitations of LLMs.
The methods introduced do not include any architectural modifications to the model used for
summarization, and hence can be incorporated into any existing summarization pipeline.
We believe that these methods effectively utilize the full potential of any existing LLM.
Though our experiments only include long document summarization, our methods can be applied
to any NLP task (sequence classification, question-answering, etc.) that requires processing
long texts.

We start by stating the problem statement\textsuperscript{\ref{sec:problem}} and discussing
some related works\textsuperscript{\ref{sec:related-works}} to gain insights into the problem
in hand and the state-of-the-art solutions.
We then introduce the datasets\textsuperscript{\ref{sec:datasets}} and
methodology\textsuperscript{\ref{sec:methodology}} used in our expriments.
For evaluating our results, we present some common metrics\textsuperscript{\ref{sec:metrics}}
used in text summariztion.
We end the report by discussing our experimental findings\textsuperscript{\ref{sec:findings}}
and concluding the study\textsuperscript{\ref{sec:conclusion}}.
