\section{Introduction}
	\label{sec:introduction}

	Due to the abundance of online text data, document summarization has become crucial
	for efficient and accurate extraction of relevant information from a piece of text.
	Over the past few years, Large Language Models (LLMs) based on the Transformer model
	\cite{vaswani2017attention} have shown ground-breaking abilities for several NLP tasks,
	including document summarization \cite{yadav2023state}.
	Recent developments have demonstrated remarkable improvements in the relevancy and
	coherence of summaries generated by such LLMs.

	Long document summarization is more crucial than ever since it involves removing redundancies
	from a piece of text, making reading the text concise and efficient.
	One of the major limitations of Transformers is limited context size, stemming from the
	quadratic memory and computational complexity of the attention mechanism in their architecture
	\cite{du2023improving}.
	This constraint hinders the extraction of relevant information from extensive texts where
	summarization is valuable to overcome the interpretative challenges posed by complex
	relations on such a large scale.

	We experiment with various novel approaches to address the input size limitations of LLMs.
	The methods introduced do not include any architectural modifications to the model used for
	summarization and can be incorporated into any existing summarization pipeline.
	We believe that these methods can effectively utilize the full potential of any existing LLM.
	Though our experiments only include the task of summarization, our methods can be applied
	to any NLP task (sequence classification, question-answering, etc.) that requires processing
	long texts.

	We start by stating the problem statement (\ref{sec:problem}) and discussing some related works
	(\ref{sec:related-works}) to gain insights into the problem and the state-of-the-art solutions.
	We then introduce the datasets (\ref{sec:datasets}) and methodology (\ref{sec:methodology}) used
	in our experiments.
	For evaluating our results, we present some common metrics (\ref{sec:metrics}) used in text
	summarization.
	We end the report by discussing our experimental findings (\ref{sec:findings}) and potential
	future work (\ref{sec:future-work}), and concluding the study (\ref{sec:conclusion}).
