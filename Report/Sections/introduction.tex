\section{Introduction}
\label{sec:introduction}

Due to the abundance of online text data, document summarization has become crucial
for efficiently and accurately extracting relevant information from a piece of text.
Over the past few years, Large Language Models (LLMs) based on the Transformer model
\cite{vaswani2017attention} have shown unique abilities for many NLP tasks, including
document summarization \cite{yadav2023state}.
Recent developments have demonstrated remarkable improvements in the relevancy and
coherence of summaries generated by such LLMs.

A significant limitation of Transformers is their context size, which is due to the quadratic
memory and computational complexity of the attention mechanism in the their architecture.
Transformers are unable to process large documents due to these restrictions.
Long document summarization is more crucial than ever since summarization involves
removing redundancies from a piece of text, which makes reading the text concise and
efficient.

This paper introduces several novel algorithms that aim to handle long documents,
removing input size limitations in Transformers.
