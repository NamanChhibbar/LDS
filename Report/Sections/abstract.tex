\begin{abstract}

	A vast amount of textual data is added to the internet daily, making utilizing and
	interpreting textual data difficult and cumbersome.
	Therefore, text summarization is crucial for efficiently extracting relevant information.
	Although many Transformer models excel in summarization, they are constrained by the input
	size, preventing them from processing texts longer than their context size.
	This study introduces several novel algorithms to efficiently overcome the input size
	limitations, utilizing the full potential of LLMs without any architectural changes.
	We test our algorithms on texts with more than \textbf{70,000 words}, and our experiments
	show a significant increase in BERTScore with competitive ROUGE scores.

\end{abstract}
