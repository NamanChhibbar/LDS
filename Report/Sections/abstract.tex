\begin{abstract}

A vast amount of textual data is added to the internet daily, making utilizing and
interpreting textual data difficult and cumbersome.
Hence, text summarization is crucial for efficiently extracting relevant information.
Although many Large Language Models (LLMs) have excelled in summarization, they are
constrained by input size, which prevents them from processing longer texts.
This prevents extracting relevant information from extensive texts, where
summarization is valuable due to the increased time required for processing.
This study proposes multiple novel algorithms to overcome input size limitations,
allowing LLMs to summarize long documents efficiently.

\end{abstract}
