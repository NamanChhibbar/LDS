\begin{abstract}

A vast amount of textual data is added to the internet daily, making utilizing and
interpreting textual data difficult and cumbersome.
Therefore, text summarization is crucial for efficiently extracting relevant information.
Although many Large Language Models (LLMs) excel in summarization, they are constrained
by input size, preventing them from processing texts longer than their context size.
This study introduces several novel algorithms to overcome the input size limitations
efficiently, utilizing the full potetial of LLMs with no architectural changes.

\end{abstract}
