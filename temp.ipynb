{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sys import getsizeof\n",
    "from time import sleep\n",
    "import json\n",
    "import re\n",
    "import pickle\n",
    "import inspect\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "\n",
    "import numpy as np\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from transformers import (\n",
    "\tBartTokenizer, BartForConditionalGeneration,\n",
    "\tT5Tokenizer, T5ForConditionalGeneration,\n",
    "\tGPT2TokenizerFast\n",
    ")\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from dotenv import load_dotenv\n",
    "import openai\n",
    "\n",
    "from utils.helpers import *\n",
    "from utils.encoders import *\n",
    "from utils.pipelines import *\n",
    "from utils.trainer_utils import *\n",
    "from utils.evaluator_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "crs files: 7238, gao files: 12228\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"/Users/naman/Workspace/Data/Long-Document-Summarization\"\n",
    "data_dir = \"/home/nchibbar/Data\"\n",
    "\n",
    "crs_files = os.listdir(crs_dir := f\"{data_dir}/GovReport/crs\")\n",
    "gao_files = os.listdir(gao_dir := f\"{data_dir}/GovReport/gao\")\n",
    "\n",
    "print(f\"crs files: {len(crs_files)}, gao files: {len(gao_files)}\")\n",
    "\n",
    "out_dir = f\"{data_dir}/GovReport/processed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1024, 4096)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sentence transformer\n",
    "sent_dir = f\"{data_dir}/Models/Sent-Transformer\"\n",
    "sent_encoder = SentenceTransformer(sent_dir)\n",
    "\n",
    "# BART\n",
    "bart_dir = f\"{data_dir}/Models/BART\"\n",
    "bart_fine_tuned = f\"{data_dir}/Models/BART-GovReport-SentenceSampler\"\n",
    "bart_tokenizer = BartTokenizer.from_pretrained(bart_dir)\n",
    "bart_model = BartForConditionalGeneration.from_pretrained(bart_fine_tuned)\n",
    "bart_context_size = bart_model.config.max_position_embeddings\n",
    "\n",
    "# T5\n",
    "t5_dir = f\"{data_dir}/Models/T5\"\n",
    "t5_tokenizer = T5Tokenizer.from_pretrained(t5_dir)\n",
    "t5_model = T5ForConditionalGeneration.from_pretrained(t5_dir)\n",
    "t5_context_size = t5_model.config.n_positions\n",
    "\n",
    "# GPT 3.5 turbo tokenizer\n",
    "gpt_dir = f\"{data_dir}/Models/GPT-3.5-turbo-tokenizer\"\n",
    "gpt_tokenizer = GPT2TokenizerFast.from_pretrained(gpt_dir)\n",
    "gpt_model = \"gpt-3.5-turbo\"\n",
    "gpt_context_size = 4096\n",
    "\n",
    "bart_context_size, t5_context_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = TextProcessor(preprocessing=True)\n",
    "postprocessor = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GovReport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_subsections(sections):\n",
    "\ttext = \"\"\n",
    "\tfor sec in sections:\n",
    "\t\tsec_text = \"\\n\\n\".join(sec[\"paragraphs\"])\n",
    "\t\tif sec[\"section_title\"]:\n",
    "\t\t\tsec_text = f\"Section {sec[\"section_title\"]}:\\n\\n{sec_text}\"\n",
    "\t\ttext = f\"{text}\\n\\n{sec_text}\" if text else sec_text\n",
    "\t\tif sec[\"subsections\"]:\n",
    "\t\t\tsub_text = combine_subsections(sec[\"subsections\"])\n",
    "\t\t\ttext = f\"{text}\\n\\n{sub_text}\" if text else sub_text\n",
    "\treturn text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_crs_files = len(crs_files)\n",
    "for i, file in enumerate(crs_files):\n",
    "\tfull_path = os.path.join(crs_dir, file)\n",
    "\twith open(full_path) as fp:\n",
    "\t\tdata = json.load(fp)\n",
    "\tclear_stdout()\n",
    "\tprint(f\"{num_crs_files - i} files left\", end=\"\")\n",
    "\ttext = f\"{data[\"title\"]}\\n\\n\"\n",
    "\ttext += combine_subsections([data[\"reports\"]])\n",
    "\tsummary = \" \".join(data[\"summary\"])\n",
    "\tsummary = preprocessor.process(summary)\n",
    "\twith open(f\"{out_dir}/{file}\", \"w\") as fp:\n",
    "\t\tjson.dump({\n",
    "\t\t\t\"text\": text,\n",
    "\t\t\t\"summary\": summary\n",
    "\t\t}, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in gao_files:\n",
    "\tfile = os.path.join(gao_dir, file)\n",
    "\twith open(file) as fp:\n",
    "\t\tdata = json.load(fp)\n",
    "\ttext = combine_subsections(data[\"report\"])\n",
    "\ttext = preprocessor.process(text)\n",
    "\tprint(data[\"highlight\"])\n",
    "\tsummary = \"\\n\".join(data[\"highlight\"])\n",
    "\tsummary = preprocessor.process(summary)\n",
    "\twith open(f\"{out_dir}/{file}\", \"w\") as fp:\n",
    "\t\tjson.dump({\n",
    "\t\t\t\"text\": text,\n",
    "\t\t\t\"summary\": summary\n",
    "\t\t}, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(stop_words=\"english\")\n",
    "vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm = vectorizer.fit_transform([data[\"text\"]])\n",
    "vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = 4\n",
    "lda = LatentDirichletAllocation(n_components=topics)\n",
    "lda.fit(dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_dist = lda.transform(dtm)\n",
    "print(topic_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_topics(model, feature_names, num_top_words):\n",
    "\tfor topic_idx, topic in enumerate(model.components_):\n",
    "\t\tprint(f\"Topic {topic_idx}:\")\n",
    "\t\tprint(\" \".join([feature_names[i] for i in topic.argsort()[:-num_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_top_words = 10\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "display_topics(lda, feature_names, num_top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts, summaries = [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max 73_791\n",
    "min_words_text = 70_000\n",
    "for file in crs_files:\n",
    "\twith open(f\"{out_dir}/{file}\") as fp:\n",
    "\t\tdata = json.load(fp)\n",
    "\tif count_words(data[\"text\"]) >= min_words_text:\n",
    "\t\tbreak\n",
    "texts.append(data[\"text\"])\n",
    "summaries.append(data[\"summary\"])\n",
    "\n",
    "count_words(data[\"text\"]), count_words(data[\"summary\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_words = 30_000\n",
    "texts, summaries = [], []\n",
    "for file in crs_files:\n",
    "\twith open(f\"{out_dir}/{file}\") as fp:\n",
    "\t\tdata = json.load(fp)\n",
    "\tif count_words(data[\"text\"]) >= min_words:\n",
    "\t\ttexts.append(data[\"text\"])\n",
    "\t\tsummaries.append(data[\"summary\"])\n",
    "\n",
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_min_words = 20\n",
    "sent_segmenter = TextSegmenter(nltk.sent_tokenize, segment_min_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_summary_tokens = 400\n",
    "head_size = .5\n",
    "threshold = .7\n",
    "seed = 69\n",
    "device = get_device()\n",
    "device = \"cpu\"\n",
    "system_prompt = \"You will be given some segments of a very long document. Your task is to summarize the entire document as a whole by extracting key information and ideas from the segments. Generate a detailed, concise, and coherent summary in 500 words. Do not refer to the document in the summary in any way.\"\n",
    "\n",
    "bart_encoders = [\n",
    "\tTruncateMiddle(\n",
    "\t\tbart_tokenizer, bart_context_size, head_size, preprocessor, True\n",
    "\t),\n",
    "\tUniformSampler(\n",
    "\t\tbart_tokenizer, bart_context_size, sent_segmenter, preprocessor,\n",
    "\t\tTrue, seed\n",
    "\t),\n",
    "\tSentenceSampler(\n",
    "\t\tbart_tokenizer, bart_context_size, sent_segmenter, sent_encoder,\n",
    "\t\tpreprocessor, True, device=device, seed=seed\n",
    "\t),\n",
    "\tRemoveRedundancy(\n",
    "\t\tbart_tokenizer, bart_context_size, sent_segmenter, sent_encoder,\n",
    "\t\tpreprocessor, True, device=device, seed=seed\n",
    "\t)\n",
    "]\n",
    "t5_encoders = [\n",
    "\tTruncateMiddle(\n",
    "\t\tt5_tokenizer, t5_context_size, head_size, preprocessor, True\n",
    "\t),\n",
    "\tUniformSampler(\n",
    "\t\tt5_tokenizer, t5_context_size, sent_segmenter, preprocessor,\n",
    "\t\tTrue, seed\n",
    "\t),\n",
    "\tSentenceSampler(\n",
    "\t\tt5_tokenizer, t5_context_size, sent_segmenter, sent_encoder,\n",
    "\t\tpreprocessor, True, device=device, seed=seed\n",
    "\t),\n",
    "\tRemoveRedundancy(\n",
    "\t\tt5_tokenizer, t5_context_size, sent_segmenter, sent_encoder,\n",
    "\t\tpreprocessor, True, device=device, seed=seed\n",
    "\t)\n",
    "]\n",
    "gpt_encoders = [\n",
    "\tTruncateMiddle(\n",
    "\t\tgpt_tokenizer, gpt_context_size, head_size, preprocessor, True\n",
    "\t),\n",
    "\tUniformSampler(\n",
    "\t\tgpt_tokenizer, gpt_context_size, sent_segmenter, preprocessor,\n",
    "\t\tTrue, seed\n",
    "\t),\n",
    "\tSentenceSampler(\n",
    "\t\tgpt_tokenizer, gpt_context_size, sent_segmenter, sent_encoder,\n",
    "\t\tpreprocessor, True, device=device, seed=seed\n",
    "\t),\n",
    "\tRemoveRedundancy(\n",
    "\t\tgpt_tokenizer, gpt_context_size, sent_segmenter, sent_encoder,\n",
    "\t\tpreprocessor, True, device=device, seed=seed\n",
    "\t)\n",
    "]\n",
    "bart_pipelines = [\n",
    "\tSummarizationPipeline(\n",
    "\t\tbart_model, enc, min_summary_tokens, bart_context_size,\n",
    "\t\tpostprocessor, device\n",
    "\t) for enc in bart_encoders\n",
    "]\n",
    "t5_pipelines = [\n",
    "\tSummarizationPipeline(\n",
    "\t\tt5_model, enc, min_summary_tokens, t5_context_size,\n",
    "\t\tpostprocessor, device\n",
    "\t) for enc in t5_encoders\n",
    "]\n",
    "gpt_pipelines = [\n",
    "\tOpenAIPipeline(\n",
    "\t\tgpt_model, enc, gpt_context_size\n",
    "\t) for enc in gpt_encoders\n",
    "]\n",
    "pipelines = bart_pipelines + t5_pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 3\n",
    "num_workers = min(len(pipelines), os.cpu_count())\n",
    "num_workers = 0\n",
    "\n",
    "evaluator = Evaluator(pipelines, num_workers, device)\n",
    "results = evaluator(texts, summaries, batch_size)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 8226,     6, 13706,  ...,   121,  1375,     1]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]])}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inps = t5_tokenizer(texts[0], return_tensors=\"pt\", max_length=t5_context_size, truncation=True)\n",
    "inps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0, 23263,   196,    10, 23263,   196,     3, 15550,    45,   215,\n",
       "            12,   215,     6,    68,    16,     8,   495,    13, 23263,   196,\n",
       "            21,   677,     6,    16,  2083,    13,  2766,   132,   130,   147,\n",
       "             3,     9,   770,   151,    30,     8,  6162,    13,  3715,   479,\n",
       "            21,   199,     5,    86,    24,    97,  1059,     6,  1315,    12,\n",
       "         10014,     6,    34,    47,    46,  1348,    13,    80,   568,   334,\n",
       "           192,   716,     5,    37,   381,    13,   151,   113,  3977,   788,\n",
       "            12, 12259,     7,  2196,    57,     8,  2095,    42,   119,  4299,\n",
       "           383,     8,   239,    11,   706,     5,  2150,    12, 10014,  1883,\n",
       "            57,     8, 10504,    13,  6409,     7,    11, 11418,   127,     7,\n",
       "             6,   344,   273,   113,  3977,    44,   161,    11,   273,   113,\n",
       "          1279,    21,     8,   789,   141,     3,     9,  2074,    13,    81,\n",
       "           874,   770,   151,     5,   242,   677,   117,   859,   175,   130,\n",
       "          2765,   113,  1513,    70,  2476,   250,    79,   410,    59,    43,\n",
       "           631,   540,    12,   726,    21,   135,     5, 22242,   113,  1204,\n",
       "           705,   145, 18358,  3151,     7,     3,     9,   239,   133,    36,\n",
       "          1866,   223,    38,     3,     9,   741,    13,    73, 12760, 15488,\n",
       "             5,  3526,     8,  9848,    13,   973,     6,  1652,   228,    36,\n",
       "           787,   339,  3074,    15,     7,   937,    79,  1736,   824,  1124,\n",
       "             5,   156,    79,  4567,    12,   942,   175,  1124,     6,    79,\n",
       "           133,    36,  1702,  1215,   699,    26,    45,     8,   907,  1323,\n",
       "             5,  1541,  4442,    65,  2804,  6704,   492,  7713,   831,    12,\n",
       "           726,     3,     9,  2572,    12,  1321,   113,   930,    21,     8,\n",
       "          2822,   789,     6,   186,  7021,     7,    43,   118,  5241,    12,\n",
       "           726,    72,  5161,   437,   258,     5,   886,  2315,    43,    92,\n",
       "           263,  3786, 19551,    53,   452,  1652,    45,   464,    21,     8,\n",
       "           538,     5,   506,  3786,    33,   341, 13321,    26,    16,   128,\n",
       "          1467,    13,     8,   684,     5,   282,   294,    13,     8,    96,\n",
       "           188,   122, 10292,   757, 18490,   121,  2426,     6,  5347,  7021,\n",
       "             7,  1553,    12, 10720,    15,     8,  1104,   257,    13, 10416,\n",
       "            31,     7, 15488,   365,     8,  7491,     7,    13,  1193, 16812,\n",
       "           257,     5,   100,  2237,    12,     8, 31226,   297,    13,     8,\n",
       "         11458,  3807,    49,    31,     7,  6585,   581,     8, 12346,   853,\n",
       "             5,  1404,  7021,     7,  4973,    15,    26,   581,     8,   993,\n",
       "            16, 15488,    11,  1596,   640,     8,  2982,     5,   555,   224,\n",
       "          1470,     6,   718,     8,    96,   371,    15,   588,   342,  2063,\n",
       "           976,  5147,    16,   957,  4560,    12,  4221,    66,   724,    13,\n",
       "             8,  1364,  2287,     5,    94,    47, 10431,    13,  8675,    45,\n",
       "           796,   569,  2287,    18,   532,   167,  8304,   271,     8,  1001,\n",
       "           573,     5,  1589,   887,   130,  8160,  1661,     7,    13,     8,\n",
       "          5034,   343,  3450,     6,   801,    38,     8,  8250,    52, 12230,\n",
       "             5,   328,  1213,  4677,    28, 13446,    11,   268,   904,    12,\n",
       "          2497,   807,    13,     8,   239,     5,   290,    47,  5054,  1918,\n",
       "             8,  7565,     7,    11,  1393,    13,  7021,     7,     5,    71,\n",
       "          2942,    13,     8,  3545,     7,  1800,    24,  3095,  1601,  2710,\n",
       "           225,   916,    12,   380,   928,  2166,  1066,   145, 25686,     5,\n",
       "          7940,     6, 15623,  8560,     8,  7707,  1602,    11,  4399,    26,\n",
       "           615, 12243,    17,    15,    15,     5,  2867,  2251,  6141,    24,\n",
       "          1371,    47,  8807,   139,  1317, 26405,     7,    18,   329,    23,\n",
       "          8437, 23081,  1285,   872,  1076,     6,  1001,   887,     6,  3850,\n",
       "          5452,     6,    11,  6271,    32,     7,     1]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = t5_model.generate(**inps, min_length=500)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 507])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<pad> FYI: FYI varies from year to year, but in the case of FYI for example, in February of 2000 there were over a million people on the streets of Chicago looking for help. In that time period, according to estimates, it was an average of one person every two hours. The number of people who died due to suicides reported by the police or other agencies during the day and night. According to estimates released by the Bureau of Costs and Surveyors, between those who died at work and those who worked for the government had a population of about five million people. For example; among these were workers who lost their jobs because they did not have enough money to pay for them. Workers who received less than fifty cents a day would be paid back as a result of unpaid wages. Under the provisions of law, employees could be given free lunches provided they met certain conditions. If they failed to meet these conditions, they would be considered exiled from the United States. Since Congress has passed legislation making employers required to pay a fee to anyone who works for the federal government, many unions have been forced to pay more taxes since then. Some states have also made laws prohibiting public employees from working for the state. These laws are still enforced in some parts of the country. As part of the \"Aggressive Reform\" movement, labor unions began to oppose the taxation of worker\\'s wages under the Articles of Confederation. This led to the abolishment of the wage earner\\'s strike against the ruling class. Many unions protested against the increase in wages and prices across the nation. One such organization, called the \"Federate Council,\" formed in 1907 to represent all members of the lower classes. It was composed of representatives from various social classes-the most prominent being the black community. Black women were elected Presidents of the Federalist Party, known as the Tearsheet. They held meetings with politicians and businessmen to discuss issues of the day. There was debate regarding the merits and benefits of unions. A majority of the Unions felt that civilized society should continue to support individual rights rather than slavery. Meanwhile, Republicans opposed the Civil War and demanded war repartee. Both parties believed that America was divided into multiple factions-Middle Classes included white men, black women, African Americans, and Latinos</s>'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t5_tokenizer.decode(out[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bart_model.config.min_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uccs-reu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
