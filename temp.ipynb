{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sys import getsizeof\n",
    "import json\n",
    "import re\n",
    "import pickle\n",
    "import inspect\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "\n",
    "import numpy as np\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from transformers import (\n",
    "\tBartTokenizer, BartForConditionalGeneration,\n",
    "\tT5Tokenizer, T5ForConditionalGeneration,\n",
    "\tGPT2TokenizerFast\n",
    ")\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from dotenv import load_dotenv\n",
    "import tiktoken\n",
    "import openai\n",
    "\n",
    "from utils.pipelines import *\n",
    "from utils.helpers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "crs files: 7238, gao files: 12228\n"
     ]
    }
   ],
   "source": [
    "# data_dir = \"/Users/naman/Workspace/Data/UCCS-REU\"\n",
    "data_dir = \"/home/nchibbar/Data\"\n",
    "\n",
    "crs_files = os.listdir(crs_dir := f\"{data_dir}/GovReport/crs\")\n",
    "gao_files = os.listdir(gao_dir := f\"{data_dir}/GovReport/gao\")\n",
    "\n",
    "print(f\"crs files: {len(crs_files)}, gao files: {len(gao_files)}\")\n",
    "\n",
    "crs_out = f\"{data_dir}/GovReport/crs-processed\"\n",
    "gao_out = f\"{data_dir}/GovReport/gao-processed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_tokens = 512\n",
    "\n",
    "# Sentence transformer\n",
    "sent_dir = f\"{data_dir}/Models/Sent-Transformer\"\n",
    "sent_checkpoint = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "sent_encoder = SentenceTransformer(sent_dir)\n",
    "sent_encoder\n",
    "\n",
    "# BART\n",
    "bart_dir = f\"{data_dir}/Models/BART\"\n",
    "bart_fine_tuned = f\"{data_dir}/Models/BART-GovReport-SentenceSampler\"\n",
    "bart_checkpoint = \"facebook/bart-large-cnn\"\n",
    "tokenizer = BartTokenizer.from_pretrained(bart_dir)\n",
    "model = BartForConditionalGeneration.from_pretrained(bart_fine_tuned)\n",
    "context_size = model.config.max_position_embeddings\n",
    "\n",
    "# T5\n",
    "# t5_dir = f\"{data_dir}/Models/T5\"\n",
    "# t5_checkpoint = \"google/flan-t5-base\"\n",
    "# t5_checkpoint = \"pszemraj/long-t5-tglobal-base-16384-book-summary\"\n",
    "# tokenizer = T5Tokenizer.from_pretrained(t5_dir)\n",
    "# model = T5ForConditionalGeneration.from_pretrained(t5_dir)\n",
    "# context_size = model.config.n_positions\n",
    "\n",
    "# GPT 3.5 turbo tokenizer\n",
    "gpt_dir = f\"{data_dir}/Models/GPT-3.5-turbo-tokenizer\"\n",
    "gpt_tokenizer = GPT2TokenizerFast.from_pretrained(gpt_dir)\n",
    "\n",
    "context_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>', '</s>', '<unk>', '</s>', '<pad>', '<s>', '<mask>']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "special_tokens = extract_special_tokens(\n",
    "\ttokenizer.special_tokens_map.values()\n",
    ")\n",
    "preprocessor = TextProcessor(preprocessing=True)\n",
    "postprocessor = None\n",
    "# postprocessor = TextProcessor(ignore_tokens=special_tokens)\n",
    "special_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head_size = .5\n",
    "threshold = .7\n",
    "seed = 69\n",
    "device = get_device()\n",
    "device = \"cpu\"\n",
    "\n",
    "encoders = [\n",
    "\tTruncateMiddle(\n",
    "\t\ttokenizer, context_size, head_size, preprocessor\n",
    "\t),\n",
    "\tUniformSampler(\n",
    "\t\ttokenizer, context_size, nltk.sent_tokenize, preprocessor, seed\n",
    "\t),\n",
    "\tSentenceSampler(\n",
    "\t\ttokenizer, context_size, nltk.sent_tokenize, sent_encoder,\n",
    "\t\tpreprocessor, threshold, device, seed\n",
    "\t),\n",
    "\tRemoveRedundancy(\n",
    "\t\ttokenizer, context_size, nltk.sent_tokenize, sent_encoder,\n",
    "\t\tpreprocessor, threshold, device, seed\n",
    "\t),\n",
    "]\n",
    "\n",
    "pipelines = [\n",
    "\tSummarizationPipeline(\n",
    "\t\tmodel, encoder, max_tokens, postprocessor, device\n",
    "\t) for encoder in encoders\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GovReport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_subsections(sections):\n",
    "\ttext = \"\"\n",
    "\tfor sec in sections:\n",
    "\t\tsec_text = \"\\n\\n\".join(sec[\"paragraphs\"])\n",
    "\t\tif sec[\"section_title\"]:\n",
    "\t\t\tsec_text = f\"Section {sec[\"section_title\"]}:\\n\\n{sec_text}\"\n",
    "\t\ttext = f\"{text}\\n\\n{sec_text}\" if text else sec_text\n",
    "\t\tif sec[\"subsections\"]:\n",
    "\t\t\tsub_text = combine_subsections(sec[\"subsections\"])\n",
    "\t\t\ttext = f\"{text}\\n\\n{sub_text}\" if text else sub_text\n",
    "\treturn text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in crs_files:\n",
    "\twith open(f\"{crs_dir}/{file}\") as fp:\n",
    "\t\tdata = json.load(fp)\n",
    "\ttext = combine_subsections([data[\"reports\"]])\n",
    "\ttext = preprocessor.process(text)\n",
    "\tsummary = \"\\n\".join(data[\"summary\"])\n",
    "\tsummary = preprocessor.process(summary)\n",
    "\twith open(f\"{crs_out}/{file}\", \"w\") as fp:\n",
    "\t\tjson.dump({\n",
    "\t\t\t\"text\": text,\n",
    "\t\t\t\"summary\": summary\n",
    "\t\t}, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in gao_files:\n",
    "\twith open(f\"{gao_dir}/{file}\") as fp:\n",
    "\t\tdata = json.load(fp)\n",
    "\ttext = combine_subsections(data[\"report\"])\n",
    "\ttext = preprocessor.process(text)\n",
    "\tprint(data[\"highlight\"])\n",
    "\tsummary = \"\\n\".join(data[\"highlight\"])\n",
    "\tsummary = preprocessor.preprocess(summary)\n",
    "\twith open(f\"{gao_out}/{file}\", \"w\") as fp:\n",
    "\t\tjson.dump({\n",
    "\t\t\t\"text\": text,\n",
    "\t\t\t\"summary\": summary\n",
    "\t\t}, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(stop_words=\"english\")\n",
    "vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm = vectorizer.fit_transform([data[\"text\"]])\n",
    "vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = 4\n",
    "lda = LatentDirichletAllocation(n_components=topics)\n",
    "lda.fit(dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_dist = lda.transform(dtm)\n",
    "print(topic_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_topics(model, feature_names, num_top_words):\n",
    "\tfor topic_idx, topic in enumerate(model.components_):\n",
    "\t\tprint(f\"Topic {topic_idx}:\")\n",
    "\t\tprint(\" \".join([feature_names[i] for i in topic.argsort()[:-num_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_top_words = 10\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "display_topics(lda, feature_names, num_top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts, summaries = [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(53559, 500)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# max 73_791\n",
    "min_words_text = 50_000\n",
    "for file in crs_files:\n",
    "\twith open(f\"{crs_out}/{file}\") as fp:\n",
    "\t\tdata = json.load(fp)\n",
    "\tif count_words(data[\"text\"]) >= min_words_text:\n",
    "\t\tbreak\n",
    "texts.append(data[\"text\"])\n",
    "summaries.append(data[\"summary\"])\n",
    "\n",
    "count_words(data[\"text\"]), count_words(data[\"summary\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7238"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts, summaries = [], []\n",
    "for file in crs_files:\n",
    "\twith open(f\"{crs_out}/{file}\") as fp:\n",
    "\t\tdata = json.load(fp)\n",
    "\ttexts.append(data[\"text\"])\n",
    "\tsummaries.append(data[\"summary\"])\n",
    "\n",
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "batch_size = None if len(texts) < 3 else 3\n",
    "evaluator = Evaluator(\n",
    "\tpipelines, texts, summaries, device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6870.044142997358, 3887.949132011272, 4018.1995559833013, 9793.55108900927]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_workers = os.cpu_count()\n",
    "# num_workers = 3\n",
    "evaluator.generate_summaries(batch_size, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'rouge-1': [0.18313253012048192, 0.76, 0.10410958904109589],\n",
       "  'rouge-2': [0.11138014527845035, 0.46938775510204084, 0.06318681318681318],\n",
       "  'rouge-l': [0.23237093419338278, 0.7251437262463217, 0.13835293516474403],\n",
       "  'rouge-w': [0.041725977592485294, 0.420434591230423, 0.02195231621305737]},\n",
       " {'rouge-1': [0.0625, 0.2549019607843137, 0.03561643835616438],\n",
       "  'rouge-2': [0.009661835748792272, 0.04, 0.005494505494505495],\n",
       "  'rouge-l': [0.08358463007595818, 0.25725166042361713, 0.04989870556426403],\n",
       "  'rouge-w': [0.01384352889403138, 0.13688925530533755, 0.00729040142495195]},\n",
       " {'rouge-1': [0.0625, 0.2549019607843137, 0.03561643835616438],\n",
       "  'rouge-2': [0.009661835748792272, 0.04, 0.005494505494505495],\n",
       "  'rouge-l': [0.08358463007595818, 0.25725166042361713, 0.04989870556426403],\n",
       "  'rouge-w': [0.01384352889403138, 0.13688925530533755, 0.00729040142495195]},\n",
       " {'rouge-1': [0.1411764705882353, 0.5, 0.0821917808219178],\n",
       "  'rouge-2': [0.023640661938534275, 0.0847457627118644, 0.013736263736263736],\n",
       "  'rouge-l': [0.15154045699862015, 0.4169238155423266, 0.09259879998407583],\n",
       "  'rouge-w': [0.026970624265536956, 0.22871285498550592, 0.01433024880261942]}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.get_rouge_score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.8340007662773132, 0.8763248920440674, 0.7955765724182129],\n",
       " [0.7822791934013367, 0.8163412809371948, 0.7509458065032959],\n",
       " [0.7822791934013367, 0.8163412809371948, 0.7509458065032959],\n",
       " [0.7969955801963806, 0.8280269503593445, 0.7682060599327087]]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.get_bert_score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<s>Participation data: In June 2009, a total of 18 million families, composed of 43 million recipients (including 33 million children), received TANF- or MOE-funded cash assistance In June 2010, a total of 19 million families, composed of 45 million recipients (including 34 million children), received TANF- or MOE-funded cash assistance The larger number of individuals or families receiving any TANF- or MOE-funded benefit or service is not known\\n\\nCRS report: CRS Report R40946, The Temporary Assistance for Needy Families Block Grant: An Introduction, by [author name scrubbed].\\nIn some limited circumstances, families may be low-income, with incomes as high as 80% of area median income\\n\\nForm and recipient of federal assistance: Project-based rental assistance contracts between HUD and private property owners HUD has not had the authority to enter into new contracts since 1983, but does have the authority to renew existing contracts when they expire There are properties with project-based rental assistance contracts in the territories (U.S Virgin Islands, Puerto Rico, and Guam).\\nAllocation formula: Portions of available annual funds are allocated under four different formulasBasic, Concentration, Targeted, and Education Finance Incentive Grants (EFIG)although funds are then combined and used for the same purposes by recipient LEAs Although the allocation formulas have several distinctive elements, the primary factors used in all four formulas are an eligible child count and an expenditure factor The eligible child count includes children aged 5-17: (a) in poor families; (b) in institutions for neglected or delinquent children or in foster homes; and (c) in families receiving Temporary Assistance for Needy Families payments above the poverty level Each element of the population factor is updated annually The expenditure factor is the state average per pupil expenditure for public K-12 education (subject to a minimum of 80% and maximum of 120% of the national average, further multiplied by 040), and is the same for all LEAs in the same state Both the Targeted and EFIG formulas include weighting schemes to increase aid to LEAs with the highest numbers or concentrations of eligible children The EFIG formula also includes an effort factor, based on average per pupil expenditure for public K-12 education compared to personal income per capita for each state compared to the nation as a whole, and an equity factor, based on variations in average per pupil expenditures among the LEAs in each state Each formula has a hold-harmless provision (no LEA may receive less than 85%-95% of its previous year grant, depending on the LEA's poverty level and whether the LEA continues to meet the formula's eligibility threshold).\\nNew obligations: FY2009: $2.324 billion FY2008: $2.038 billion\\n\\nBudgetary classification: Mandatory (open-ended entitlement to states).\\n</s>\""
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp = tokenizer.decode(encoders[2](texts)[\"input_ids\"][0])\n",
    "inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encountered exception of type OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable, Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_26527/1819395941.py\", line 2, in <module>\n",
      "    response = openai.chat.completions.create(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nchibbar/naman-venv-3.12/lib/python3.12/site-packages/openai/_utils/_proxy.py\", line 20, in __getattr__\n",
      "    proxied = self.__get_proxied__()\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nchibbar/naman-venv-3.12/lib/python3.12/site-packages/openai/_utils/_proxy.py\", line 55, in __get_proxied__\n",
      "    return self.__load__()\n",
      "           ^^^^^^^^^^^^^^^\n",
      "  File \"/home/nchibbar/naman-venv-3.12/lib/python3.12/site-packages/openai/_module_client.py\", line 12, in __load__\n",
      "    return _load_client().chat\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"/home/nchibbar/naman-venv-3.12/lib/python3.12/site-packages/openai/__init__.py\", line 323, in _load_client\n",
      "    _client = _ModuleClient(\n",
      "              ^^^^^^^^^^^^^^\n",
      "  File \"/home/nchibbar/naman-venv-3.12/lib/python3.12/site-packages/openai/_client.py\", line 104, in __init__\n",
      "    raise OpenAIError(\n",
      "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "\tresponse = openai.chat.completions.create(\n",
    "\t\tmodel=\"gpt-3.5-turbo\",\n",
    "\t\tmessages=[\n",
    "\t\t\t{\"role\": \"system\", \"content\": \"You are a summarizer who summarizes very long texts given important sentences.\"},\n",
    "\t\t\t{\"role\": \"user\", \"content\": inp}\n",
    "\t\t],\n",
    "\t\tmax_tokens=4097\n",
    "\t)\n",
    "except Exception as e:\n",
    "\tshow_exception(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated = response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The federal government spent almost $708 billion in FY2009 on programs for low-income people, and nearly $578 billion the previous year. The increased spending between the two years was largely due to the recession, with almost two-thirds coming from the American Recovery and Reinvestment Act (ARRA, P.L. 111-5), the economic stimulus enacted in February 2009.\\nLow-income programs discussed in this report are distinct from social insurance programs, such as Social Security or Medicare, which aim to protect American workers universally against lost wages or benefits when they retire, become disabled, or lose a job. In contrast, programs addressed here focus explicitly on low-income populations. They provide assistance in obtaining basic needs, such as health care, food, or housing, and seek to address the causes of low income through education, training, or other services. While these programs are very diverse, the analysis in this report yields certain general findings:\\n Health care dominates all other categories of benefits and services, accounting for nearly half of federal spending for low-income people. Cash aid is second but trails far behind, comprising 18% of spending in FY2009. Other categories, in decreasing size, are food assistance, housing and development, education, social services, energy assistance, and employment and training.\\n Four programs account for 60% of federal spending for low-income people and 10 programs make up more than three-fourths. Medicaid alone accounted for nearly 40% of FY2009 low-income spending; next were the Supplemental Nutrition Assistance Program (SNAP, formerly food stamps), Supplemental Security Income, and the refundable portion of the Earned Income Tax Credit.\\n Elderly and disabled individuals, and families with children are key target populations for much of the spending for low-income people. Federal policy toward families with children generally encourages work and includes incentives to \"make work pay.\" Other populations served by selected programs include veterans, students, homeless people, Indians, and refugees.\\n Within broad target populations, programs use different concepts to determine who is eligible. Most spending is on behalf of people determined individually eligible by virtue of their low income or eligibility for another income-tested program. \"Low income\" is defined in a multitude of ways, using different percentages of the federal poverty guidelines, specific dollar amounts, percentages of local area median income (primarily for housing programs), or other measures.\\n Many programs distribute funding to states or other entities to provide benefits and services to low-income people, using population-based allocation factors, cost-sharing formulas, or other mechanisms to target resources toward areas or entities with the greatest need. Some of these programs (especially in elementary and secondary education) have no further requirements for individuals to be determined income-eligible.\\n Programs for low-income people are most likely to use formula grants to distribute funds to states or another unit of government. Under many of these programs, notably including Medicaid, states must spend a specified amount of their own funds to receive federal dollars. State and local governments administer most of these federal programs; however, many of the largest programs provide federal benefits directly to individuals or via a nongovernmental intermediary.'"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summaries[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You are an expert summarizer.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system_prompt = \"You are an expert summarizer. Your task is to summarize long texts into concise and informative summaries. When provided with key segments of a longer text, please summarize these segments while ensuring the main ideas and important details are preserved.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OpenAIPipeline:\n",
    "\n",
    "\tdef __init__(\n",
    "\t\tself, model: str, encoder: Encoder, max_tokens: int,\n",
    "\t\tprompt_template: str=\"\", system_prompt: str=\"\",\n",
    "\t\tprevious_messages: list[str]|None=None\n",
    "\t) -> None:\n",
    "\t\tself.model = model\n",
    "\t\tself.encoder = encoder\n",
    "\t\tself.max_tokens = max_tokens\n",
    "\t\tself.prompt_template = prompt_template\n",
    "\t\tself.system_prompt = system_prompt\n",
    "\t\tself.previous_messages = previous_messages\n",
    "\t\tself.call_inputs = None\n",
    "\t\n",
    "\tdef create_inputs(self, text: str):\n",
    "\t\tsystem_prompt = self.system_prompt\n",
    "\t\tprevious_messages = self.previous_messages\n",
    "\t\tmessages = []\n",
    "\t\tif system_prompt:\n",
    "\t\t\tmessages.append({\"role\": \"system\", \"content\": system_prompt})\n",
    "\t\tif previous_messages is not None:\n",
    "\t\t\tfor text, summary in previous_messages:\n",
    "\t\t\t\tmessages.append({\"role\": \"user\", \"content\": text})\n",
    "\t\t\t\tmessages.append({\"role\": \"assistant\", \"content\": summary})\n",
    "\t\tprompt = f\"{self.prompt_template}{text}\"\n",
    "\t\tmessages.append({\"role\": \"user\", \"content\": prompt})\n",
    "\t\tself.call_inputs = {\n",
    "\t\t\t\"model\": self.model,\n",
    "\t\t\t\"messages\": messages,\n",
    "\t\t\t\"max_tokens\": self.max_tokens\n",
    "\t\t}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uccs-reu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
