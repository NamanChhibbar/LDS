{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sys import getsizeof\n",
    "from time import sleep\n",
    "import pickle\n",
    "import json\n",
    "import re\n",
    "import inspect\n",
    "from warnings import filterwarnings\n",
    "\n",
    "import numpy as np\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from transformers import (\n",
    "\tBartTokenizer, BartForConditionalGeneration,\n",
    "\tT5Tokenizer, T5ForConditionalGeneration,\n",
    "\tPegasusForConditionalGeneration, PegasusTokenizerFast,\n",
    "\tGPT2TokenizerFast\n",
    ")\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from utils.helpers import *\n",
    "from utils.encoders import *\n",
    "from utils.pipelines import *\n",
    "from utils.trainer_utils import *\n",
    "from utils.evaluator_utils import *\n",
    "\n",
    "def plot_histogram(data):\n",
    "\tbins = int(len(data) ** .5)\n",
    "\tplt.hist(data, bins=bins)\n",
    "\tplt.show()\n",
    "\n",
    "inf = float(\"inf\")\n",
    "filterwarnings(\"ignore\")\n",
    "device = get_device(500)\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"pegasus\"\n",
    "\n",
    "data_dir = \"/Users/naman/Workspace/Data/Long-Document-Summarization\"\n",
    "data_dir = \"/home/nchibbar/Data\"\n",
    "\n",
    "sent_dir = f\"{data_dir}/Models/Sent-Transformer\"\n",
    "bart_dir = f\"{data_dir}/Models/BART\"\n",
    "t5_dir = f\"{data_dir}/Models/T5\"\n",
    "pegasus_dir = f\"{data_dir}/Models/PEGASUS\"\n",
    "gpt_dir = f\"{data_dir}/Models/GPT-3.5-turbo-tokenizer\"\n",
    "\n",
    "govreport_dir = f\"{data_dir}/GovReport/processed\"\n",
    "bigpatent_dir = f\"{data_dir}/BigPatent/processed\"\n",
    "govreport_files = os.listdir(govreport_dir)\n",
    "bigpatent_files = os.listdir(bigpatent_dir)\n",
    "\n",
    "len(govreport_files), len(bigpatent_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentence transformer\n",
    "# Automatically loads into gpu if available\n",
    "sent_encoder = SentenceTransformer(sent_dir, device=device)\n",
    "\n",
    "match model_name:\n",
    "\n",
    "\tcase \"bart\":\n",
    "\t\ttokenizer = BartTokenizer.from_pretrained(bart_dir)\n",
    "\t\tmodel = BartForConditionalGeneration.from_pretrained(bart_dir)\n",
    "\t\tcontext_size = model.config.max_position_embeddings\n",
    "\n",
    "\tcase \"t5\":\n",
    "\t\ttokenizer = T5Tokenizer.from_pretrained(t5_dir)\n",
    "\t\tmodel = T5ForConditionalGeneration.from_pretrained(t5_dir)\n",
    "\t\tcontext_size = model.config.n_positions\n",
    "\n",
    "\tcase \"pegasus\":\n",
    "\t\ttokenizer = PegasusTokenizerFast.from_pretrained(pegasus_dir)\n",
    "\t\tmodel = PegasusForConditionalGeneration.from_pretrained(pegasus_dir)\n",
    "\t\tcontext_size = model.config.max_position_embeddings\n",
    "\n",
    "\tcase \"gpt\":\n",
    "\t\ttokenizer = GPT2TokenizerFast.from_pretrained(gpt_dir)\n",
    "\t\tmodel = \"gpt-3.5-turbo\"\n",
    "\t\tcontext_size = 4096\n",
    "\n",
    "context_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = TextProcessor(preprocessing=True)\n",
    "postprocessor = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BigPatent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts = []\n",
    "for file in bigpatent_files:\n",
    "\tfile_path = f\"{bigpatent_dir}/{file}\"\n",
    "\twith open(file_path) as fp:\n",
    "\t\tdata = json.load(fp)\n",
    "\tfor text in data[\"texts\"]:\n",
    "\t\tword_counts.append(count_words(text))\n",
    "\n",
    "plot_histogram(word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(word_counts), np.mean(word_counts), len(word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum([\n",
    "\t1\n",
    "\tfor count in word_counts\n",
    "\tif count > 40_000\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_words = 20_000\n",
    "max_words = inf\n",
    "max_texts = inf\n",
    "texts, summaries = [], []\n",
    "num_texts = 0\n",
    "for file in govreport_files:\n",
    "\tfile_path = f\"{govreport_dir}/{file}\"\n",
    "\twith open(file_path) as fp:\n",
    "\t\tdata = json.load(fp)\n",
    "\tif min_words < count_words(data[\"text\"]) < max_words:\n",
    "\t\ttexts.append(data[\"text\"])\n",
    "\t\tsummaries.append(data[\"summary\"])\n",
    "\t\tnum_texts += 1\n",
    "\tif num_texts == max_texts:\n",
    "\t\tbreak\n",
    "\n",
    "num_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_min_words = 20\n",
    "text_segmenter = TextSegmenter(nltk.sent_tokenize, segment_min_words)\n",
    "keywords_preprocessor = TextProcessor(\n",
    "\tonly_words_nums = True,\n",
    "\tremove_nums = True\n",
    ")\n",
    "stop_words = get_stop_words(extra_stop_words=STOP_WORDS)\n",
    "len(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_tokens_frac = .5\n",
    "min_summary_tokens = 300\n",
    "head_size = .5\n",
    "threshold = .7\n",
    "boost = .03\n",
    "num_keywords = 20\n",
    "seed = 69\n",
    "system_prompt = \"You will be given some segments of a very long document. Your task is to summarize the entire document as a whole by extracting key information and ideas from the segments. Generate a detailed, concise, and coherent summary in 300 words. Do not refer to the document in the summary in any way.\"\n",
    "\n",
    "temperature = 2.\n",
    "repetition_penalty = 3.\n",
    "top_p = .95\n",
    "\n",
    "encoders = [\n",
    "\tTruncateMiddle(\n",
    "\t\ttokenizer, context_size, 1, preprocessor\n",
    "\t),\n",
    "\tTruncateMiddle(\n",
    "\t\ttokenizer, context_size, head_size, preprocessor, True\n",
    "\t),\n",
    "\tUniformSampler(\n",
    "\t\ttokenizer, min_tokens_frac * context_size, context_size,\n",
    "\t\ttext_segmenter, preprocessor, True, seed\n",
    "\t),\n",
    "\tSegmentSampler(\n",
    "\t\ttokenizer, min_tokens_frac * context_size, context_size,\n",
    "\t\ttext_segmenter, sent_encoder, preprocessor, threshold, boost, seed\n",
    "\t),\n",
    "\tRemoveRedundancy(\n",
    "\t\ttokenizer, min_tokens_frac * context_size, context_size,\n",
    "\t\ttext_segmenter, sent_encoder, preprocessor, threshold, seed\n",
    "\t),\n",
    "\tKeywordScorer(\n",
    "\t\ttokenizer, context_size, text_segmenter, sent_encoder,\n",
    "\t\tpreprocessor, num_keywords, keywords_preprocessor, stop_words\n",
    "\t)\n",
    "]\n",
    "\n",
    "pipelines = [\n",
    "\tSummarizationPipeline(\n",
    "\t\tmodel, enc, postprocessor, min_summary_tokens,\n",
    "\t\tcontext_size, device, temperature, repetition_penalty, top_p\n",
    "\t) for enc in encoders\n",
    "] if model_name != \"gpt\" else [\n",
    "\tOpenAIPipeline(\n",
    "\t\tmodel, enc, postprocessor, system_prompt\n",
    "\t) for enc in encoders\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_texts = preprocessor(texts)\n",
    "threshold = .5\n",
    "num_segments_found = []\n",
    "for text in processed_texts:\n",
    "\tnum_segments = 0\n",
    "\tkeywords = get_keywords(text, 20, stop_words, keywords_preprocessor)\n",
    "\tkeywords = \" \".join(keywords)\n",
    "\tkeyword_emb = sent_encoder.encode(keywords)\n",
    "\tsegments = text_segmenter(text)\n",
    "\tsegment_embs = sent_encoder.encode(segments)\n",
    "\tfor segment, embedding in zip(segments, segment_embs):\n",
    "\t\tscore = keyword_emb @ embedding\n",
    "\t\tif score > threshold:\n",
    "\t\t\tnum_segments += 1\n",
    "\tnum_segments_found.append(num_segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sort(num_segments_found)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{data_dir}/pegasus-govreport.pkl\", \"rb\") as fp:\n",
    "\tresults = pickle.load(fp)\n",
    "scores = results[\"scores\"]\n",
    "sort1, sort2, sort3 = results[\"sort1\"], results[\"sort2\"], results[\"sort3\"]\n",
    "gen_summaries = results[\"gen_summaries\"]\n",
    "scores[0][sort1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = 0\n",
    "problem_text = results[\"texts\"][sort1[ind]]\n",
    "print(gen_summaries[sort1[ind]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{data_dir}/bart-bigpatent-times.json\") as fp:\n",
    "\tresults = json.load(fp)\n",
    "times = np.array(results[\"encoder_times\"])[1:]\n",
    "times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar([\n",
    "\t\"Truncate\\nMiddle\", \"Document\\nSkimming\",\n",
    "\t\"Skimming w/\\npost-sampling\\nremoval\",\n",
    "\t\"Skimming\\nw/ pre-\\nsampling\\nremoval\", \"Summarization\\nw/ Keyword\\nExtraction\"\n",
    "], times, color=\"green\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uccs-reu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
