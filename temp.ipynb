{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import nltk\n",
    "import stanza\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Fusce feugiat condimentum felis, id lacinia purus. Nullam vitae elit at magna tempus ultricies. Vestibulum ante ipsum primis in faucibus orci luctus et ultrices posuere cubilia Curae; Phasellus fermentum diam eu velit maximus tempus. Maecenas varius nunc non ligula malesuada, eget suscipit nisi hendrerit. Quisque vel odio nec mi dapibus pharetra. Sed ultricies, libero vel placerat convallis, quam justo accumsan odio, sit amet mattis nulla dui non nisi. Sed at eros ac nunc condimentum tristique. Donec ut justo id turpis facilisis gravida. Duis luctus felis eget velit hendrerit, in sollicitudin tortor vulputate. Suspendisse suscipit interdum diam, eu convallis purus tristique id. Nulla facilisi. Integer rutrum, ipsum ut malesuada tempor, est risus posuere odio, vel varius felis risus non nunc. Vivamus fermentum velit eu purus lacinia, eget commodo purus malesuada. Mauris ut varius felis. Curabitur ut magna non ante fringilla finibus. Integer mattis mi sed odio ullamcorper, ut eleifend enim varius. Nam lobortis ipsum quis libero eleifend, nec laoreet orci aliquam. Vivamus semper, arcu et rutrum consequat, eros dolor lobortis erat, eget posuere sapien ligula at leo. Proin sollicitudin metus nec orci suscipit, eget sodales dolor vehicula. Nullam id efficitur velit. Vestibulum ante ipsum primis in faucibus orci luctus et ultrices posuere cubilia Curae; Ut ac justo id magna vehicula commodo vel in nunc. Nunc vel magna nec risus lacinia sollicitudin at et arcu. In hac habitasse platea dictumst. Aenean non nisi in est varius elementum. Suspendisse vel tortor eget ligula sodales dignissim vel at risus. Curabitur sodales dolor nec nisl ullamcorper bibendum. Integer nec lorem ac lectus fermentum efficitur.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_tokenized = nltk.sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-10 13:09:01 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3968b012543848f59428f96121261be0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json:   0%|   â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-10 13:09:01 INFO: Downloaded file to /Users/naman/stanza_resources/resources.json\n",
      "2024-06-10 13:09:02 INFO: Loading these models for language: en (English):\n",
      "============================================\n",
      "| Processor    | Package                   |\n",
      "--------------------------------------------\n",
      "| tokenize     | combined                  |\n",
      "| mwt          | combined                  |\n",
      "| pos          | combined_charlm           |\n",
      "| lemma        | combined_nocharlm         |\n",
      "| constituency | ptb3-revised_charlm       |\n",
      "| depparse     | combined_charlm           |\n",
      "| sentiment    | sstplus_charlm            |\n",
      "| ner          | ontonotes-ww-multi_charlm |\n",
      "============================================\n",
      "\n",
      "2024-06-10 13:09:02 INFO: Using device: cpu\n",
      "2024-06-10 13:09:02 INFO: Loading: tokenize\n",
      "2024-06-10 13:09:02 INFO: Loading: mwt\n",
      "2024-06-10 13:09:02 INFO: Loading: pos\n",
      "2024-06-10 13:09:03 INFO: Loading: lemma\n",
      "2024-06-10 13:09:03 INFO: Loading: constituency\n",
      "2024-06-10 13:09:03 INFO: Loading: depparse\n",
      "2024-06-10 13:09:03 INFO: Loading: sentiment\n",
      "2024-06-10 13:09:03 INFO: Loading: ner\n",
      "2024-06-10 13:09:04 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "stanza_tokenizer = stanza.Pipeline(\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = stanza_tokenizer(text)\n",
    "stanza_tokenized = [sent.text for sent in doc.sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk_tokenized == stanza_tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## gov-report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7238"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = \"/Users/naman/Workspace/Data/UCCS-REU/gov-report/crs\"\n",
    "all_files = os.listdir(data_dir)\n",
    "len(all_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = all_files[1]\n",
    "file = f\"{data_dir}/{file}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['id', 'title', 'released_date', 'summary', 'reports'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(file) as fp:\n",
    "\tdata = json.load(fp)\n",
    "\n",
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['section_title', 'paragraphs', 'subsections'])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"reports\"][\"subsections\"][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_words = lambda text_list: len(\" \".join(text_list).split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_subsections(subsection):\n",
    "\twords = count_words(subsection[\"paragraphs\"])\n",
    "\tfor sub in subsection[\"subsections\"]:\n",
    "\t\twords += parse_subsections(sub)\n",
    "\treturn words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12474"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse_subsections(data[\"reports\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7564.149212489638, 451.45537441282124)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_words, avg_summary_words = 0, 0\n",
    "num_files = len(all_files)\n",
    "\n",
    "for file in all_files:\n",
    "\twith open(f\"{data_dir}/{file}\") as fp:\n",
    "\t\tdata = json.load(fp)\n",
    "\tavg_words += parse_subsections(data[\"reports\"])\t\n",
    "\tavg_summary_words += count_words(data[\"summary\"])\n",
    "\n",
    "avg_words / num_files, avg_summary_words / num_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12228"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = \"/Users/naman/Workspace/Data/UCCS-REU/gov-report/gao\"\n",
    "all_files = os.listdir(data_dir)\n",
    "len(all_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['id', 'url', 'title', 'published_date', 'released_date', 'highlight', 'report', 'fastfact'])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = all_files[0]\n",
    "\n",
    "with open(f\"{data_dir}/{file}\") as fp:\n",
    "\tdata = json.load(fp)\n",
    "\n",
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['section_title', 'paragraphs', 'subsections'])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"report\"][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9527"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = 0\n",
    "for report in data[\"report\"]:\n",
    "\twords += parse_subsections(report)\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8781"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse_subsections(data[\"report\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8521.906771344455"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = 0\n",
    "for file in all_files:\n",
    "\twith open(f\"{data_dir}/{file}\") as fp:\n",
    "\t\tdata = json.load(fp)\n",
    "\tfor report in data[\"report\"]:\n",
    "\t\twords += parse_subsections(report)\n",
    "words / len(all_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['## My ideas\\n', '\\n', 'Generating multiple words at a time in AR transformers to improve efficiency\\n', '\\n', 'Sentence level attention mechanism for summarizing long inputs\\n', '\\n', 'Compression of sentences (maybe by using discourse theory) and extraction for summarization\\n', '\\n', 'Summarize iteratively evaluating and improving each summary\\n', '\\n', 'Extraction based on topic words\\n', '\\n', 'QA with long input contexts by uniformly sampling sentences or phrases\\n', '\\n', 'Employ speed reading techniques\\n', 'Skimming: Quickly scan over the whole text skipping some parts\\n', 'Chunking: Break text into chunks and go over each one individually\\n', 'Pacing: Read many words at once\\n', 'Eliminating: Ignore redundant parts of text while reading\\n', '\\n', 'Summarizing books based on chapter names or title\\n', '\\n', 'Use 1D convolution to reduce number of word embeddings\\n', '\\n', 'Truncating from the middle instead of extremeties\\n', '\\n', '\\n', \"## Papers' ideas\\n\", '\\n', 'Long and short term memory in form of natural language for summarizing long inputs using prompt engineering\\n', '\\n', 'Use an LLM (chatgpt) to summarize and evaluate itslef using promp engineering\\n', '\\n', 'Summarizing multiple documents by encoding and clustering similar documents\\n', '\\n', '\\n', \"## Melkamu's papers (left out)\\n\", '2, 3, 6\\n']\n"
     ]
    }
   ],
   "source": [
    "with open(\"ideas.txt\") as fp:\n",
    "\tfile_iter = iter(fp.readlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uccs-reu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
