{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sys import getsizeof\n",
    "from time import sleep\n",
    "import json\n",
    "import re\n",
    "import inspect\n",
    "from warnings import filterwarnings\n",
    "\n",
    "import numpy as np\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from transformers import (\n",
    "\tBartTokenizer, BartForConditionalGeneration,\n",
    "\tT5Tokenizer, T5ForConditionalGeneration,\n",
    "\tPegasusForConditionalGeneration, PegasusTokenizerFast,\n",
    "\tGPT2TokenizerFast\n",
    ")\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from utils.helpers import *\n",
    "from utils.encoders import *\n",
    "from utils.pipelines import *\n",
    "from utils.trainer_utils import *\n",
    "from utils.evaluator_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inf = float(\"inf\")\n",
    "filterwarnings(\"ignore\")\n",
    "device = get_device()\n",
    "# device = \"cpu\"\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "crs files: 7238, gao files: 12228\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"/Users/naman/Workspace/Data/Long-Document-Summarization\"\n",
    "data_dir = \"/home/nchibbar/Data\"\n",
    "\n",
    "crs_files = os.listdir(crs_dir := f\"{data_dir}/GovReport/crs\")\n",
    "gao_files = os.listdir(gao_dir := f\"{data_dir}/GovReport/gao\")\n",
    "\n",
    "print(f\"crs files: {len(crs_files)}, gao files: {len(gao_files)}\")\n",
    "\n",
    "out_dir = f\"{data_dir}/GovReport/processed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1024, 4096, 512)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sentence transformer\n",
    "# Automatically loads into gpu if available\n",
    "sent_dir = f\"{data_dir}/Models/Sent-Transformer\"\n",
    "sent_encoder = SentenceTransformer(sent_dir).to(\"cpu\")\n",
    "\n",
    "# BART\n",
    "bart_dir = f\"{data_dir}/Models/BART\"\n",
    "bart_tokenizer = BartTokenizer.from_pretrained(bart_dir)\n",
    "bart_model = BartForConditionalGeneration.from_pretrained(bart_dir)\n",
    "bart_context_size = bart_model.config.max_position_embeddings\n",
    "\n",
    "# T5\n",
    "t5_dir = f\"{data_dir}/Models/T5\"\n",
    "t5_tokenizer = T5Tokenizer.from_pretrained(t5_dir)\n",
    "t5_model = T5ForConditionalGeneration.from_pretrained(t5_dir)\n",
    "t5_context_size = t5_model.config.n_positions\n",
    "\n",
    "# Pegasus\n",
    "pegasus_dir = f\"{data_dir}/Models/PEGASUS\"\n",
    "pegasus_tokenizer = PegasusTokenizerFast.from_pretrained(pegasus_dir)\n",
    "pegasus_model = PegasusForConditionalGeneration.from_pretrained(pegasus_dir)\n",
    "pegasus_context_size = pegasus_model.config.max_position_embeddings\n",
    "\n",
    "# GPT 3.5 turbo tokenizer\n",
    "gpt_dir = f\"{data_dir}/Models/GPT-3.5-turbo-tokenizer\"\n",
    "gpt_tokenizer = GPT2TokenizerFast.from_pretrained(gpt_dir)\n",
    "gpt_model = \"gpt-3.5-turbo\"\n",
    "gpt_context_size = 4096\n",
    "\n",
    "bart_context_size, t5_context_size, pegasus_context_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = TextProcessor(preprocessing=True)\n",
    "postprocessor = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GovReport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_subsections(sections):\n",
    "\ttext = \"\"\n",
    "\tfor sec in sections:\n",
    "\t\tsec_text = \"\\n\\n\".join(sec[\"paragraphs\"])\n",
    "\t\tif sec[\"section_title\"]:\n",
    "\t\t\tsec_text = f\"Section {sec[\"section_title\"]}:\\n\\n{sec_text}\"\n",
    "\t\ttext = f\"{text}\\n\\n{sec_text}\" if text else sec_text\n",
    "\t\tif sec[\"subsections\"]:\n",
    "\t\t\tsub_text = combine_subsections(sec[\"subsections\"])\n",
    "\t\t\ttext = f\"{text}\\n\\n{sub_text}\" if text else sub_text\n",
    "\treturn text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_crs_files = len(crs_files)\n",
    "for i, file in enumerate(crs_files):\n",
    "\tfull_path = os.path.join(crs_dir, file)\n",
    "\twith open(full_path) as fp:\n",
    "\t\tdata = json.load(fp)\n",
    "\tclear_stdout()\n",
    "\tprint(f\"{num_crs_files - i} files left\", end=\"\")\n",
    "\ttext = f\"{data[\"title\"]}\\n\\n\"\n",
    "\ttext += combine_subsections([data[\"reports\"]])\n",
    "\tsummary = \" \".join(data[\"summary\"])\n",
    "\tsummary = preprocessor.process(summary)\n",
    "\twith open(f\"{out_dir}/{file}\", \"w\") as fp:\n",
    "\t\tjson.dump({\n",
    "\t\t\t\"text\": text,\n",
    "\t\t\t\"summary\": summary\n",
    "\t\t}, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in gao_files:\n",
    "\tfile = os.path.join(gao_dir, file)\n",
    "\twith open(file) as fp:\n",
    "\t\tdata = json.load(fp)\n",
    "\ttext = combine_subsections(data[\"report\"])\n",
    "\ttext = preprocessor.process(text)\n",
    "\tprint(data[\"highlight\"])\n",
    "\tsummary = \"\\n\".join(data[\"highlight\"])\n",
    "\tsummary = preprocessor.process(summary)\n",
    "\twith open(f\"{out_dir}/{file}\", \"w\") as fp:\n",
    "\t\tjson.dump({\n",
    "\t\t\t\"text\": text,\n",
    "\t\t\t\"summary\": summary\n",
    "\t\t}, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BigPatent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigpatent_dir = f\"{data_dir}/BigPatent/train/a\"\n",
    "\n",
    "bigpatent_files = os.listdir(bigpatent_dir)\n",
    "\n",
    "word_counts = []\n",
    "for file in bigpatent_files:\n",
    "\twith open(f\"{bigpatent_dir}/{file}\") as fp:\n",
    "\t\tfor line in fp.readlines():\n",
    "\t\t\tdata = json.loads(line)\n",
    "\t\t\ttext = data[\"description\"]\n",
    "\t\t\tword_counts.append(count_words(text))\n",
    "\n",
    "bins = int(len(word_counts)**.5)\n",
    "plt.hist(word_counts, bins=bins)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(word_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_words = 70_000\n",
    "max_words = inf\n",
    "max_texts = 10\n",
    "texts, summaries = [], []\n",
    "num_texts = 0\n",
    "for file in crs_files:\n",
    "\twith open(f\"{out_dir}/{file}\") as fp:\n",
    "\t\tdata = json.load(fp)\n",
    "\tif min_words < count_words(data[\"text\"]) < max_words:\n",
    "\t\ttexts.append(data[\"text\"])\n",
    "\t\tsummaries.append(data[\"summary\"])\n",
    "\t\tnum_texts += 1\n",
    "\tif num_texts == max_texts:\n",
    "\t\tbreak\n",
    "\n",
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_min_words = 20\n",
    "text_segmenter = TextSegmenter(nltk.sent_tokenize, segment_min_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_tokens_frac = .5\n",
    "min_summary_tokens = 400\n",
    "head_size = .5\n",
    "threshold = .7\n",
    "boost = .03\n",
    "seed = 69\n",
    "system_prompt = \"You will be given some segments of a very long document. Your task is to summarize the entire document as a whole by extracting key information and ideas from the segments. Generate a detailed, concise, and coherent summary in 500 words. Do not refer to the document in the summary in any way.\"\n",
    "\n",
    "sent_encoder.to(device)\n",
    "\n",
    "bart_encoders = [\n",
    "\tTruncateMiddle(\n",
    "\t\tbart_tokenizer, bart_context_size, head_size, preprocessor, True\n",
    "\t),\n",
    "\tUniformSampler(\n",
    "\t\tbart_tokenizer, min_tokens_frac * bart_context_size, bart_context_size,\n",
    "\t\ttext_segmenter, preprocessor, True, seed\n",
    "\t),\n",
    "\tSegmentSampler(\n",
    "\t\tbart_tokenizer, min_tokens_frac * bart_context_size, bart_context_size,\n",
    "\t\ttext_segmenter, sent_encoder, preprocessor, True, threshold, boost, seed\n",
    "\t),\n",
    "\tRemoveRedundancy(\n",
    "\t\tbart_tokenizer, min_tokens_frac * bart_context_size, bart_context_size,\n",
    "\t\ttext_segmenter, sent_encoder, preprocessor, True, threshold, seed\n",
    "\t)\n",
    "]\n",
    "t5_encoders = [\n",
    "\tTruncateMiddle(\n",
    "\t\tt5_tokenizer, t5_context_size, head_size, preprocessor, True\n",
    "\t),\n",
    "\tUniformSampler(\n",
    "\t\tt5_tokenizer, min_tokens_frac * bart_context_size, t5_context_size,\n",
    "\t\ttext_segmenter, preprocessor, True, seed\n",
    "\t),\n",
    "\tSegmentSampler(\n",
    "\t\tt5_tokenizer, min_tokens_frac * bart_context_size, t5_context_size,\n",
    "\t\ttext_segmenter, sent_encoder, preprocessor, True, threshold, boost, seed\n",
    "\t),\n",
    "\tRemoveRedundancy(\n",
    "\t\tt5_tokenizer, min_tokens_frac * bart_context_size, t5_context_size,\n",
    "\t\ttext_segmenter, sent_encoder, preprocessor, True, threshold, seed\n",
    "\t)\n",
    "]\n",
    "gpt_encoders = [\n",
    "\tTruncateMiddle(\n",
    "\t\tgpt_tokenizer, gpt_context_size, head_size, preprocessor, True\n",
    "\t),\n",
    "\tUniformSampler(\n",
    "\t\tgpt_tokenizer, min_tokens_frac * gpt_context_size, gpt_context_size,\n",
    "\t\ttext_segmenter, preprocessor, True, seed\n",
    "\t),\n",
    "\tSegmentSampler(\n",
    "\t\tgpt_tokenizer, min_tokens_frac * gpt_context_size, gpt_context_size,\n",
    "\t\ttext_segmenter, sent_encoder, preprocessor, True, threshold, boost, seed\n",
    "\t),\n",
    "\tRemoveRedundancy(\n",
    "\t\tgpt_tokenizer, min_tokens_frac * gpt_context_size, gpt_context_size,\n",
    "\t\ttext_segmenter, sent_encoder, preprocessor, True, threshold, seed\n",
    "\t)\n",
    "]\n",
    "bart_pipelines = [\n",
    "\tSummarizationPipeline(\n",
    "\t\tbart_model, enc, postprocessor, min_summary_tokens,\n",
    "\t\tbart_context_size, device\n",
    "\t) for enc in bart_encoders\n",
    "]\n",
    "t5_pipelines = [\n",
    "\tSummarizationPipeline(\n",
    "\t\tt5_model, enc, postprocessor, min_summary_tokens,\n",
    "\t\tt5_context_size, device\n",
    "\t) for enc in t5_encoders\n",
    "]\n",
    "gpt_pipelines = [\n",
    "\tOpenAIPipeline(\n",
    "\t\tgpt_model, enc, system_prompt=system_prompt\n",
    "\t) for enc in gpt_encoders\n",
    "]\n",
    "pipelines = bart_pipelines + t5_pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encodings1 = bart_encoders[1](texts, return_batch=False)\n",
    "encodings2 = bart_encoders[2](texts, return_batch=False)\n",
    "\n",
    "token_lengths1 = [len(enc) for enc in encodings1]\n",
    "token_lengths2 = [len(enc) for enc in encodings2]\n",
    "\n",
    "avg_tokens1 = np.mean(token_lengths1)\n",
    "avg_tokens2 = np.mean(token_lengths2)\n",
    "\n",
    "avg_tokens1, avg_tokens2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, text in enumerate(texts):\n",
    "\tprint(f\"Processing text {i + 1}\")\n",
    "\tbart_encoders[3](text, return_batch=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = preprocessor(texts[3])\n",
    "text = text_segmenter(text)\n",
    "\n",
    "[count_words(seg) for seg in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "73246"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = texts[1]\n",
    "text = preprocessor(text)\n",
    "count_words(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "392"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_stop_words = [\n",
    "\t\"also\", \"however\", \"therefore\", \"thus\", \"hence\", \"moreover\",\n",
    "\t\"must\", \"may\", \"might\", \"could\", \"would\", \"shall\", \"need\",\n",
    "\t\"needs\", \"given\", \"since\", \"though\",\n",
    "]\n",
    "for word in my_stop_words:\n",
    "\tif word in nltk.corpus.stopwords.words(\"english\"):\n",
    "\t\tprint(word)\n",
    "\n",
    "stop_words = nltk.corpus.stopwords.words(\"english\") + my_stop_words\n",
    "stop_words += [\n",
    "\tword.capitalize()\n",
    "\tfor word in stop_words\n",
    "\tif not word.istitle()\n",
    "]\n",
    "\n",
    "len(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['financial',\n",
       " 'billion',\n",
       " 'crisis',\n",
       " 'US',\n",
       " 'countries',\n",
       " 'government',\n",
       " 'economic',\n",
       " 'banks',\n",
       " 'IMF',\n",
       " 'China',\n",
       " 'global',\n",
       " 'United',\n",
       " 'economy',\n",
       " 'Bank',\n",
       " 'markets',\n",
       " 'market',\n",
       " 'growth',\n",
       " 'States',\n",
       " 'European',\n",
       " 'stimulus']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords_preprocessor = TextProcessor(\n",
    "\tonly_words_nums = True,\n",
    "\tremove_nums = True\n",
    ")\n",
    "\n",
    "text_keywords = get_keywords(\n",
    "\ttext,\n",
    "\tstop_words = stop_words,\n",
    "\tpreprocessor = keywords_preprocessor\n",
    ")\n",
    "\n",
    "text_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5087323,\n",
       " 0.31412268,\n",
       " 0.42385107,\n",
       " 0.38462245,\n",
       " 0.39949688,\n",
       " 0.40291756,\n",
       " 0.4064793,\n",
       " 0.3320877,\n",
       " 0.2817063,\n",
       " 0.3819891,\n",
       " 0.43558234,\n",
       " 0.3503595,\n",
       " 0.34029588,\n",
       " 0.39509365,\n",
       " 0.33220628,\n",
       " 0.48161525,\n",
       " 0.39366293,\n",
       " 0.2780938,\n",
       " 0.39710695,\n",
       " 0.46799344,\n",
       " 0.376144,\n",
       " 0.26847208,\n",
       " 0.12157059,\n",
       " 0.2683338,\n",
       " 0.39711612,\n",
       " 0.49235654,\n",
       " 0.43655294,\n",
       " 0.55438876,\n",
       " 0.46940482,\n",
       " 0.35324788,\n",
       " 0.4461823,\n",
       " 0.3173254,\n",
       " 0.41072786,\n",
       " 0.36700398,\n",
       " 0.41445583,\n",
       " 0.35214165,\n",
       " 0.1902087,\n",
       " 0.286025,\n",
       " 0.13096064,\n",
       " 0.20859425,\n",
       " 0.03328281,\n",
       " 0.2003568,\n",
       " 0.1734933,\n",
       " 0.16377819,\n",
       " 0.11001223,\n",
       " 0.13316962,\n",
       " 0.2011598,\n",
       " 0.46279424,\n",
       " 0.4596409,\n",
       " 0.42880607,\n",
       " 0.45569068,\n",
       " 0.42798108,\n",
       " 0.23360702,\n",
       " 0.46032944,\n",
       " 0.35720992,\n",
       " 0.40242657,\n",
       " 0.26050115,\n",
       " 0.40356386,\n",
       " 0.40613547,\n",
       " 0.31893802,\n",
       " 0.3422554,\n",
       " 0.40949938,\n",
       " 0.39509344,\n",
       " 0.36719924,\n",
       " 0.35781807,\n",
       " 0.28919172,\n",
       " 0.21282825,\n",
       " 0.33405596,\n",
       " 0.37637442,\n",
       " 0.26593393,\n",
       " 0.25659642,\n",
       " 0.27755123,\n",
       " 0.41967025,\n",
       " 0.35555887,\n",
       " 0.4852451,\n",
       " 0.28947666,\n",
       " 0.28974143,\n",
       " 0.18432653,\n",
       " 0.37879178,\n",
       " 0.23786478,\n",
       " 0.26357788,\n",
       " 0.4382711,\n",
       " 0.37483197,\n",
       " 0.21912406,\n",
       " 0.27804458,\n",
       " 0.39401478,\n",
       " 0.2873729,\n",
       " 0.25304973,\n",
       " 0.3794393,\n",
       " 0.39727372,\n",
       " 0.33577463,\n",
       " 0.27026582,\n",
       " 0.1637924,\n",
       " 0.15213923,\n",
       " 0.21754207,\n",
       " 0.16760717,\n",
       " 0.17282684,\n",
       " 0.3229844,\n",
       " 0.34758306,\n",
       " 0.13384694,\n",
       " 0.048473828,\n",
       " 0.34960544,\n",
       " 0.22242515,\n",
       " 0.22825646,\n",
       " 0.3348443,\n",
       " 0.27898648,\n",
       " 0.31374612,\n",
       " 0.14956266,\n",
       " 0.056296997,\n",
       " 0.32293192,\n",
       " 0.17215723,\n",
       " 0.32287347,\n",
       " 0.27351186,\n",
       " 0.18816555,\n",
       " 0.24068779,\n",
       " 0.18263258,\n",
       " 0.10348981,\n",
       " 0.14131373,\n",
       " 0.4929791,\n",
       " 0.4000976,\n",
       " 0.4985898,\n",
       " 0.09458536,\n",
       " 0.37636453,\n",
       " 0.2722938,\n",
       " 0.3102218,\n",
       " 0.41189283,\n",
       " 0.44167185,\n",
       " 0.33403504,\n",
       " 0.23856987,\n",
       " 0.3311722,\n",
       " 0.4404965,\n",
       " 0.3229108,\n",
       " 0.37261266,\n",
       " 0.34145588,\n",
       " 0.42462665,\n",
       " 0.471222,\n",
       " 0.40883392,\n",
       " 0.36831534,\n",
       " 0.4302622,\n",
       " 0.4077353,\n",
       " 0.52973795,\n",
       " 0.4631607,\n",
       " 0.4298259,\n",
       " 0.48946333,\n",
       " 0.34705243,\n",
       " 0.37997758,\n",
       " 0.56079274,\n",
       " 0.50713664,\n",
       " 0.45688933,\n",
       " 0.49065977,\n",
       " 0.45283395,\n",
       " 0.39693373,\n",
       " 0.5750641,\n",
       " 0.39457023,\n",
       " 0.31770992,\n",
       " 0.38171586,\n",
       " 0.5511464,\n",
       " 0.44178474,\n",
       " 0.39627004,\n",
       " 0.4096117,\n",
       " 0.46373716,\n",
       " 0.41876245,\n",
       " 0.37962034,\n",
       " 0.13923934,\n",
       " 0.30125046,\n",
       " 0.13727273,\n",
       " 0.24202406,\n",
       " 0.22749127,\n",
       " 0.3691555,\n",
       " 0.279463,\n",
       " 0.30930263,\n",
       " 0.2668606,\n",
       " 0.26970226,\n",
       " 0.32877994,\n",
       " 0.37608546,\n",
       " 0.32160547,\n",
       " 0.3387722,\n",
       " 0.41854507,\n",
       " 0.23730314,\n",
       " 0.3918403,\n",
       " 0.28956345,\n",
       " 0.30171928,\n",
       " 0.21051678,\n",
       " 0.2697056,\n",
       " 0.35990834,\n",
       " 0.3935256,\n",
       " 0.31980008,\n",
       " 0.26196414,\n",
       " 0.25414404,\n",
       " 0.38653156,\n",
       " 0.24863975,\n",
       " 0.0736899,\n",
       " 0.19109601,\n",
       " 0.14411376,\n",
       " 0.22690034,\n",
       " 0.39995724,\n",
       " 0.23029938,\n",
       " 0.41358164,\n",
       " 0.24798518,\n",
       " 0.17796546,\n",
       " 0.25076562,\n",
       " 0.35942107,\n",
       " 0.3841722,\n",
       " 0.3090496,\n",
       " 0.26392263,\n",
       " 0.47911355,\n",
       " 0.09801164,\n",
       " 0.21744199,\n",
       " 0.30556566,\n",
       " 0.20570093,\n",
       " 0.22066562,\n",
       " 0.29121247,\n",
       " 0.15089124,\n",
       " 0.14725143,\n",
       " 0.34574682,\n",
       " 0.23364764,\n",
       " 0.28164148,\n",
       " 0.16044018,\n",
       " 0.17038572,\n",
       " 0.24784128,\n",
       " 0.36885473,\n",
       " 0.2600486,\n",
       " 0.20988026,\n",
       " 0.43090504,\n",
       " 0.3544895,\n",
       " 0.5667749,\n",
       " 0.18283617,\n",
       " 0.39985463,\n",
       " 0.47660303,\n",
       " 0.24196872,\n",
       " 0.1991268,\n",
       " 0.17502627,\n",
       " 0.30068636,\n",
       " 0.15620106,\n",
       " 0.38859195,\n",
       " 0.2953693,\n",
       " 0.49468842,\n",
       " 0.37151182,\n",
       " 0.35553488,\n",
       " 0.32858387,\n",
       " 0.21652383,\n",
       " 0.4420139,\n",
       " 0.5076634,\n",
       " 0.32980078,\n",
       " 0.38571826,\n",
       " 0.12406041,\n",
       " 0.44402114,\n",
       " 0.41399944,\n",
       " 0.38848293,\n",
       " 0.44161424,\n",
       " 0.23784587,\n",
       " 0.3751307,\n",
       " 0.2395604,\n",
       " 0.36719924,\n",
       " 0.24933562,\n",
       " 0.113324,\n",
       " 0.21641368,\n",
       " 0.3009936,\n",
       " 0.31904796,\n",
       " 0.39341742,\n",
       " 0.23543887,\n",
       " 0.28369096,\n",
       " 0.23977193,\n",
       " 0.47981894,\n",
       " 0.28499323,\n",
       " 0.24256295,\n",
       " 0.17709441,\n",
       " 0.42834872,\n",
       " 0.353978,\n",
       " 0.4503655,\n",
       " 0.23071933,\n",
       " 0.3606543,\n",
       " 0.3485161,\n",
       " 0.30934048,\n",
       " 0.38397366,\n",
       " 0.35039356,\n",
       " 0.3689267,\n",
       " 0.3203634,\n",
       " 0.38020086,\n",
       " 0.39725536,\n",
       " 0.37260514,\n",
       " 0.518655,\n",
       " 0.48262194,\n",
       " 0.2317958,\n",
       " 0.27864146,\n",
       " 0.29770672,\n",
       " 0.2367464,\n",
       " 0.21320029,\n",
       " 0.30274597,\n",
       " 0.23660925,\n",
       " 0.4085821,\n",
       " 0.39033943,\n",
       " 0.34636864,\n",
       " 0.3173972,\n",
       " 0.41979796,\n",
       " 0.26247323,\n",
       " 0.23003845,\n",
       " 0.29793805,\n",
       " 0.2257806,\n",
       " 0.24380276,\n",
       " 0.2677898,\n",
       " 0.2969756,\n",
       " 0.20361795,\n",
       " 0.3080602,\n",
       " 0.0399938,\n",
       " 0.3492186,\n",
       " 0.42160204,\n",
       " 0.2158929,\n",
       " 0.08282893,\n",
       " 0.37807617,\n",
       " 0.36893833,\n",
       " 0.21381366,\n",
       " 0.30965585,\n",
       " 0.34257478,\n",
       " 0.3604012,\n",
       " 0.36100405,\n",
       " 0.1976141,\n",
       " 0.22226305,\n",
       " 0.2643776,\n",
       " 0.13124698,\n",
       " 0.30283833,\n",
       " 0.42315167,\n",
       " 0.16764727,\n",
       " 0.29229444,\n",
       " 0.49726248,\n",
       " 0.47417167,\n",
       " 0.44225472,\n",
       " 0.43377408,\n",
       " 0.44967377,\n",
       " 0.32331103,\n",
       " 0.27819633,\n",
       " 0.5131389,\n",
       " 0.28804982,\n",
       " 0.2871557,\n",
       " 0.2763864,\n",
       " 0.27143836,\n",
       " 0.27672467,\n",
       " 0.14383984,\n",
       " 0.23193094,\n",
       " 0.1458013,\n",
       " 0.19403347,\n",
       " 0.046821453,\n",
       " 0.19985145,\n",
       " 0.23392418,\n",
       " 0.24194227,\n",
       " 0.2733429,\n",
       " 0.21375373,\n",
       " 0.30899495,\n",
       " 0.26887622,\n",
       " 0.18940367,\n",
       " 0.2875014,\n",
       " 0.20156556,\n",
       " 0.27774957,\n",
       " 0.19566986,\n",
       " 0.22388312,\n",
       " 0.1630246,\n",
       " 0.22810127,\n",
       " 0.32251143,\n",
       " 0.073605634,\n",
       " 0.2929774,\n",
       " 0.29931518,\n",
       " 0.24005903,\n",
       " 0.35941225,\n",
       " 0.49166426,\n",
       " 0.34017372,\n",
       " 0.38340914,\n",
       " 0.2544009,\n",
       " 0.3339341,\n",
       " 0.257999,\n",
       " 0.3405336,\n",
       " 0.13309078,\n",
       " 0.28308034,\n",
       " 0.22292893,\n",
       " 0.2902625,\n",
       " 0.21179165,\n",
       " 0.32251757,\n",
       " 0.37025723,\n",
       " 0.37228814,\n",
       " 0.40527388,\n",
       " 0.35933954,\n",
       " 0.24013416,\n",
       " 0.14057177,\n",
       " 0.4577812,\n",
       " 0.4045544,\n",
       " 0.41526592,\n",
       " 0.41585827,\n",
       " 0.42096013,\n",
       " 0.37651554,\n",
       " 0.35231423,\n",
       " 0.31814915,\n",
       " 0.33346385,\n",
       " 0.3683831,\n",
       " 0.31216434,\n",
       " 0.19583587,\n",
       " 0.53920525,\n",
       " 0.45596832,\n",
       " 0.44047958,\n",
       " 0.5690793,\n",
       " 0.44353303,\n",
       " 0.47167388,\n",
       " 0.4810314,\n",
       " 0.28312612,\n",
       " 0.4815026,\n",
       " 0.5073185,\n",
       " 0.37768662,\n",
       " 0.30153272,\n",
       " 0.45669323,\n",
       " 0.30083978,\n",
       " 0.3095014,\n",
       " 0.41456646,\n",
       " 0.48832393,\n",
       " 0.37523907,\n",
       " 0.25037438,\n",
       " 0.42955074,\n",
       " 0.451535,\n",
       " 0.40212336,\n",
       " 0.40164575,\n",
       " 0.3302023,\n",
       " 0.39020604,\n",
       " 0.29466206,\n",
       " 0.42134637,\n",
       " 0.36394137,\n",
       " 0.5106626,\n",
       " 0.34372562,\n",
       " 0.44659364,\n",
       " 0.41968384,\n",
       " 0.42456955,\n",
       " 0.24487135,\n",
       " 0.43009126,\n",
       " 0.52337193,\n",
       " 0.46807504,\n",
       " 0.43750522,\n",
       " 0.38722533,\n",
       " 0.2722997,\n",
       " 0.22473636,\n",
       " 0.34968737,\n",
       " 0.26961857,\n",
       " 0.38137543,\n",
       " 0.31289378,\n",
       " 0.2902245,\n",
       " 0.40684563,\n",
       " 0.32625866,\n",
       " 0.3395459,\n",
       " 0.27098235,\n",
       " 0.38168156,\n",
       " 0.32229137,\n",
       " 0.39151424,\n",
       " 0.26000637,\n",
       " 0.37525326,\n",
       " 0.28827,\n",
       " 0.36470753,\n",
       " 0.2910508,\n",
       " 0.15549561,\n",
       " 0.32216066,\n",
       " 0.35679206,\n",
       " 0.35947356,\n",
       " 0.24895477,\n",
       " 0.3147114,\n",
       " 0.38427544,\n",
       " 0.37469605,\n",
       " 0.2385203,\n",
       " 0.23140453,\n",
       " 0.35748208,\n",
       " 0.3386115,\n",
       " 0.24903892,\n",
       " 0.1663216,\n",
       " 0.3945645,\n",
       " 0.32433724,\n",
       " 0.28919542,\n",
       " 0.41899574,\n",
       " 0.4964751,\n",
       " 0.5410372,\n",
       " 0.36992705,\n",
       " 0.31702262,\n",
       " 0.33265182,\n",
       " 0.49783385,\n",
       " 0.42449933,\n",
       " 0.3965884,\n",
       " 0.28428665,\n",
       " 0.28894353,\n",
       " 0.32540774,\n",
       " 0.3250237,\n",
       " 0.24304214,\n",
       " 0.39047706,\n",
       " 0.28549713,\n",
       " 0.22447354,\n",
       " 0.33271873,\n",
       " 0.36603767,\n",
       " 0.36796325,\n",
       " 0.27698123,\n",
       " 0.24312983,\n",
       " 0.3399748,\n",
       " 0.34584162,\n",
       " 0.290744,\n",
       " 0.33460882,\n",
       " 0.52508295,\n",
       " 0.44140527,\n",
       " 0.3260233,\n",
       " 0.35194045,\n",
       " 0.4397691,\n",
       " 0.39811602,\n",
       " 0.3507874,\n",
       " 0.20585531,\n",
       " 0.46711886,\n",
       " 0.3192035,\n",
       " 0.23556559,\n",
       " 0.33847925,\n",
       " 0.2926246,\n",
       " 0.29988682,\n",
       " 0.32428992,\n",
       " 0.3180747,\n",
       " 0.30397427,\n",
       " 0.28602186,\n",
       " 0.24675907,\n",
       " 0.24198675,\n",
       " 0.354793,\n",
       " 0.43046832,\n",
       " 0.44721603,\n",
       " 0.26786366,\n",
       " 0.29076147,\n",
       " 0.45870426,\n",
       " 0.4004504,\n",
       " 0.34929514,\n",
       " 0.29134244,\n",
       " 0.17857811,\n",
       " 0.31167293,\n",
       " 0.19920173,\n",
       " 0.16079618,\n",
       " 0.3526315,\n",
       " 0.3296047,\n",
       " 0.32418394,\n",
       " 0.333682,\n",
       " 0.40011948,\n",
       " 0.24521713,\n",
       " 0.19754985,\n",
       " 0.11569284,\n",
       " 0.14874676,\n",
       " 0.2836073,\n",
       " 0.18282293,\n",
       " 0.28553212,\n",
       " 0.41477478,\n",
       " 0.34594357,\n",
       " 0.2789758,\n",
       " 0.2631963,\n",
       " 0.20204966,\n",
       " 0.332877,\n",
       " 0.37748557,\n",
       " 0.4068169,\n",
       " 0.26231357,\n",
       " 0.17822686,\n",
       " 0.18959746,\n",
       " 0.33035174,\n",
       " 0.3911072,\n",
       " 0.41980696,\n",
       " 0.3885762,\n",
       " 0.229683,\n",
       " 0.42577633,\n",
       " 0.45841682,\n",
       " 0.46149242,\n",
       " 0.22350706,\n",
       " 0.3774894,\n",
       " 0.45947915,\n",
       " 0.30547187,\n",
       " 0.2515917,\n",
       " 0.355797,\n",
       " 0.28469846,\n",
       " 0.32513672,\n",
       " 0.3129661,\n",
       " 0.18971121,\n",
       " 0.11361292,\n",
       " 0.13094674,\n",
       " 0.43164024,\n",
       " 0.19624566,\n",
       " 0.20704359,\n",
       " 0.29209858,\n",
       " 0.35020506,\n",
       " 0.36893842,\n",
       " 0.19531383,\n",
       " 0.2366201,\n",
       " 0.2596289,\n",
       " 0.14696397,\n",
       " 0.11728521,\n",
       " 0.2382282,\n",
       " 0.30614758,\n",
       " 0.24748787,\n",
       " 0.45696023,\n",
       " 0.21638149,\n",
       " 0.40524352,\n",
       " 0.39986947,\n",
       " 0.41659284,\n",
       " 0.36357194,\n",
       " 0.31767297,\n",
       " 0.35655636,\n",
       " 0.26699594,\n",
       " 0.34927076,\n",
       " 0.34886762,\n",
       " 0.5185213,\n",
       " 0.3897953,\n",
       " 0.23183067,\n",
       " 0.24787648,\n",
       " 0.2450014,\n",
       " 0.29239595,\n",
       " 0.28969812,\n",
       " 0.3675253,\n",
       " 0.3471036,\n",
       " 0.43087304,\n",
       " 0.35067332,\n",
       " 0.4113684,\n",
       " 0.55107605,\n",
       " 0.3210041,\n",
       " 0.36204895,\n",
       " 0.3368917,\n",
       " 0.44059184,\n",
       " 0.40482366,\n",
       " 0.5023595,\n",
       " 0.3629235,\n",
       " 0.14865065,\n",
       " 0.31608677,\n",
       " 0.33772397,\n",
       " 0.43145758,\n",
       " 0.38489464,\n",
       " 0.14262001,\n",
       " 0.28924513,\n",
       " 0.3226444,\n",
       " 0.23536749,\n",
       " 0.2875739,\n",
       " 0.33270687,\n",
       " 0.21619292,\n",
       " 0.28504348,\n",
       " 0.18821436,\n",
       " 0.31629318,\n",
       " 0.35812756,\n",
       " 0.42202562,\n",
       " 0.24155474,\n",
       " 0.23905739,\n",
       " 0.5259211,\n",
       " 0.4274422,\n",
       " 0.34057835,\n",
       " 0.34903434,\n",
       " 0.31418657,\n",
       " 0.41506058,\n",
       " 0.49640203,\n",
       " 0.30860937,\n",
       " 0.3374599,\n",
       " 0.31446522,\n",
       " 0.11117572,\n",
       " 0.22558498,\n",
       " 0.38936505,\n",
       " 0.44164163,\n",
       " 0.24099624,\n",
       " 0.46335647,\n",
       " 0.2829821,\n",
       " 0.35485613,\n",
       " 0.43856484,\n",
       " 0.36797756,\n",
       " 0.3088317,\n",
       " 0.37832072,\n",
       " 0.21184054,\n",
       " 0.36709782,\n",
       " 0.27953142,\n",
       " 0.43929178,\n",
       " 0.3193454,\n",
       " 0.49988538,\n",
       " 0.2897069,\n",
       " 0.2878096,\n",
       " 0.21957932,\n",
       " 0.28585637,\n",
       " 0.24773784,\n",
       " 0.29428047,\n",
       " 0.39458364,\n",
       " 0.24583697,\n",
       " 0.22499472,\n",
       " 0.3221163,\n",
       " 0.30775592,\n",
       " 0.19329849,\n",
       " 0.31216913,\n",
       " 0.30545035,\n",
       " 0.004022332,\n",
       " 0.12706065,\n",
       " 0.19045474,\n",
       " 0.19904417,\n",
       " 0.22219999,\n",
       " 0.20812288,\n",
       " 0.10290003,\n",
       " 0.13961343,\n",
       " 0.25386143,\n",
       " 0.47098464,\n",
       " 0.2524374,\n",
       " 0.41438293,\n",
       " 0.067723185,\n",
       " 0.24812037,\n",
       " 0.12054195,\n",
       " 0.23673978,\n",
       " 0.33184803,\n",
       " 0.30370098,\n",
       " 0.35030264,\n",
       " 0.31574,\n",
       " 0.28653544,\n",
       " 0.20691684,\n",
       " 0.164206,\n",
       " 0.37803918,\n",
       " 0.37514588,\n",
       " 0.25338522,\n",
       " 0.29527932,\n",
       " 0.25228503,\n",
       " 0.19045702,\n",
       " 0.2806524,\n",
       " 0.34908947,\n",
       " 0.35393417,\n",
       " 0.26439282,\n",
       " 0.3542645,\n",
       " 0.17327966,\n",
       " 0.28472513,\n",
       " 0.16394977,\n",
       " 0.32199642,\n",
       " 0.37683132,\n",
       " 0.330405,\n",
       " 0.39135376,\n",
       " 0.27418476,\n",
       " 0.43453184,\n",
       " 0.35314927,\n",
       " 0.35391268,\n",
       " 0.23777711,\n",
       " 0.2891773,\n",
       " 0.26138628,\n",
       " 0.23500675,\n",
       " 0.33132452,\n",
       " 0.053777948,\n",
       " 0.11425075,\n",
       " 0.2528434,\n",
       " 0.30908304,\n",
       " 0.08474042,\n",
       " 0.34846362,\n",
       " 0.39197862,\n",
       " 0.42523178,\n",
       " 0.32348084,\n",
       " 0.23866834,\n",
       " 0.41806698,\n",
       " 0.3760806,\n",
       " 0.1974892,\n",
       " 0.48672062,\n",
       " 0.476533,\n",
       " 0.439874,\n",
       " 0.5480356,\n",
       " 0.37777248,\n",
       " 0.44266647,\n",
       " 0.40788823,\n",
       " 0.36017784,\n",
       " 0.40531832,\n",
       " 0.37615246,\n",
       " 0.4757444,\n",
       " 0.3860097,\n",
       " 0.39843485,\n",
       " 0.40264624,\n",
       " 0.53194046,\n",
       " 0.60414845,\n",
       " 0.48697817,\n",
       " 0.5725498,\n",
       " 0.3423534,\n",
       " 0.32670212,\n",
       " 0.34640366,\n",
       " 0.37822485,\n",
       " 0.3946973,\n",
       " 0.34871686,\n",
       " 0.28860292,\n",
       " 0.34924006,\n",
       " 0.36742774,\n",
       " 0.34118915,\n",
       " 0.37459353,\n",
       " 0.3017645,\n",
       " 0.25913298,\n",
       " 0.39086097,\n",
       " 0.35857764,\n",
       " 0.39828107,\n",
       " 0.5298234,\n",
       " 0.41111773,\n",
       " 0.37934017,\n",
       " 0.34845343,\n",
       " 0.28307182,\n",
       " 0.2497046,\n",
       " 0.31923833,\n",
       " 0.20843653,\n",
       " 0.41681218,\n",
       " 0.4377977,\n",
       " 0.34927696,\n",
       " 0.424367,\n",
       " 0.28722847,\n",
       " 0.39586174,\n",
       " 0.41071,\n",
       " 0.3142128,\n",
       " 0.38295618,\n",
       " 0.43216783,\n",
       " 0.07577793,\n",
       " 0.13494071,\n",
       " 0.5083033,\n",
       " 0.43375984,\n",
       " 0.5024332,\n",
       " 0.47877494,\n",
       " 0.35508317,\n",
       " 0.55647373,\n",
       " 0.33572942,\n",
       " 0.3077418,\n",
       " 0.20894007,\n",
       " 0.31724527,\n",
       " 0.38805175,\n",
       " 0.38060308,\n",
       " 0.34587714,\n",
       " 0.49268985,\n",
       " 0.4771397,\n",
       " 0.51981616,\n",
       " 0.47722065,\n",
       " 0.34412223,\n",
       " 0.54055256,\n",
       " 0.5871098,\n",
       " 0.42693862,\n",
       " 0.35149342,\n",
       " 0.4391266,\n",
       " 0.35570535,\n",
       " 0.45445347,\n",
       " 0.5477893,\n",
       " 0.38962513,\n",
       " 0.34288913,\n",
       " 0.23711157,\n",
       " 0.42835116,\n",
       " 0.37129936,\n",
       " 0.4939846,\n",
       " 0.26098505,\n",
       " 0.36237428,\n",
       " 0.41722503,\n",
       " 0.37476832,\n",
       " 0.4017189,\n",
       " 0.31430927,\n",
       " 0.4742432,\n",
       " 0.32888606,\n",
       " 0.3231213,\n",
       " 0.3704018,\n",
       " 0.31231755,\n",
       " 0.29145163,\n",
       " 0.26674393,\n",
       " 0.31173488,\n",
       " 0.4268285,\n",
       " 0.3379138,\n",
       " 0.23345551,\n",
       " 0.3659631,\n",
       " 0.3966793,\n",
       " 0.26871055,\n",
       " 0.42176884,\n",
       " 0.31388563,\n",
       " 0.10606438,\n",
       " 0.26660398,\n",
       " 0.08780023,\n",
       " 0.0749536,\n",
       " 0.151172,\n",
       " 0.23389742,\n",
       " 0.35892045,\n",
       " 0.30886978,\n",
       " 0.3707124,\n",
       " 0.38878465,\n",
       " 0.07959297,\n",
       " 0.15610199,\n",
       " 0.18829907,\n",
       " 0.40567893,\n",
       " 0.43222192,\n",
       " 0.24160971,\n",
       " 0.4003533,\n",
       " 0.3951792,\n",
       " 0.2534418,\n",
       " 0.30482057,\n",
       " 0.25125948,\n",
       " 0.21970016,\n",
       " 0.2837561,\n",
       " 0.2343097,\n",
       " 0.44972742,\n",
       " 0.29742652,\n",
       " 0.34460437,\n",
       " 0.3521589,\n",
       " 0.32864586,\n",
       " 0.2926991,\n",
       " 0.37982064,\n",
       " 0.4161579,\n",
       " 0.35610366,\n",
       " 0.1573945,\n",
       " 0.18320803,\n",
       " 0.082420304,\n",
       " 0.432495,\n",
       " 0.3841324,\n",
       " 0.3439678,\n",
       " 0.41998696,\n",
       " 0.37591842,\n",
       " 0.2595982,\n",
       " 0.33444616,\n",
       " 0.35738394,\n",
       " 0.16245732,\n",
       " 0.14113386,\n",
       " 0.3634565,\n",
       " 0.55860525,\n",
       " 0.25530553,\n",
       " 0.19994923,\n",
       " 0.28168976,\n",
       " 0.26836276,\n",
       " 0.34454405,\n",
       " 0.048381798,\n",
       " 0.37025476,\n",
       " 0.3691555,\n",
       " 0.2566436,\n",
       " 0.1785951,\n",
       " 0.39057153,\n",
       " 0.44500905,\n",
       " 0.32857034,\n",
       " 0.13420996,\n",
       " 0.37771153,\n",
       " 0.3604356,\n",
       " 0.43931317,\n",
       " 0.38794482,\n",
       " 0.33245984,\n",
       " 0.4245305,\n",
       " 0.3767363,\n",
       " 0.3896065,\n",
       " 0.41368875,\n",
       " 0.37121397,\n",
       " 0.43617183,\n",
       " 0.32546788,\n",
       " 0.39690614,\n",
       " 0.46373716,\n",
       " 0.27626768,\n",
       " 0.47416204,\n",
       " 0.37397522,\n",
       " 0.29860315,\n",
       " 0.3131641,\n",
       " 0.45336375,\n",
       " 0.478095,\n",
       " 0.30515796,\n",
       " 0.35096467,\n",
       " 0.32669988,\n",
       " 0.3568903,\n",
       " 0.4654769,\n",
       " 0.39857957,\n",
       " 0.428714,\n",
       " 0.5313927,\n",
       " 0.34689608,\n",
       " 0.35011885,\n",
       " 0.28548968,\n",
       " 0.25216636,\n",
       " 0.5850584,\n",
       " 0.33261353,\n",
       " 0.35965878,\n",
       " 0.26296556,\n",
       " 0.2935447,\n",
       " 0.44582486,\n",
       " 0.23694281,\n",
       " 0.32772994,\n",
       " 0.3319718,\n",
       " 0.3592892,\n",
       " 0.28791237,\n",
       " 0.32068068,\n",
       " 0.31627917,\n",
       " 0.36814845,\n",
       " 0.26227814,\n",
       " 0.33652824,\n",
       " 0.2628485,\n",
       " 0.353673,\n",
       " 0.14393085,\n",
       " 0.34598005,\n",
       " 0.30902392,\n",
       " 0.32705715,\n",
       " 0.34843543,\n",
       " 0.23389885,\n",
       " 0.31375507,\n",
       " 0.36799395,\n",
       " 0.267152,\n",
       " 0.25031108,\n",
       " 0.30760247,\n",
       " 0.19408104,\n",
       " 0.32915533,\n",
       " 0.10292424,\n",
       " 0.26230842,\n",
       " 0.3607005,\n",
       " 0.47423193,\n",
       " 0.2784991,\n",
       " 0.25248992,\n",
       " 0.20671573,\n",
       " 0.28200674,\n",
       " 0.01820346,\n",
       " 0.23609562,\n",
       " 0.10961129,\n",
       " 0.016415596,\n",
       " 0.52466965,\n",
       " 0.31324875,\n",
       " 0.24335209,\n",
       " 0.16598883,\n",
       " 0.2239084,\n",
       " 0.23078522,\n",
       " 0.16674088,\n",
       " 0.22738424,\n",
       " 0.28913262,\n",
       " 0.21405962,\n",
       " 0.11158353,\n",
       " 0.10412775,\n",
       " 0.44646105,\n",
       " 0.30989447,\n",
       " ...]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords_vec = sent_encoder.encode(\" \".join(text_keywords))\n",
    "\n",
    "segment_similarities = []\n",
    "segments = text_segmenter(text)\n",
    "for segment in segments:\n",
    "\tsegment_vec = sent_encoder.encode(segment)\n",
    "\tsegment_similarities.append(\n",
    "\t\tkeywords_vec @ segment_vec\n",
    "\t)\n",
    "\n",
    "segment_similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = torch.nn.functional.softmax(\n",
    "\ttorch.tensor(segment_similarities) * 10\n",
    ").numpy()\n",
    "\n",
    "plt.plot(probs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected = np.random.choice(segments, size=2, p=probs, replace=False)\n",
    "selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[\n",
    "\tsegments[i]\n",
    "\tfor i, sim in enumerate(segment_similarities)\n",
    "\tif sim < 0\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KeywordScorer(Encoder):\n",
    "\n",
    "\tdef __init__(\n",
    "\t\tself,\n",
    "\t\ttokenizer,\n",
    "\t\tmax_tokens: int,\n",
    "\t\ttext_segmenter: Callable[[str], list[str]],\n",
    "\t\tpreprocessor: Callable[[list[str]], list[str]] | None = None,\n",
    "\t\tstop_words: list[str] | None = None,\n",
    "\t\tadd_special_tokens: bool = True\n",
    "\t) -> None:\n",
    "\t\tsuper().__init__(\n",
    "\t\t\ttokenizer, 0, max_tokens, preprocessor,\n",
    "\t\t\tadd_special_tokens, tokenizer.bos_token_id,\n",
    "\t\t\ttokenizer.eos_token_id\n",
    "\t\t)\n",
    "\t\tself.text_segmenter = text_segmenter\n",
    "\t\tself.stop_words = stop_words\n",
    "\n",
    "\tdef encode(\n",
    "\t\tself,\n",
    "\t\ttext: str,\n",
    "\t\t_ = None,\n",
    "\t\tmax_tokens: int | None = None\n",
    "\t) -> list[str]:\n",
    "\t\tmax_tokens = max_tokens or self.max_tokens\n",
    "\t\tsegments = self.text_segmenter(text)\n",
    "\t\tkeywords = get_keywords(\n",
    "\t\t\ttext,\n",
    "\t\t\tstop_words = stop_words,\n",
    "\t\t\tpreprocessor = keywords_preprocessor\n",
    "\t\t)\n",
    "\t\tkeywords_vec = sent_encoder.encode(\" \".join(keywords))\n",
    "\t\tsegment_similarities = []\n",
    "\t\tfor segment in text:\n",
    "\t\t\tsegment_vec = sent_encoder.encode(segment)\n",
    "\t\t\tsegment_similarities.append(\n",
    "\t\t\t\tkeywords_vec @ segment_vec\n",
    "\t\t\t)\n",
    "\t\tprobs = torch.nn.functional.softmax(\n",
    "\t\t\ttorch.tensor(segment_similarities) * 10\n",
    "\t\t).numpy()\n",
    "\t\tselected = np.random.choice(text, size=2, p=probs, replace=False)\n",
    "\t\treturn selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uccs-reu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
