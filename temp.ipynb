{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sys import getsizeof\n",
    "from time import sleep\n",
    "import json\n",
    "import re\n",
    "import pickle\n",
    "import inspect\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "\n",
    "import numpy as np\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from transformers import (\n",
    "\tBartTokenizer, BartForConditionalGeneration,\n",
    "\tT5Tokenizer, T5ForConditionalGeneration,\n",
    "\tGPT2TokenizerFast\n",
    ")\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from dotenv import load_dotenv\n",
    "import openai\n",
    "\n",
    "from utils.helpers import *\n",
    "from utils.encoders import *\n",
    "from utils.pipelines import *\n",
    "from utils.trainer_utils import *\n",
    "from utils.evaluator_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "crs files: 7238, gao files: 12228\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"/Users/naman/Workspace/Data/Long-Document-Summarization\"\n",
    "# data_dir = \"/home/nchibbar/Data\"\n",
    "\n",
    "crs_files = os.listdir(crs_dir := f\"{data_dir}/GovReport/crs\")\n",
    "gao_files = os.listdir(gao_dir := f\"{data_dir}/GovReport/gao\")\n",
    "\n",
    "print(f\"crs files: {len(crs_files)}, gao files: {len(gao_files)}\")\n",
    "\n",
    "out_dir = f\"{data_dir}/GovReport/processed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1024, 4096)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sentence transformer\n",
    "sent_dir = f\"{data_dir}/Models/Sent-Transformer\"\n",
    "sent_encoder = SentenceTransformer(sent_dir)\n",
    "\n",
    "# BART\n",
    "bart_dir = f\"{data_dir}/Models/BART\"\n",
    "bart_fine_tuned = f\"{data_dir}/Models/BART-GovReport-SentenceSampler\"\n",
    "bart_tokenizer = BartTokenizer.from_pretrained(bart_dir)\n",
    "bart_model = BartForConditionalGeneration.from_pretrained(bart_fine_tuned)\n",
    "bart_context_size = bart_model.config.max_position_embeddings\n",
    "\n",
    "# T5\n",
    "t5_dir = f\"{data_dir}/Models/T5\"\n",
    "t5_tokenizer = T5Tokenizer.from_pretrained(t5_dir)\n",
    "t5_model = T5ForConditionalGeneration.from_pretrained(t5_dir)\n",
    "t5_context_size = t5_model.config.n_positions\n",
    "\n",
    "# GPT 3.5 turbo tokenizer\n",
    "gpt_dir = f\"{data_dir}/Models/GPT-3.5-turbo-tokenizer\"\n",
    "gpt_tokenizer = GPT2TokenizerFast.from_pretrained(gpt_dir)\n",
    "gpt_model = \"gpt-3.5-turbo\"\n",
    "gpt_context_size = 4096\n",
    "\n",
    "bart_context_size, t5_context_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = TextProcessor(preprocessing=True)\n",
    "postprocessor = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GovReport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_subsections(sections):\n",
    "\ttext = \"\"\n",
    "\tfor sec in sections:\n",
    "\t\tsec_text = \"\\n\\n\".join(sec[\"paragraphs\"])\n",
    "\t\tif sec[\"section_title\"]:\n",
    "\t\t\tsec_text = f\"Section {sec[\"section_title\"]}:\\n\\n{sec_text}\"\n",
    "\t\ttext = f\"{text}\\n\\n{sec_text}\" if text else sec_text\n",
    "\t\tif sec[\"subsections\"]:\n",
    "\t\t\tsub_text = combine_subsections(sec[\"subsections\"])\n",
    "\t\t\ttext = f\"{text}\\n\\n{sub_text}\" if text else sub_text\n",
    "\treturn text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_crs_files = len(crs_files)\n",
    "for i, file in enumerate(crs_files):\n",
    "\tfull_path = os.path.join(crs_dir, file)\n",
    "\twith open(full_path) as fp:\n",
    "\t\tdata = json.load(fp)\n",
    "\tclear_stdout()\n",
    "\tprint(f\"{num_crs_files - i} files left\", end=\"\")\n",
    "\ttext = f\"{data[\"title\"]}\\n\\n\"\n",
    "\ttext += combine_subsections([data[\"reports\"]])\n",
    "\tsummary = \" \".join(data[\"summary\"])\n",
    "\tsummary = preprocessor.process(summary)\n",
    "\twith open(f\"{out_dir}/{file}\", \"w\") as fp:\n",
    "\t\tjson.dump({\n",
    "\t\t\t\"text\": text,\n",
    "\t\t\t\"summary\": summary\n",
    "\t\t}, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in gao_files:\n",
    "\tfile = os.path.join(gao_dir, file)\n",
    "\twith open(file) as fp:\n",
    "\t\tdata = json.load(fp)\n",
    "\ttext = combine_subsections(data[\"report\"])\n",
    "\ttext = preprocessor.process(text)\n",
    "\tprint(data[\"highlight\"])\n",
    "\tsummary = \"\\n\".join(data[\"highlight\"])\n",
    "\tsummary = preprocessor.process(summary)\n",
    "\twith open(f\"{out_dir}/{file}\", \"w\") as fp:\n",
    "\t\tjson.dump({\n",
    "\t\t\t\"text\": text,\n",
    "\t\t\t\"summary\": summary\n",
    "\t\t}, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(stop_words=\"english\")\n",
    "vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm = vectorizer.fit_transform([data[\"text\"]])\n",
    "vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = 4\n",
    "lda = LatentDirichletAllocation(n_components=topics)\n",
    "lda.fit(dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_dist = lda.transform(dtm)\n",
    "print(topic_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_topics(model, feature_names, num_top_words):\n",
    "\tfor topic_idx, topic in enumerate(model.components_):\n",
    "\t\tprint(f\"Topic {topic_idx}:\")\n",
    "\t\tprint(\" \".join([feature_names[i] for i in topic.argsort()[:-num_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_top_words = 10\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "display_topics(lda, feature_names, num_top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts, summaries = [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(73815, 553)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# max 73_791\n",
    "min_words_text = 70_000\n",
    "for file in crs_files:\n",
    "\twith open(f\"{out_dir}/{file}\") as fp:\n",
    "\t\tdata = json.load(fp)\n",
    "\tif count_words(data[\"text\"]) >= min_words_text:\n",
    "\t\tbreak\n",
    "texts.append(data[\"text\"])\n",
    "summaries.append(data[\"summary\"])\n",
    "\n",
    "count_words(data[\"text\"]), count_words(data[\"summary\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_words = 30_000\n",
    "texts, summaries = [], []\n",
    "for file in crs_files:\n",
    "\twith open(f\"{out_dir}/{file}\") as fp:\n",
    "\t\tdata = json.load(fp)\n",
    "\tif count_words(data[\"text\"]) >= min_words:\n",
    "\t\ttexts.append(data[\"text\"])\n",
    "\t\tsummaries.append(data[\"summary\"])\n",
    "\n",
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_min_words = 20\n",
    "sent_segmenter = TextSegmenter(nltk.sent_tokenize, segment_min_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "head_size = .5\n",
    "threshold = .7\n",
    "seed = 69\n",
    "device = get_device()\n",
    "device = \"cpu\"\n",
    "system_prompt = \"You will be given some segments of a very long document. Your task is to summarize the entire document as a whole by extracting key information and ideas from the segments. Generate a detailed, concise, and coherent summary in 500 words. Do not refer to the document in the summary in any way.\"\n",
    "\n",
    "bart_encoders = [\n",
    "\tTruncateMiddle(\n",
    "\t\tbart_tokenizer, bart_context_size, head_size, preprocessor, True\n",
    "\t),\n",
    "\tUniformSampler(\n",
    "\t\tbart_tokenizer, bart_context_size, sent_segmenter, preprocessor,\n",
    "\t\tTrue, seed\n",
    "\t),\n",
    "\tSentenceSampler(\n",
    "\t\tbart_tokenizer, bart_context_size, sent_segmenter, sent_encoder,\n",
    "\t\tpreprocessor, True, device=device, seed=seed\n",
    "\t),\n",
    "\tRemoveRedundancy(\n",
    "\t\tbart_tokenizer, bart_context_size, sent_segmenter, sent_encoder,\n",
    "\t\tpreprocessor, True, device=device, seed=seed\n",
    "\t)\n",
    "]\n",
    "t5_encoders = [\n",
    "\tTruncateMiddle(\n",
    "\t\tt5_tokenizer, t5_context_size, head_size, preprocessor, True\n",
    "\t),\n",
    "\tUniformSampler(\n",
    "\t\tt5_tokenizer, t5_context_size, sent_segmenter, preprocessor,\n",
    "\t\tTrue, seed\n",
    "\t),\n",
    "\tSentenceSampler(\n",
    "\t\tt5_tokenizer, t5_context_size, sent_segmenter, sent_encoder,\n",
    "\t\tpreprocessor, True, device=device, seed=seed\n",
    "\t),\n",
    "\tRemoveRedundancy(\n",
    "\t\tt5_tokenizer, t5_context_size, sent_segmenter, sent_encoder,\n",
    "\t\tpreprocessor, True, device=device, seed=seed\n",
    "\t)\n",
    "]\n",
    "gpt_encoders = [\n",
    "\tTruncateMiddle(\n",
    "\t\tgpt_tokenizer, gpt_context_size, head_size, preprocessor, True\n",
    "\t),\n",
    "\tUniformSampler(\n",
    "\t\tgpt_tokenizer, gpt_context_size, sent_segmenter, preprocessor,\n",
    "\t\tTrue, seed\n",
    "\t),\n",
    "\tSentenceSampler(\n",
    "\t\tgpt_tokenizer, gpt_context_size, sent_segmenter, sent_encoder,\n",
    "\t\tpreprocessor, True, device=device, seed=seed\n",
    "\t),\n",
    "\tRemoveRedundancy(\n",
    "\t\tgpt_tokenizer, gpt_context_size, sent_segmenter, sent_encoder,\n",
    "\t\tpreprocessor, True, device=device, seed=seed\n",
    "\t)\n",
    "]\n",
    "bart_pipelines = [\n",
    "\tSummarizationPipeline(\n",
    "\t\tbart_model, enc, bart_context_size, device=device\n",
    "\t) for enc in bart_encoders\n",
    "]\n",
    "t5_pipelines = [\n",
    "\tSummarizationPipeline(\n",
    "\t\tt5_model, enc, t5_context_size, device=device\n",
    "\t) for enc in t5_encoders\n",
    "]\n",
    "gpt_pipelines = [\n",
    "\tOpenAIPipeline(\n",
    "\t\tgpt_model, enc, gpt_context_size\n",
    "\t) for enc in gpt_encoders\n",
    "]\n",
    "pipelines = bart_pipelines + t5_pipelines + gpt_pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 3\n",
    "num_workers = min(len(pipelines), os.cpu_count())\n",
    "\n",
    "evaluator = Evaluator(pipelines, num_workers, device)\n",
    "results = evaluator(texts, summaries, batch_size)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uccs-reu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
