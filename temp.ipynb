{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sys import getsizeof\n",
    "from time import sleep\n",
    "import json\n",
    "import re\n",
    "import inspect\n",
    "from warnings import filterwarnings\n",
    "\n",
    "import numpy as np\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from transformers import (\n",
    "\tBartTokenizer, BartForConditionalGeneration,\n",
    "\tT5Tokenizer, T5ForConditionalGeneration,\n",
    "\tPegasusForConditionalGeneration, PegasusTokenizerFast,\n",
    "\tGPT2TokenizerFast\n",
    ")\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from utils.helpers import *\n",
    "from utils.encoders import *\n",
    "from utils.pipelines import *\n",
    "from utils.trainer_utils import *\n",
    "from utils.evaluator_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inf = float(\"inf\")\n",
    "filterwarnings(\"ignore\")\n",
    "device = get_device()\n",
    "# device = \"cpu\"\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/Users/naman/Workspace/Data/Long-Document-Summarization\"\n",
    "data_dir = \"/home/nchibbar/Data\"\n",
    "\n",
    "crs_files = os.listdir(crs_dir := f\"{data_dir}/GovReport/crs\")\n",
    "gao_files = os.listdir(gao_dir := f\"{data_dir}/GovReport/gao\")\n",
    "\n",
    "print(f\"crs files: {len(crs_files)}, gao files: {len(gao_files)}\")\n",
    "\n",
    "out_dir = f\"{data_dir}/GovReport/processed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentence transformer\n",
    "# Automatically loads into gpu if available\n",
    "sent_dir = f\"{data_dir}/Models/Sent-Transformer\"\n",
    "sent_encoder = SentenceTransformer(sent_dir).to(\"cpu\")\n",
    "\n",
    "# BART\n",
    "bart_dir = f\"{data_dir}/Models/BART\"\n",
    "bart_tokenizer = BartTokenizer.from_pretrained(bart_dir)\n",
    "bart_model = BartForConditionalGeneration.from_pretrained(bart_dir)\n",
    "bart_context_size = bart_model.config.max_position_embeddings\n",
    "\n",
    "# T5\n",
    "t5_dir = f\"{data_dir}/Models/T5\"\n",
    "t5_tokenizer = T5Tokenizer.from_pretrained(t5_dir)\n",
    "t5_model = T5ForConditionalGeneration.from_pretrained(t5_dir)\n",
    "t5_context_size = t5_model.config.n_positions\n",
    "\n",
    "# Pegasus\n",
    "pegasus_dir = f\"{data_dir}/Models/PEGASUS\"\n",
    "pegasus_tokenizer = PegasusTokenizerFast.from_pretrained(pegasus_dir)\n",
    "pegasus_model = PegasusForConditionalGeneration.from_pretrained(pegasus_dir)\n",
    "pegasus_context_size = pegasus_model.config.max_position_embeddings\n",
    "\n",
    "# GPT 3.5 turbo tokenizer\n",
    "gpt_dir = f\"{data_dir}/Models/GPT-3.5-turbo-tokenizer\"\n",
    "gpt_tokenizer = GPT2TokenizerFast.from_pretrained(gpt_dir)\n",
    "gpt_model = \"gpt-3.5-turbo\"\n",
    "gpt_context_size = 4096\n",
    "\n",
    "bart_context_size, t5_context_size, pegasus_context_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = TextProcessor(preprocessing=True)\n",
    "postprocessor = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GovReport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_subsections(sections):\n",
    "\ttext = \"\"\n",
    "\tfor sec in sections:\n",
    "\t\tsec_text = \"\\n\\n\".join(sec[\"paragraphs\"])\n",
    "\t\tif sec[\"section_title\"]:\n",
    "\t\t\tsec_text = f\"Section {sec[\"section_title\"]}:\\n\\n{sec_text}\"\n",
    "\t\ttext = f\"{text}\\n\\n{sec_text}\" if text else sec_text\n",
    "\t\tif sec[\"subsections\"]:\n",
    "\t\t\tsub_text = combine_subsections(sec[\"subsections\"])\n",
    "\t\t\ttext = f\"{text}\\n\\n{sub_text}\" if text else sub_text\n",
    "\treturn text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_crs_files = len(crs_files)\n",
    "for i, file in enumerate(crs_files):\n",
    "\tfull_path = os.path.join(crs_dir, file)\n",
    "\twith open(full_path) as fp:\n",
    "\t\tdata = json.load(fp)\n",
    "\tclear_stdout()\n",
    "\tprint(f\"{num_crs_files - i} files left\", end=\"\")\n",
    "\ttext = f\"{data[\"title\"]}\\n\\n\"\n",
    "\ttext += combine_subsections([data[\"reports\"]])\n",
    "\tsummary = \" \".join(data[\"summary\"])\n",
    "\tsummary = preprocessor.process(summary)\n",
    "\twith open(f\"{out_dir}/{file}\", \"w\") as fp:\n",
    "\t\tjson.dump({\n",
    "\t\t\t\"text\": text,\n",
    "\t\t\t\"summary\": summary\n",
    "\t\t}, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in gao_files:\n",
    "\tfile = os.path.join(gao_dir, file)\n",
    "\twith open(file) as fp:\n",
    "\t\tdata = json.load(fp)\n",
    "\ttext = combine_subsections(data[\"report\"])\n",
    "\ttext = preprocessor.process(text)\n",
    "\tprint(data[\"highlight\"])\n",
    "\tsummary = \"\\n\".join(data[\"highlight\"])\n",
    "\tsummary = preprocessor.process(summary)\n",
    "\twith open(f\"{out_dir}/{file}\", \"w\") as fp:\n",
    "\t\tjson.dump({\n",
    "\t\t\t\"text\": text,\n",
    "\t\t\t\"summary\": summary\n",
    "\t\t}, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BigPatent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigpatent_dir = f\"{data_dir}/BigPatent/train/a\"\n",
    "\n",
    "bigpatent_files = os.listdir(bigpatent_dir)\n",
    "\n",
    "word_counts = []\n",
    "for file in bigpatent_files:\n",
    "\twith open(f\"{bigpatent_dir}/{file}\") as fp:\n",
    "\t\tfor line in fp.readlines():\n",
    "\t\t\tdata = json.loads(line)\n",
    "\t\t\ttext = data[\"description\"]\n",
    "\t\t\tword_counts.append(count_words(text))\n",
    "\n",
    "bins = int(len(word_counts)**.5)\n",
    "plt.hist(word_counts, bins=bins)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(word_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_words = 70_000\n",
    "max_words = inf\n",
    "max_texts = 10\n",
    "texts, summaries = [], []\n",
    "num_texts = 0\n",
    "for file in crs_files:\n",
    "\twith open(f\"{out_dir}/{file}\") as fp:\n",
    "\t\tdata = json.load(fp)\n",
    "\tif min_words < count_words(data[\"text\"]) < max_words:\n",
    "\t\ttexts.append(data[\"text\"])\n",
    "\t\tsummaries.append(data[\"summary\"])\n",
    "\t\tnum_texts += 1\n",
    "\tif num_texts == max_texts:\n",
    "\t\tbreak\n",
    "\n",
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_min_words = 20\n",
    "text_segmenter = TextSegmenter(nltk.sent_tokenize, segment_min_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_tokens_frac = .5\n",
    "min_summary_tokens = 400\n",
    "head_size = .5\n",
    "threshold = .7\n",
    "boost = .03\n",
    "seed = 69\n",
    "system_prompt = \"You will be given some segments of a very long document. Your task is to summarize the entire document as a whole by extracting key information and ideas from the segments. Generate a detailed, concise, and coherent summary in 500 words. Do not refer to the document in the summary in any way.\"\n",
    "\n",
    "sent_encoder.to(device)\n",
    "\n",
    "bart_encoders = [\n",
    "\tTruncateMiddle(\n",
    "\t\tbart_tokenizer, bart_context_size, head_size, preprocessor, True\n",
    "\t),\n",
    "\tUniformSampler(\n",
    "\t\tbart_tokenizer, min_tokens_frac * bart_context_size, bart_context_size,\n",
    "\t\ttext_segmenter, preprocessor, True, seed\n",
    "\t),\n",
    "\tSegmentSampler(\n",
    "\t\tbart_tokenizer, min_tokens_frac * bart_context_size, bart_context_size,\n",
    "\t\ttext_segmenter, sent_encoder, preprocessor, True, threshold, boost, seed\n",
    "\t),\n",
    "\tRemoveRedundancy(\n",
    "\t\tbart_tokenizer, min_tokens_frac * bart_context_size, bart_context_size,\n",
    "\t\ttext_segmenter, sent_encoder, preprocessor, True, threshold, seed\n",
    "\t)\n",
    "]\n",
    "t5_encoders = [\n",
    "\tTruncateMiddle(\n",
    "\t\tt5_tokenizer, t5_context_size, head_size, preprocessor, True\n",
    "\t),\n",
    "\tUniformSampler(\n",
    "\t\tt5_tokenizer, min_tokens_frac * bart_context_size, t5_context_size,\n",
    "\t\ttext_segmenter, preprocessor, True, seed\n",
    "\t),\n",
    "\tSegmentSampler(\n",
    "\t\tt5_tokenizer, min_tokens_frac * bart_context_size, t5_context_size,\n",
    "\t\ttext_segmenter, sent_encoder, preprocessor, True, threshold, boost, seed\n",
    "\t),\n",
    "\tRemoveRedundancy(\n",
    "\t\tt5_tokenizer, min_tokens_frac * bart_context_size, t5_context_size,\n",
    "\t\ttext_segmenter, sent_encoder, preprocessor, True, threshold, seed\n",
    "\t)\n",
    "]\n",
    "gpt_encoders = [\n",
    "\tTruncateMiddle(\n",
    "\t\tgpt_tokenizer, gpt_context_size, head_size, preprocessor, True\n",
    "\t),\n",
    "\tUniformSampler(\n",
    "\t\tgpt_tokenizer, min_tokens_frac * gpt_context_size, gpt_context_size,\n",
    "\t\ttext_segmenter, preprocessor, True, seed\n",
    "\t),\n",
    "\tSegmentSampler(\n",
    "\t\tgpt_tokenizer, min_tokens_frac * gpt_context_size, gpt_context_size,\n",
    "\t\ttext_segmenter, sent_encoder, preprocessor, True, threshold, boost, seed\n",
    "\t),\n",
    "\tRemoveRedundancy(\n",
    "\t\tgpt_tokenizer, min_tokens_frac * gpt_context_size, gpt_context_size,\n",
    "\t\ttext_segmenter, sent_encoder, preprocessor, True, threshold, seed\n",
    "\t)\n",
    "]\n",
    "bart_pipelines = [\n",
    "\tSummarizationPipeline(\n",
    "\t\tbart_model, enc, postprocessor, min_summary_tokens,\n",
    "\t\tbart_context_size, device\n",
    "\t) for enc in bart_encoders\n",
    "]\n",
    "t5_pipelines = [\n",
    "\tSummarizationPipeline(\n",
    "\t\tt5_model, enc, postprocessor, min_summary_tokens,\n",
    "\t\tt5_context_size, device\n",
    "\t) for enc in t5_encoders\n",
    "]\n",
    "gpt_pipelines = [\n",
    "\tOpenAIPipeline(\n",
    "\t\tgpt_model, enc, system_prompt=system_prompt\n",
    "\t) for enc in gpt_encoders\n",
    "]\n",
    "pipelines = bart_pipelines + t5_pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encodings1 = bart_encoders[1](texts, return_batch=False)\n",
    "encodings2 = bart_encoders[2](texts, return_batch=False)\n",
    "\n",
    "token_lengths1 = [len(enc) for enc in encodings1]\n",
    "token_lengths2 = [len(enc) for enc in encodings2]\n",
    "\n",
    "avg_tokens1 = np.mean(token_lengths1)\n",
    "avg_tokens2 = np.mean(token_lengths2)\n",
    "\n",
    "avg_tokens1, avg_tokens2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, text in enumerate(texts):\n",
    "\tprint(f\"Processing text {i + 1}\")\n",
    "\tbart_encoders[3](text, return_batch=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = preprocessor(texts[3])\n",
    "text = text_segmenter(text)\n",
    "\n",
    "[count_words(seg) for seg in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = texts[1]\n",
    "text = preprocessor(text)\n",
    "count_words(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_stop_words = [\n",
    "\t\"also\", \"however\", \"therefore\", \"thus\", \"hence\", \"moreover\",\n",
    "\t\"must\", \"may\", \"might\", \"could\", \"would\", \"shall\", \"need\",\n",
    "\t\"needs\", \"given\", \"since\", \"though\",\n",
    "]\n",
    "for word in my_stop_words:\n",
    "\tif word in nltk.corpus.stopwords.words(\"english\"):\n",
    "\t\tprint(word)\n",
    "\n",
    "stop_words = nltk.corpus.stopwords.words(\"english\") + my_stop_words\n",
    "stop_words += [\n",
    "\tword.capitalize()\n",
    "\tfor word in stop_words\n",
    "\tif not word.istitle()\n",
    "]\n",
    "\n",
    "len(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_preprocessor = TextProcessor(\n",
    "\tonly_words_nums = True,\n",
    "\tremove_nums = True\n",
    ")\n",
    "\n",
    "text_keywords = get_keywords(\n",
    "\ttext,\n",
    "\tstop_words = stop_words,\n",
    "\tpreprocessor = keywords_preprocessor\n",
    ")\n",
    "\n",
    "text_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_vec = sent_encoder.encode(\" \".join(text_keywords))\n",
    "\n",
    "segment_similarities = []\n",
    "segments = text_segmenter(text)\n",
    "for segment in segments:\n",
    "\tsegment_vec = sent_encoder.encode(segment)\n",
    "\tsegment_similarities.append(\n",
    "\t\tkeywords_vec @ segment_vec\n",
    "\t)\n",
    "\n",
    "segment_similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = torch.nn.functional.softmax(\n",
    "\ttorch.tensor(segment_similarities) * 10\n",
    ").numpy()\n",
    "\n",
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(probs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected = np.random.choice(segments, size=2, p=probs, replace=False)\n",
    "selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[\n",
    "\tsegments[i]\n",
    "\tfor i, sim in enumerate(segment_similarities)\n",
    "\tif sim < 0\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.sub(r\"(\\b|\\+)[\\d+-]+\\b\", \"\", \"+1234-5678 +90\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uccs-reu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
