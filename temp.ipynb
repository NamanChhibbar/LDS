{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "from time import perf_counter\n",
    "import inspect\n",
    "from sys import getsizeof\n",
    "import numpy as np\n",
    "import nltk\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from transformers import (\n",
    "\tBartTokenizer, BartForConditionalGeneration,\n",
    "\tT5Tokenizer, T5ForConditionalGeneration\n",
    ")\n",
    "from transformers.tokenization_utils_base import BatchEncoding\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from bert_score import BERTScorer\n",
    "from rouge import Rouge\n",
    "\n",
    "from utils.pipelines import *\n",
    "from utils.helpers import *\n",
    "\n",
    "device = get_device()\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_dir = \"/Users/naman/Workspace/Data/UCCS-REU\"\n",
    "data_dir = \"/home/nchibbar/Data\"\n",
    "\n",
    "crs_files = os.listdir(crs_dir := f\"{data_dir}/GovReport/crs\")\n",
    "gao_files = os.listdir(gao_dir := f\"{data_dir}/GovReport/gao\")\n",
    "\n",
    "print(f\"crs files: {len(crs_files)}, gao files: {len(gao_files)}\")\n",
    "\n",
    "crs_out = f\"{data_dir}/GovReport/crs-processed\"\n",
    "gao_out = f\"{data_dir}/GovReport/gao-processed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_tokens = 512\n",
    "\n",
    "# BART\n",
    "bart_dir = f\"{data_dir}/Models/BART\"\n",
    "bart_fine_tuned = f\"{data_dir}/Models/BART-GovReport-SentenceSampler\"\n",
    "bart_checkpoint = \"facebook/bart-large-cnn\"\n",
    "tokenizer = BartTokenizer.from_pretrained(bart_dir)\n",
    "model = BartForConditionalGeneration.from_pretrained(bart_fine_tuned)\n",
    "context_size = model.config.max_position_embeddings\n",
    "\n",
    "# T5\n",
    "# t5_dir = f\"{data_dir}/Models/T5\"\n",
    "# t5_checkpoint = \"google/flan-t5-base\"\n",
    "# t5_checkpoint = \"pszemraj/long-t5-tglobal-base-16384-book-summary\"\n",
    "# tokenizer = T5Tokenizer.from_pretrained(t5_dir)\n",
    "# model = T5ForConditionalGeneration.from_pretrained(t5_dir)\n",
    "# context_size = model.config.n_positions\n",
    "\n",
    "context_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens = extract_special_tokens(\n",
    "\ttokenizer.special_tokens_map.values()\n",
    ")\n",
    "preprocessor = TextProcessor(preprocessing=True)\n",
    "postprocessor = TextProcessor(ignore_tokens=special_tokens)\n",
    "special_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GovReport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_subsections(sections):\n",
    "\ttext = \"\"\n",
    "\tfor sec in sections:\n",
    "\t\tsec_text = \"\\n\\n\".join(sec[\"paragraphs\"])\n",
    "\t\tif sec[\"section_title\"]:\n",
    "\t\t\tsec_text = f\"Section {sec[\"section_title\"]}:\\n\\n{sec_text}\"\n",
    "\t\ttext = f\"{text}\\n\\n{sec_text}\" if text else sec_text\n",
    "\t\tif sec[\"subsections\"]:\n",
    "\t\t\tsub_text = combine_subsections(sec[\"subsections\"])\n",
    "\t\t\ttext = f\"{text}\\n\\n{sub_text}\" if text else sub_text\n",
    "\treturn text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in crs_files:\n",
    "\twith open(f\"{crs_dir}/{file}\") as fp:\n",
    "\t\tdata = json.load(fp)\n",
    "\ttext = combine_subsections([data[\"reports\"]])\n",
    "\ttext = preprocessor.process(text)\n",
    "\tsummary = \"\\n\".join(data[\"summary\"])\n",
    "\tsummary = preprocessor.process(summary)\n",
    "\twith open(f\"{crs_out}/{file}\", \"w\") as fp:\n",
    "\t\tjson.dump({\n",
    "\t\t\t\"text\": text,\n",
    "\t\t\t\"summary\": summary\n",
    "\t\t}, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in gao_files:\n",
    "\twith open(f\"{gao_dir}/{file}\") as fp:\n",
    "\t\tdata = json.load(fp)\n",
    "\ttext = combine_subsections(data[\"report\"])\n",
    "\ttext = preprocessor.process(text)\n",
    "\tprint(data[\"highlight\"])\n",
    "\tsummary = \"\\n\".join(data[\"highlight\"])\n",
    "\tsummary = preprocessor.preprocess(summary)\n",
    "\twith open(f\"{gao_out}/{file}\", \"w\") as fp:\n",
    "\t\tjson.dump({\n",
    "\t\t\t\"text\": text,\n",
    "\t\t\t\"summary\": summary\n",
    "\t\t}, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(stop_words=\"english\")\n",
    "vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm = vectorizer.fit_transform([data[\"text\"]])\n",
    "vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = 4\n",
    "lda = LatentDirichletAllocation(n_components=topics)\n",
    "lda.fit(dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_dist = lda.transform(dtm)\n",
    "print(topic_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_topics(model, feature_names, num_top_words):\n",
    "\tfor topic_idx, topic in enumerate(model.components_):\n",
    "\t\tprint(f\"Topic {topic_idx}:\")\n",
    "\t\tprint(\" \".join([feature_names[i] for i in topic.argsort()[:-num_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_top_words = 10\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "display_topics(lda, feature_names, num_top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_summaries = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max 73_791\n",
    "min_words_text = 70_000\n",
    "for file in crs_files:\n",
    "\twith open(f\"{crs_out}/{file}\") as fp:\n",
    "\t\tdata = json.load(fp)\n",
    "\tif count_words(data[\"text\"]) >= min_words_text:\n",
    "\t\tbreak\n",
    "texts_summaries.append((data[\"text\"], data[\"summary\"]))\n",
    "\n",
    "count_words(data[\"text\"]), count_words(data[\"summary\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_summaries = []\n",
    "for file in crs_files:\n",
    "\twith open(f\"{crs_out}/{file}\") as fp:\n",
    "\t\tdata = json.load(fp)\n",
    "\ttexts_summaries.append((data[\"text\"], data[\"summary\"]))\n",
    "\n",
    "len(texts_summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_checkpoint = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "sent_dir = f\"{data_dir}/Models/Sent-Transformer\"\n",
    "\n",
    "sent_encoder = SentenceTransformer(sent_dir)\n",
    "sent_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head_size = .5\n",
    "threshold = .85\n",
    "seed = 69\n",
    "# device=\"cpu\"\n",
    "\n",
    "encoders = [\n",
    "\tTruncateMiddle(\n",
    "\t\ttokenizer, context_size, head_size, preprocessor\n",
    "\t),\n",
    "\tUniformSampler(\n",
    "\t\ttokenizer, context_size, nltk.sent_tokenize, preprocessor, seed\n",
    "\t),\n",
    "\tSentenceSampler(\n",
    "\t\ttokenizer, context_size, nltk.sent_tokenize, sent_encoder, threshold,\n",
    "\t\tpreprocessor, device, seed\n",
    "\t),\n",
    "\tRemoveRedundancy(\n",
    "\t\ttokenizer, context_size, nltk.sent_tokenize, sent_encoder, threshold,\n",
    "\t\tpreprocessor, device, seed\n",
    "\t),\n",
    "]\n",
    "\n",
    "pipelines = [\n",
    "\tSummarizationPipeline(model, encoder, max_tokens, postprocessor, device)\n",
    "\tfor encoder in encoders\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = Evaluator(\n",
    "\tpipelines, texts_summaries, device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator.generate_summaries()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator.get_rouge_score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator.get_bert_score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(evaluator.generated_summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_words(evaluator.generated_summaries[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = encoders[0](texts_summaries[0][0])\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs[\"input_ids\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model.generate(**inputs, max_length=max_tokens)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uccs-reu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
