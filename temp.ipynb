{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mps'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import torch\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import inspect\n",
    "\n",
    "from utils import *\n",
    "\n",
    "device = get_device()\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## gov-report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "crs files: 7238, gao files: 12228\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"/Users/naman/Workspace/Data/UCCS-REU\"\n",
    "\n",
    "crs_files = os.listdir(crs_dir := f\"{data_dir}/GovReport/crs\")\n",
    "gao_files = os.listdir(gao_dir := f\"{data_dir}/GovReport/gao\")\n",
    "\n",
    "print(f\"crs files: {len(crs_files)}, gao files: {len(gao_files)}\")\n",
    "\n",
    "crs_out = f\"{data_dir}/GovReport/crs-processed\"\n",
    "gao_out = f\"{data_dir}/GovReport/gao-processed\"\n",
    "\n",
    "preprocessor = TextPreprocessor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in crs_files:\n",
    "\twith open(f\"{crs_dir}/{file}\") as fp:\n",
    "\t\tdata = json.load(fp)\n",
    "\ttext = combine_subsections([data[\"reports\"]])\n",
    "\ttext = preprocessor.preprocess(text)\n",
    "\tsummary = \"\\n\".join(data[\"summary\"])\n",
    "\tsummary = preprocessor.preprocess(summary)\n",
    "\twith open(f\"{crs_out}/{file}\", \"w\") as fp:\n",
    "\t\tjson.dump({\n",
    "\t\t\t\"text\": text,\n",
    "\t\t\t\"summary\": summary\n",
    "\t\t}, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "for file in gao_files:\n",
    "\twith open(f\"{gao_dir}/{file}\") as fp:\n",
    "\t\tdata = json.load(fp)\n",
    "\ttext = combine_subsections(data[\"report\"])\n",
    "\ttext = preprocessor.preprocess(text)\n",
    "\tprint(data[\"highlight\"])\n",
    "\tsummary = \"\\n\".join(data[\"highlight\"])\n",
    "\tsummary = preprocessor.preprocess(summary)\n",
    "\twith open(f\"{gao_out}/{file}\", \"w\") as fp:\n",
    "\t\tjson.dump({\n",
    "\t\t\t\"text\": text,\n",
    "\t\t\t\"summary\": summary\n",
    "\t\t}, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_dir = f\"{data_dir}/Models/BART/tokenizer\"\n",
    "model_dir = f\"{data_dir}/Models/BART/model\"\n",
    "checkpoint = \"facebook/bart-large-cnn\"\n",
    "\n",
    "tokenizer = BartTokenizer.from_pretrained(tokenizer_dir)\n",
    "model = BartForConditionalGeneration.from_pretrained(model_dir)\n",
    "\n",
    "context_size, _ = max_lengths(model)\n",
    "context_size\n",
    "\n",
    "max_output_tokens = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8357, 479)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = f\"{crs_out}/{crs_files[0]}\"\n",
    "\n",
    "with open(file) as fp:\n",
    "\tdata = json.load(fp)\n",
    "count_words(data[\"text\"]), count_words(data[\"summary\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_sents(texts, sent_tokenizer, tokenizer, context_size):\n",
    "\tprocessed_texts = []\n",
    "\n",
    "\tfor text in texts:\n",
    "\t\t# Extract and encode sentences\n",
    "\t\tsents = sent_tokenizer(text)\n",
    "\t\tsents = tokenizer(sents)[\"input_ids\"]\n",
    "\t\tsents = np.array(sents, dtype=list)\n",
    "\n",
    "\t\t# Mean length of sentences\n",
    "\t\tmean_length = np.mean([\n",
    "\t\t\tlen(sent) for sent in sents\n",
    "\t\t])\n",
    "\n",
    "\t\t# Approximate number of sentences needed\n",
    "\t\tnum_samples = int(context_size / mean_length)\n",
    "\n",
    "\t\t# Check if there are enough sentences\n",
    "\t\tif len(sents) <= num_samples:\n",
    "\t\t\tflattened = [elm for lis in sents for elm in lis]\n",
    "\t\t\tprocessed_texts.append(flattened)\n",
    "\t\t\tcontinue\n",
    "\n",
    "\t\t# Sample until sentences fit in model\n",
    "\t\twhile True:\n",
    "\t\t\tsampled = np.random.choice(sents, size=num_samples, replace=False)\n",
    "\t\t\tflattened = [elm for lis in sampled for elm in lis]\n",
    "\t\t\tif len(flattened) <= context_size:\n",
    "\t\t\t\tprocessed_texts.append(flattened)\n",
    "\t\t\t\tbreak\n",
    "\n",
    "\t# Pad sentences and create attention mask\n",
    "\tpadded_ids = tokenizer.pad({\n",
    "\t\t\"input_ids\": processed_texts\n",
    "\t}, return_tensors=\"pt\")\n",
    "\n",
    "\treturn padded_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    0, 34375,    16,  ...,     1,     1,     1],\n",
       "        [    0, 42803,     6,  ...,     1,     1,     1],\n",
       "        [    0,  4528,  4504,  ...,   532,     4,     2]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 1, 1, 1]])}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_input = pick_sents([\n",
    "\t\"today is a fine\",\n",
    "\t\"yeah, you're right its a really nice\",\n",
    "\tdata[\"text\"]\n",
    "], nltk.sent_tokenize, tokenizer, context_size)\n",
    "model_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    2,     0, 34375,    16,    10,  2051,   183,     4,  2477,    16,\n",
       "            10,   372,   183,     4,    85,    18,    86,     7,   386,    10,\n",
       "            92,   183,     4,  2776,    10,    92,   301,     4,  2776,   452,\n",
       "             4,  2776,   110,    92,   183,    19,    10,    92,  1786,     4,\n",
       "          2776,    30,  1158,    23,     5,  2576,     9,     5,  1842,     8,\n",
       "           173,   110,   169,    62,     7,     5,   299,     4,  2578,   259,\n",
       "             7,   386,     4,     2,     1,     1],\n",
       "        [    2,     0,     0, 42803,     6,    47,   214,   235,    63,    10,\n",
       "           269,  2579,     4, 42803,     6,   370,   214,   235,     4,  3139,\n",
       "            10,   269, 34033,     4,  8976,     6,    47,    32,   235,     4,\n",
       "            85,    18,    10,   269, 41541,     4,  8976,     4,   370,   214,\n",
       "          4070,     4,    85,    16,    10,   269, 16911,     4,  8976, 39747,\n",
       "           370,   214, 13984,     4,    85,    17,    27,    29,    10, 30327,\n",
       "         41541,     4,     2,     1,     1,     1],\n",
       "        [    2,     0,   133, 10352,    12, 21426, 44195,  1783,    21, 14673,\n",
       "            11, 26873,     7,  2097,  1520,    31,   602,    15,   350,   203,\n",
       "          1126,     4,    20,  1760,    21,  3833,     7,  2097,     5,  5012,\n",
       "             9,    10,   613,   467,    14,    21,   350, 11788,    13,   867,\n",
       "             4,    20,  1783,    21,   423, 13522,     7,  1157,  1520,     7,\n",
       "           185,    15,    55,  1126,     4,    85,    21,    67,  3833,     7,\n",
       "          1744,   867,   108,  3168,     4,     2]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = model.generate(**model_input, max_length=max_output_tokens)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 66])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "</s><s>The Glass-Steagall Act was enacted in 1933 to prevent banks from taking on too much debt. The act was intended to prevent the creation of a financial system that was too risky for investors. The Act was later amended to allow banks to take on more debt. It was also intended to protect investors' interests.</s>\n"
     ]
    }
   ],
   "source": [
    "summary = tokenizer.decode(output[2])\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextPostprocessor:\n",
    "\n",
    "\tdef __init__(self, special_tokens: list[str]):\n",
    "\t\tself.special_tokens = re.compile(r\"|\".join(special_tokens))\n",
    "\t\n",
    "\tdef __call__(self, texts: list[str]):\n",
    "\t\tif isinstance(texts, str):\n",
    "\t\t\ttexts = [texts]\n",
    "\t\ttexts = [self.special_tokens.sub(\"\", text) for text in texts]\n",
    "\t\treturn texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values(['<s>', '</s>', '<unk>', '</s>', '<pad>', '<s>', '<mask>'])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "special_tokens = tokenizer.special_tokens_map.values()\n",
    "postprocessor = TextPostprocessor(special_tokens)\n",
    "special_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"The Glass-Steagall Act was enacted in 1933 to prevent banks from taking on too much debt. The act was intended to prevent the creation of a financial system that was too risky for investors. The Act was later amended to allow banks to take on more debt. It was also intended to protect investors' interests.\"]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "postprocessor(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    0, 43480, 24474,  ..., 10914,     4,     2]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]])}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp = truncate_middle([data[\"text\"]], tokenizer, context_size, .4)\n",
    "inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    2,     0, 44189,    12, 21426, 44195,  1783,  8254,  1861,  3454,\n",
       "            31,   915,  3454,     4,  6974, 36466,     9,  1861,     8,   915,\n",
       "          3454,    64,   244,  7540, 10246, 24323,  8068,  9023,    31,  4975,\n",
       "            11,  5157,  1048,     4,    20, 10875,     6,    30,  1495,     6,\n",
       "           473,    45,  1100,   141,   915,  1520,    32, 13588,   624,  5157,\n",
       "          1048,    50,   141,   786, 27045,    64,   304,  5157,  1713,     7,\n",
       "          1391,  2267,     8,  1861,  1126,     4,     2]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = model.generate(**inp, max_length=max_output_tokens)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'</s><s>Glass-Steagall Act separated commercial banking from investment banking. Separation of commercial and investment banking can help insulate insured depositories from volatility in securities markets. The separation, by itself, does not address how investment banks are regulated within securities markets or how nonbanks can use securities activities to fund consumer and commercial debt.</s>'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(out[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    0, 12229,     6,  ...,     1,     1,     1],\n",
       "        [    0, 43480, 24474,  ..., 10914,     4,     2]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 1, 1, 1]])}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp = truncate_middle([\n",
    "\t\"hey, this is a small text!\",\n",
    "\tdata[\"text\"]\n",
    "], tokenizer, context_size)\n",
    "inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "618a16ae090f4dbeb8eca36a86cddad9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecb48b76aedc406c9f38104bc8c579de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9170bfb53fdd47cb9f7aaa11683016a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11fe44d4e7374d7788ffbde2430ca367",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/naman/Workspace/Environments/uccs-reu/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a0449070fbe40da93b26e3abffed497",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66f6f4adfc4d420dbd8d46e81ef4a4f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08a0601fd1a84e2b81d0c77fd1af2a1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1d49365cb9e4f0098c8fd49212b3cda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8931899fc3b348bfbb8c67d381834ef5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a12b14c0fb4e4ad28824fb8d701e155c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a476ea3520d467abbd4e845235bdb0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \n",
       "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
       "  (2): Normalize()\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_checkpoint = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "sent_save_dir = f\"{data_dir}/Models/Sent-Transformer\"\n",
    "\n",
    "sent_model = SentenceTransformer(sent_save_dir)\n",
    "sent_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.06765417, -0.03888808,  0.05449073, ...,  0.01187451,\n",
       "         0.04905998,  0.01324423],\n",
       "       [-0.11608477, -0.07559214,  0.05973152, ..., -0.01747406,\n",
       "        -0.00257643,  0.05474273],\n",
       "       [ 0.10994177, -0.04852243, -0.07865883, ...,  0.00304659,\n",
       "         0.0611313 , -0.02873247]], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = sent_model.encode([\n",
    "\t\"hey bruh\",\n",
    "\t\"whats up huh?\",\n",
    "\tdata[\"text\"]\n",
    "])\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 384)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UniformSampler:\n",
    "\n",
    "\tdef __init__(\n",
    "\t\t\tself, text_preprocessor, text_postprocessor,  sent_tokenizer, tokenizer,\n",
    "\t\t\tsummarizer, summarizer_context_size, max_output_tokens\n",
    "\t\t):\n",
    "\t\tself.preprocessor = text_preprocessor\n",
    "\t\tself.postprocessor = text_postprocessor\n",
    "\t\tself.sent_tokenizer = sent_tokenizer\n",
    "\t\tself.tokenizer = tokenizer\n",
    "\t\tself.summarizer = summarizer\n",
    "\t\tself.context_size = summarizer_context_size\n",
    "\t\tself.max_tokens = max_output_tokens\n",
    "\n",
    "\tdef __call__(self, texts: list[str]):\n",
    "\t\ttexts = self.preprocessor(texts)\n",
    "\t\tinputs = self.pick_sents(texts)\n",
    "\t\toutputs = self.summarizer.generate(**inputs, max_length=self.max_tokens)\n",
    "\t\tsummaries = [self.tokenizer.decode(out) for out in outputs]\n",
    "\t\tprocessed_summaries = self.postprocessor(summaries)\n",
    "\t\treturn processed_summaries\n",
    "\n",
    "\tdef pick_sents(self, texts):\n",
    "\t\tsent_tokenizer = self.sent_tokenizer\n",
    "\t\ttokenizer = self.tokenizer\n",
    "\t\tcontext_size = self.context_size\n",
    "\n",
    "\t\tprocessed_texts = []\n",
    "\t\tfor text in texts:\n",
    "\t\t\t# Extract and encode sentences\n",
    "\t\t\tsents = sent_tokenizer(text)\n",
    "\t\t\tsents = tokenizer(sents)[\"input_ids\"]\n",
    "\t\t\tsents = np.array(sents, dtype=list)\n",
    "\n",
    "\t\t\t# Mean length of sentences\n",
    "\t\t\tmean_length = np.mean([\n",
    "\t\t\t\tlen(sent) for sent in sents\n",
    "\t\t\t])\n",
    "\n",
    "\t\t\t# Approximate number of sentences needed\n",
    "\t\t\tnum_samples = int(context_size / mean_length)\n",
    "\n",
    "\t\t\t# Check if there are enough sentences\n",
    "\t\t\tif len(sents) <= num_samples:\n",
    "\t\t\t\tflattened = [elm for lis in sents for elm in lis]\n",
    "\t\t\t\tprocessed_texts.append(flattened)\n",
    "\t\t\t\tcontinue\n",
    "\n",
    "\t\t\t# Sample until sentences fit in model\n",
    "\t\t\twhile True:\n",
    "\t\t\t\tsampled = np.random.choice(sents, size=num_samples, replace=False)\n",
    "\t\t\t\tflattened = [elm for lis in sampled for elm in lis]\n",
    "\t\t\t\tif len(flattened) <= context_size:\n",
    "\t\t\t\t\tprocessed_texts.append(flattened)\n",
    "\t\t\t\t\tbreak\n",
    "\n",
    "\t\t# Pad sentences and create attention mask\n",
    "\t\tpadded_ids = tokenizer.pad({\n",
    "\t\t\t\"input_ids\": processed_texts\n",
    "\t\t}, return_tensors=\"pt\")\n",
    "\n",
    "\t\treturn padded_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarizer = UniformSampler(\n",
    "\tpreprocessor, postprocessor, nltk.sent_tokenize, tokenizer,\n",
    "\tmodel, context_size, max_output_tokens\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"The Glass-Steagall Act was designed to prevent banks from lending money to each other. It was intended to prevent the banks from using the money to invest in each other's businesses. The law was not intended to stop banks from investing in one another's businesses, but to prevent them from lending to one another.\"]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarizer(data[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uccs-reu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
