{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sys import getsizeof\n",
    "import json\n",
    "import re\n",
    "import pickle\n",
    "import inspect\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "\n",
    "import numpy as np\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from transformers import (\n",
    "\tBartTokenizer, BartForConditionalGeneration,\n",
    "\tT5Tokenizer, T5ForConditionalGeneration,\n",
    "\tGPT2TokenizerFast\n",
    ")\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from dotenv import load_dotenv\n",
    "import tiktoken\n",
    "import openai\n",
    "\n",
    "from utils.helpers import *\n",
    "from utils.encoders import *\n",
    "from utils.pipelines import *\n",
    "from utils.trainer_utils import *\n",
    "from utils.evaluator_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "crs files: 7238, gao files: 12228\n"
     ]
    }
   ],
   "source": [
    "# data_dir = \"/Users/naman/Workspace/Data/UCCS-REU\"\n",
    "data_dir = \"/home/nchibbar/Data\"\n",
    "\n",
    "crs_files = os.listdir(crs_dir := f\"{data_dir}/GovReport/crs\")\n",
    "gao_files = os.listdir(gao_dir := f\"{data_dir}/GovReport/gao\")\n",
    "\n",
    "print(f\"crs files: {len(crs_files)}, gao files: {len(gao_files)}\")\n",
    "\n",
    "crs_out = f\"{data_dir}/GovReport/crs-processed\"\n",
    "gao_out = f\"{data_dir}/GovReport/gao-processed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_tokens = 512\n",
    "\n",
    "# Sentence transformer\n",
    "sent_dir = f\"{data_dir}/Models/Sent-Transformer\"\n",
    "sent_encoder = SentenceTransformer(sent_dir)\n",
    "\n",
    "# BART\n",
    "bart_dir = f\"{data_dir}/Models/BART\"\n",
    "bart_fine_tuned = f\"{data_dir}/Models/BART-GovReport-SentenceSampler\"\n",
    "tokenizer = BartTokenizer.from_pretrained(bart_dir)\n",
    "model = BartForConditionalGeneration.from_pretrained(bart_fine_tuned)\n",
    "context_size = model.config.max_position_embeddings\n",
    "\n",
    "# T5\n",
    "# t5_dir = f\"{data_dir}/Models/T5\"\n",
    "# tokenizer = T5Tokenizer.from_pretrained(t5_dir)\n",
    "# model = T5ForConditionalGeneration.from_pretrained(t5_dir)\n",
    "# context_size = model.config.n_positions\n",
    "\n",
    "# GPT 3.5 turbo tokenizer\n",
    "gpt_dir = f\"{data_dir}/Models/GPT-3.5-turbo-tokenizer\"\n",
    "gpt_tokenizer = GPT2TokenizerFast.from_pretrained(gpt_dir)\n",
    "\n",
    "context_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = TextProcessor(preprocessing=True)\n",
    "postprocessor = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GovReport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_subsections(sections):\n",
    "\ttext = \"\"\n",
    "\tfor sec in sections:\n",
    "\t\tsec_text = \"\\n\\n\".join(sec[\"paragraphs\"])\n",
    "\t\tif sec[\"section_title\"]:\n",
    "\t\t\tsec_text = f\"Section {sec[\"section_title\"]}:\\n\\n{sec_text}\"\n",
    "\t\ttext = f\"{text}\\n\\n{sec_text}\" if text else sec_text\n",
    "\t\tif sec[\"subsections\"]:\n",
    "\t\t\tsub_text = combine_subsections(sec[\"subsections\"])\n",
    "\t\t\ttext = f\"{text}\\n\\n{sub_text}\" if text else sub_text\n",
    "\treturn text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in crs_files:\n",
    "\tfile = os.path.join(crs_dir, file)\n",
    "\twith open(file) as fp:\n",
    "\t\tdata = json.load(fp)\n",
    "\ttext = combine_subsections([data[\"reports\"]])\n",
    "\ttext = preprocessor.process(text)\n",
    "\tsummary = \"\\n\".join(data[\"summary\"])\n",
    "\tsummary = preprocessor.process(summary)\n",
    "\twith open(f\"{crs_out}/{file}\", \"w\") as fp:\n",
    "\t\tjson.dump({\n",
    "\t\t\t\"text\": text,\n",
    "\t\t\t\"summary\": summary\n",
    "\t\t}, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in gao_files:\n",
    "\tfile = os.path.join(gao_dir, file)\n",
    "\twith open(file) as fp:\n",
    "\t\tdata = json.load(fp)\n",
    "\ttext = combine_subsections(data[\"report\"])\n",
    "\ttext = preprocessor.process(text)\n",
    "\tprint(data[\"highlight\"])\n",
    "\tsummary = \"\\n\".join(data[\"highlight\"])\n",
    "\tsummary = preprocessor.process(summary)\n",
    "\twith open(f\"{gao_out}/{file}\", \"w\") as fp:\n",
    "\t\tjson.dump({\n",
    "\t\t\t\"text\": text,\n",
    "\t\t\t\"summary\": summary\n",
    "\t\t}, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(stop_words=\"english\")\n",
    "vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm = vectorizer.fit_transform([data[\"text\"]])\n",
    "vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = 4\n",
    "lda = LatentDirichletAllocation(n_components=topics)\n",
    "lda.fit(dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_dist = lda.transform(dtm)\n",
    "print(topic_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_topics(model, feature_names, num_top_words):\n",
    "\tfor topic_idx, topic in enumerate(model.components_):\n",
    "\t\tprint(f\"Topic {topic_idx}:\")\n",
    "\t\tprint(\" \".join([feature_names[i] for i in topic.argsort()[:-num_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_top_words = 10\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "display_topics(lda, feature_names, num_top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts, summaries = [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70163, 364)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# max 73_791\n",
    "min_words_text = 70_000\n",
    "for file in crs_files:\n",
    "\twith open(f\"{crs_out}/{file}\") as fp:\n",
    "\t\tdata = json.load(fp)\n",
    "\tif count_words(data[\"text\"]) >= min_words_text:\n",
    "\t\tbreak\n",
    "texts.append(data[\"text\"])\n",
    "summaries.append(data[\"summary\"])\n",
    "\n",
    "count_words(data[\"text\"]), count_words(data[\"summary\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts, summaries = [], []\n",
    "for file in crs_files:\n",
    "\twith open(f\"{crs_out}/{file}\") as fp:\n",
    "\t\tdata = json.load(fp)\n",
    "\ttexts.append(data[\"text\"])\n",
    "\tsummaries.append(data[\"summary\"])\n",
    "\n",
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_min_words = 20\n",
    "sent_segmenter = TextSegmenter(nltk.sent_tokenize, segment_min_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_tokens = 4096\n",
    "head_size = .5\n",
    "threshold = .7\n",
    "seed = 69\n",
    "device = get_device()\n",
    "# device = \"cpu\"\n",
    "\n",
    "encoder = RemoveRedundancy(\n",
    "\tgpt_tokenizer, max_tokens, sent_segmenter, sent_encoder,\n",
    "\tpreprocessor, False, device=device, seed=seed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_model = \"gpt-3.5-turbo\"\n",
    "system_prompt = \"You will be given some segments of a very long document. Your task is to summarize the entire document as a whole by extracting key information and ideas from the segments. Generate a detailed, concise, and coherent summary in 500 words. Do not refer to the document in the summary in any way.\"\n",
    "\n",
    "pipeline = OpenAIPipeline(\n",
    "\topenai_model, encoder, system_prompt=system_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "evaluator = Evaluator(pipeline, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (86421 > 4096). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated summary for pipeline 1 in 12.328539667010773s\n",
      "Time taken to generate summaries: [12.328539667010773]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'time-taken': [12.328539667010773],\n",
       " 'bert-scores': [[0.8367783427238464, 0.8560788631439209, 0.8183289766311646]],\n",
       " 'rouge-scores': [{'rouge-1': [0.3233743409490334,\n",
       "    0.45098039215686275,\n",
       "    0.25205479452054796],\n",
       "   'rouge-2': [0.09876543209876544, 0.13793103448275862, 0.07692307692307693],\n",
       "   'rouge-l': [0.30886557463257347, 0.40521212872319956, 0.24953426443349247],\n",
       "   'rouge-w': [0.06733274264372192,\n",
       "    0.19170171911866954,\n",
       "    0.04083833995914776]}]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = evaluator(texts, summaries)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uccs-reu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
