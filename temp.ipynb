{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sys import getsizeof\n",
    "import json\n",
    "import re\n",
    "import pickle\n",
    "from time import perf_counter\n",
    "import inspect\n",
    "import numpy as np\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from transformers import (\n",
    "\tBartTokenizer, BartForConditionalGeneration,\n",
    "\tT5Tokenizer, T5ForConditionalGeneration\n",
    ")\n",
    "from transformers.tokenization_utils_base import BatchEncoding\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from bert_score import BERTScorer\n",
    "from rouge import Rouge\n",
    "\n",
    "from utils.pipelines import *\n",
    "from utils.helpers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "crs files: 7238, gao files: 12228\n"
     ]
    }
   ],
   "source": [
    "# data_dir = \"/Users/naman/Workspace/Data/UCCS-REU\"\n",
    "data_dir = \"/home/nchibbar/Data\"\n",
    "\n",
    "crs_files = os.listdir(crs_dir := f\"{data_dir}/GovReport/crs\")\n",
    "gao_files = os.listdir(gao_dir := f\"{data_dir}/GovReport/gao\")\n",
    "\n",
    "print(f\"crs files: {len(crs_files)}, gao files: {len(gao_files)}\")\n",
    "\n",
    "crs_out = f\"{data_dir}/GovReport/crs-processed\"\n",
    "gao_out = f\"{data_dir}/GovReport/gao-processed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_tokens = 512\n",
    "\n",
    "# BART\n",
    "bart_dir = f\"{data_dir}/Models/BART\"\n",
    "bart_fine_tuned = f\"{data_dir}/Models/BART-GovReport-SentenceSampler\"\n",
    "bart_checkpoint = \"facebook/bart-large-cnn\"\n",
    "tokenizer = BartTokenizer.from_pretrained(bart_dir)\n",
    "model = BartForConditionalGeneration.from_pretrained(bart_dir)\n",
    "context_size = model.config.max_position_embeddings\n",
    "\n",
    "# T5\n",
    "# t5_dir = f\"{data_dir}/Models/T5\"\n",
    "# t5_checkpoint = \"google/flan-t5-base\"\n",
    "# t5_checkpoint = \"pszemraj/long-t5-tglobal-base-16384-book-summary\"\n",
    "# tokenizer = T5Tokenizer.from_pretrained(t5_dir)\n",
    "# model = T5ForConditionalGeneration.from_pretrained(t5_dir)\n",
    "# context_size = model.config.n_positions\n",
    "\n",
    "context_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bos_id = tokenizer.bos_token_id\n",
    "eos_id = tokenizer.eos_token_id\n",
    "bos_id, eos_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>', '</s>', '<unk>', '</s>', '<pad>', '<s>', '<mask>']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "special_tokens = extract_special_tokens(\n",
    "\ttokenizer.special_tokens_map.values()\n",
    ")\n",
    "preprocessor = TextProcessor(preprocessing=True)\n",
    "postprocessor = TextProcessor(ignore_tokens=special_tokens)\n",
    "special_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GovReport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_subsections(sections):\n",
    "\ttext = \"\"\n",
    "\tfor sec in sections:\n",
    "\t\tsec_text = \"\\n\\n\".join(sec[\"paragraphs\"])\n",
    "\t\tif sec[\"section_title\"]:\n",
    "\t\t\tsec_text = f\"Section {sec[\"section_title\"]}:\\n\\n{sec_text}\"\n",
    "\t\ttext = f\"{text}\\n\\n{sec_text}\" if text else sec_text\n",
    "\t\tif sec[\"subsections\"]:\n",
    "\t\t\tsub_text = combine_subsections(sec[\"subsections\"])\n",
    "\t\t\ttext = f\"{text}\\n\\n{sub_text}\" if text else sub_text\n",
    "\treturn text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in crs_files:\n",
    "\twith open(f\"{crs_dir}/{file}\") as fp:\n",
    "\t\tdata = json.load(fp)\n",
    "\ttext = combine_subsections([data[\"reports\"]])\n",
    "\ttext = preprocessor.process(text)\n",
    "\tsummary = \"\\n\".join(data[\"summary\"])\n",
    "\tsummary = preprocessor.process(summary)\n",
    "\twith open(f\"{crs_out}/{file}\", \"w\") as fp:\n",
    "\t\tjson.dump({\n",
    "\t\t\t\"text\": text,\n",
    "\t\t\t\"summary\": summary\n",
    "\t\t}, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in gao_files:\n",
    "\twith open(f\"{gao_dir}/{file}\") as fp:\n",
    "\t\tdata = json.load(fp)\n",
    "\ttext = combine_subsections(data[\"report\"])\n",
    "\ttext = preprocessor.process(text)\n",
    "\tprint(data[\"highlight\"])\n",
    "\tsummary = \"\\n\".join(data[\"highlight\"])\n",
    "\tsummary = preprocessor.preprocess(summary)\n",
    "\twith open(f\"{gao_out}/{file}\", \"w\") as fp:\n",
    "\t\tjson.dump({\n",
    "\t\t\t\"text\": text,\n",
    "\t\t\t\"summary\": summary\n",
    "\t\t}, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(stop_words=\"english\")\n",
    "vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm = vectorizer.fit_transform([data[\"text\"]])\n",
    "vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = 4\n",
    "lda = LatentDirichletAllocation(n_components=topics)\n",
    "lda.fit(dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_dist = lda.transform(dtm)\n",
    "print(topic_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_topics(model, feature_names, num_top_words):\n",
    "\tfor topic_idx, topic in enumerate(model.components_):\n",
    "\t\tprint(f\"Topic {topic_idx}:\")\n",
    "\t\tprint(\" \".join([feature_names[i] for i in topic.argsort()[:-num_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_top_words = 10\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "display_topics(lda, feature_names, num_top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts, summaries = [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31609, 460)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# max 73_791\n",
    "min_words_text = 30_000\n",
    "for file in crs_files:\n",
    "\twith open(f\"{crs_out}/{file}\") as fp:\n",
    "\t\tdata = json.load(fp)\n",
    "\tif count_words(data[\"text\"]) >= min_words_text:\n",
    "\t\tbreak\n",
    "texts.append(data[\"text\"])\n",
    "summaries.append(data[\"summary\"])\n",
    "\n",
    "count_words(data[\"text\"]), count_words(data[\"summary\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts, summaries = [], []\n",
    "for file in crs_files:\n",
    "\twith open(f\"{crs_out}/{file}\") as fp:\n",
    "\t\tdata = json.load(fp)\n",
    "\ttexts.append(data[\"text\"])\n",
    "\tsummaries.append(data[\"summary\"])\n",
    "\n",
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \n",
       "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
       "  (2): Normalize()\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_checkpoint = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "sent_dir = f\"{data_dir}/Models/Sent-Transformer\"\n",
    "\n",
    "sent_encoder = SentenceTransformer(sent_dir)\n",
    "sent_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "head_size = .5\n",
    "threshold = .85\n",
    "seed = 69\n",
    "device = get_device()\n",
    "device = \"cpu\"\n",
    "\n",
    "encoders = [\n",
    "\tTruncateMiddle(\n",
    "\t\ttokenizer, context_size, head_size, preprocessor\n",
    "\t),\n",
    "\tUniformSampler(\n",
    "\t\ttokenizer, context_size, nltk.sent_tokenize, bos_id, eos_id,\n",
    "\t\tpreprocessor, seed\n",
    "\t),\n",
    "\tSentenceSampler(\n",
    "\t\ttokenizer, context_size, nltk.sent_tokenize, sent_encoder,\n",
    "\t\tbos_id, eos_id, preprocessor, threshold, device, seed\n",
    "\t),\n",
    "\tRemoveRedundancy(\n",
    "\t\ttokenizer, context_size, nltk.sent_tokenize, sent_encoder,\n",
    "\t\tbos_id, eos_id, preprocessor, threshold, device, seed\n",
    "\t),\n",
    "]\n",
    "\n",
    "pipelines = [\n",
    "\tSummarizationPipeline(model, encoder, max_tokens, postprocessor, device)\n",
    "\tfor encoder in encoders\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "batch_size = None if len(texts) < 3 else 3\n",
    "evaluator = Evaluator(\n",
    "\tpipelines, texts, summaries, device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[22529.993814998306, 19385.686802997952, 23740.515432000393, 114102.0073260006]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.generate_summaries(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'rouge-1': array([0.07881505, 0.39801494, 0.04381384]),\n",
       "  'rouge-2': array([0.0092141 , 0.04927629, 0.00508998]),\n",
       "  'rouge-l': array([0.10601738, 0.38872851, 0.06148066]),\n",
       "  'rouge-w': array([0.01665617, 0.20201155, 0.00869101])},\n",
       " {'rouge-1': array([0.07985993, 0.42174518, 0.04421659]),\n",
       "  'rouge-2': array([0.00542675, 0.02924679, 0.00299664]),\n",
       "  'rouge-l': array([0.10913477, 0.40795453, 0.06315046]),\n",
       "  'rouge-w': array([0.0173585 , 0.21706709, 0.00905006])},\n",
       " {'rouge-1': array([0.0865571 , 0.40104155, 0.04883324]),\n",
       "  'rouge-2': array([0.00357995, 0.01363636, 0.00206044]),\n",
       "  'rouge-l': array([0.1112349, 0.3764925, 0.0656536]),\n",
       "  'rouge-w': array([0.01813235, 0.194941  , 0.00954205])},\n",
       " {'rouge-1': array([0.08479274, 0.39276652, 0.04794122]),\n",
       "  'rouge-2': array([0.01062947, 0.04936439, 0.00602607]),\n",
       "  'rouge-l': array([0.10690865, 0.36669271, 0.06307668]),\n",
       "  'rouge-w': array([0.01642158, 0.17955114, 0.00863377])}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.get_rouge_score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([0.8138, 0.8254, 0.8233, 0.8213]),\n",
       " tensor([0.7712, 0.7729, 0.7749, 0.7747]),\n",
       " tensor([0.7919, 0.7983, 0.7983, 0.7972])]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.get_bert_score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uccs-reu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
