{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import torch\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "import inspect\n",
    "\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## gov-report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "crs files: 7238, gao files: 12228\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"/Users/naman/Workspace/Data/UCCS-REU\"\n",
    "\n",
    "crs_files = os.listdir(crs_dir := f\"{data_dir}/GovReport/crs\")\n",
    "gao_files = os.listdir(gao_dir := f\"{data_dir}/GovReport/gao\")\n",
    "\n",
    "print(f\"crs files: {len(crs_files)}, gao files: {len(gao_files)}\")\n",
    "\n",
    "crs_out = f\"{data_dir}/GovReport/crs-processed\"\n",
    "gao_out = f\"{data_dir}/GovReport/gao-processed\"\n",
    "\n",
    "preprocessor = TextPreprocessor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in crs_files:\n",
    "\twith open(f\"{crs_dir}/{file}\") as fp:\n",
    "\t\tdata = json.load(fp)\n",
    "\ttext = combine_subsections([data[\"reports\"]])\n",
    "\ttext = preprocessor.preprocess(text)\n",
    "\tsummary = \"\\n\".join(data[\"summary\"])\n",
    "\tsummary = preprocessor.preprocess(summary)\n",
    "\twith open(f\"{crs_out}/{file}\", \"w\") as fp:\n",
    "\t\tjson.dump({\n",
    "\t\t\t\"text\": text,\n",
    "\t\t\t\"summary\": summary\n",
    "\t\t}, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "for file in gao_files:\n",
    "\twith open(f\"{gao_dir}/{file}\") as fp:\n",
    "\t\tdata = json.load(fp)\n",
    "\ttext = combine_subsections(data[\"report\"])\n",
    "\ttext = preprocessor.preprocess(text)\n",
    "\tprint(data[\"highlight\"])\n",
    "\tsummary = \"\\n\".join(data[\"highlight\"])\n",
    "\tsummary = preprocessor.preprocess(summary)\n",
    "\twith open(f\"{gao_out}/{file}\", \"w\") as fp:\n",
    "\t\tjson.dump({\n",
    "\t\t\t\"text\": text,\n",
    "\t\t\t\"summary\": summary\n",
    "\t\t}, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_dir = f\"{data_dir}/Models/BART/tokenizer\"\n",
    "model_dir = f\"{data_dir}/Models/BART/model\"\n",
    "checkpoint = \"facebook/bart-large-cnn\"\n",
    "\n",
    "tokenizer = BartTokenizer.from_pretrained(tokenizer_dir)\n",
    "model = BartForConditionalGeneration.from_pretrained(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8357, 479)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = f\"{crs_out}/{crs_files[0]}\"\n",
    "\n",
    "with open(file) as fp:\n",
    "\tdata = json.load(fp)\n",
    "count_words(data[\"text\"]), count_words(data[\"summary\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_size, _ = max_lengths(model)\n",
    "context_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10740])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer([data[\"text\"]], return_tensors=\"pt\")\n",
    "inputs[\"input_ids\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    0,   133,   315,   532,  2152,  6636,  3454, 16645,  2052,     7,\n",
       "             5, 39553,     9,     5, 10352,    12, 21426, 44195,  1783,     4,\n",
       "             2,     0,   133,  4033,    74,  2120,     7,  7057, 10352,    12,\n",
       "         21426, 44195,    11,   430,  1319,     6,  1712,  4146,     9,   106,\n",
       "            74,  1622,   769,    12,   225,  7257,     5,  1461, 10352,    12,\n",
       "         21426, 44195, 17947,  2788,    14,    21, 29643,    30, 12209,  3813,\n",
       "             4,     2,     0, 38867,   940, 10514,     7,  2337,  2973,     6,\n",
       "           217,  3881,     7,   696,  1126,  5157,     6,    67,    21,  1202,\n",
       "            13,   650,     8,  1084,    12,  8407,  1520,     6,    11,   233,\n",
       "           142,  5157,  1048,    58,    67, 15902,   148,     5,  2860, 23384,\n",
       "             8,   142,  2735,  3353,   747,    56,    45, 14681,  5157,  1048,\n",
       "          5705,   615,    50,    15,    10,   739,   615,  3189,  2052,     7,\n",
       "             5,  2860, 23384,     7,  5242,  7719,  2851, 31127,    11,     5,\n",
       "           210,     4,     2,     0, 35155,  2156,     5, 11968,     7,  2869,\n",
       "          1906,  1342, 12539,  1783,     9,   570,     4,     2,     0, 34443,\n",
       "           101,     5,   384,  3376,    18,  2600,     9,  7162,   545,     6,\n",
       "             5,  2337,  5091,  3150,  4007, 31600,     9,   209,  7668,     4,\n",
       "             2,     0,   133,  8189,     9,     5, 10352,    12, 21426, 44195,\n",
       "          1783,    18, 23114,  5165,   115,    33,  2325, 14659,  1164,    15,\n",
       "          5501,   223, 13293,  2820,    25,    10,   898,     9,     5,  9643,\n",
       "          3059,    19, 10547,  1520,   108,    36,   118,     4,   242,   482,\n",
       "           613,  1826,   451,    19,   258,  8273, 39415,   827,     8,  8122,\n",
       "            12, 14861,   254, 11609,    43,  3881,     7,  5242,  2557,   210,\n",
       "           458,    11,  5157,  1048,    71, 12209,  3813,    18, 39553,     4,\n",
       "             2,     0,   133,  2337,  8672,   209,   451,     7,   223, 29631,\n",
       "             8,   432,    11,   827,    12, 16653, 41965,  5157,     6,   217,\n",
       "          8185,  3554,   131,  1861,  2225,   131,  5501,    12,  6996,  5157,\n",
       "             8,    97,  2267,    12,  3368,  5157,   131,  2365,  1126,  5157,\n",
       "           131,     8,  2365,  2355,  5157,     4,     2,     0,   133, 10352,\n",
       "            12, 21426, 44195,  1783,   222,    45,  2024,  1100, 22652,  6946,\n",
       "          2820,     4,     2,     0, 43480,   384,  3376, 25233, 43380,  1635,\n",
       "            35, 50118, 50118,  4993,  4746,  2156,     5,   384,  3376,  2334,\n",
       "             7,  3617,  5034,    31,  1520,  2624,   549,  1402,  1713,    58,\n",
       "         37585,    25,   233,     9,    36,   368, 40913,     7,    43,     5,\n",
       "            22, 11880,     9,  3454,   113,   223, 10352,    12, 21426, 44195,\n",
       "          1783,  7162,   545,     4,     2,     0,   713, 16192,    16,  4249,\n",
       "           142,     6,    25,  1433,  3373,    11,     5,    22, 24474,    22,\n",
       "          2810,     9,    42,   266,     6,  2111,    12,  9756,  5157,  1713,\n",
       "             6,   215,    25,  3501,    25,    10,   210,    12,  5406,    50,\n",
       "          8122,     6,   109,    45, 13595,  2566,     7,    25,   203,     9,\n",
       "             5,   425,    12, 13728, 25979,   810,    25, 14101,  1446,     4,\n",
       "             2,     0,   133,  5896,  8956, 16654,  3489, 20026,    22,   428,\n",
       "         20327,  8866,   113,    36,   118,     4,   242,   482,  1861,  1520,\n",
       "             8,    49, 20347,    43,    31,  7580,    11,     5, 14101,  1446,\n",
       "             9,  5157,     6,    53,  8621,  2111,    12,  9756,  5157,  5538,\n",
       "             4,     2,     0, 33099, 31226,  1048,    32, 22646, 11788,   142,\n",
       "          5157,   850,    32,  9234,     4,     2,     0,   133,   384,  3376,\n",
       "            56,  1167,  3478, 36103,  1520,     7,   904, 41880,   915,  1188,\n",
       "             6,    61,    21,  6835,    30,    10,  5157,   539,   333,    25,\n",
       "         17976,     5,  7401,     9,   827,  3446,  1286,    30,     5, 10352,\n",
       "            12, 21426, 44195,  1783,     4,     2,     0,  2765,     5,    86,\n",
       "         12209,  3813,    21, 14673,     6,     5,  2337,    56,  2033,  2975,\n",
       "          3329,    23,   513,  3492,    22,   844, 11609,    72,     2,     0,\n",
       "           970, 10522,    16,    10,    55,  2228,  2748,   227,  8068,  1911,\n",
       "             8,     5, 11801,     9,   827,  1237,    30, 27649,  9314,    87,\n",
       "          5165,    15,     5,  5157,  1713,     9,  1861,  1520,   142, 24064,\n",
       "          2371,  1911,  7540, 15719, 27649,  9314,    31,   827,  2687,     4,\n",
       "             2,     0,  4528,  4504,  1165,     5, 12539,  1783,     9, 26873,\n",
       "             6,    25,   157,    25,     5,  3484,  1783,     9, 26873,     8,\n",
       "             5,  3484,     8,  3080,  1783,     9, 28955,     4,     2,     0,\n",
       "         10787,     9,   209,  1520,    58,    45,   453,     9,     5,  1853,\n",
       "          3965,  5149,     8,     6,  3891,     6,   222,    45,    33,   899,\n",
       "             7,     5,  2337,    18,  6946,  2644,     4,     2,     0,  2709,\n",
       "          1246,     6,    10,   418,   210,  1391,     6,    20,  3965, 11518,\n",
       "          2896,     6,  2152,  2687,    15,  1126,  1167,    30,    10,  8122,\n",
       "            12, 14861,   254,     6,    61,  3284,   669,     5,   923,     9,\n",
       "            63,   327,     7,    22, 10339,    12,   627,    12, 28777,   113,\n",
       "            36,   118,     4,   242,   482,  1136,   874,  2242,     9,    68,\n",
       "           134,     4,   612,   228,   458,    43,    11,  2266,     4,     2,\n",
       "             0,   133,   384,  3376,     8,  2337,     6, 37212, 10352,    12,\n",
       "         21426, 44195,    18, 18262,  4405,  2777,     8, 25499,  1295,    15,\n",
       "          1713, 37585,   223,     5,   163, 13459,  1783,    18,    22, 22641,\n",
       "           352,    12,  3368,   113,  6397,     6,  9097, 11047,  1861,  1520,\n",
       "             7,  4949,    11,    41,  2284,   346,     9,  1713, 32947,  2065,\n",
       "          5157,   785,     8,   518,     4,     2,     0, 41895, 19245,     9,\n",
       "            10,   671,     7, 10352,    12, 21426, 44195,   189,  5848,    14,\n",
       "           613,  5443,    11,     5,   315,   532,  2092,     7,    33,  2782,\n",
       "            11,     5,   675,    71, 10352,    12, 21426, 44195,    21,  1595,\n",
       "             4,     2,     0, 16991, 15059,    41, 37435,    12,   560,    12,\n",
       "         17165, 25006,  1421,     8,  7580,    11,  1403,    12,   417, 27638,\n",
       "           115,  3838,  2463,  5501,  9813,  3629,     7,  4023,   338,  1906,\n",
       "          7240, 30686,   223, 13293,  2820,     4,     2,     0,   133, 18753,\n",
       "            12, 16025,  1783,  3881,     7,  3720,   613,  5443,     8,     7,\n",
       "          3000,   103,     9,     5,   801, 21779,  1713,     9,  1861,  1520,\n",
       "            11,  1337,  1319,     6,   215,    25,   149,     5,  5896,  8956,\n",
       "         16654,     6,    53,    24,   473,    45,  2198,  7057,     5, 10352,\n",
       "            12, 21426, 44195,  1783,     4,     2,     0,  4148,     5,    65,\n",
       "           865,     6,     5, 10875,     9,  1861,  3454,    31,  5157, 14787,\n",
       "            64,  8924,    41, 37435,    12,   560,    12, 17165, 25006,  1421,\n",
       "           114,  5157,  1048,   244,  1391,  1126,     4,     2,     0,   243,\n",
       "            67,   115,   244,  1744,     5,  3207,   467,    30,  9107,  2687,\n",
       "            11,  5157,  1048,    31,  3735,  8273, 39415,  1520,     7,  5998,\n",
       "             4,     2,     0,   133, 20347, 36450,  5157,  2164,    30,   106,\n",
       "            15, 32276,   867,     8, 27649,  9314,     4,     2,     0, 31990,\n",
       "           963,    10,  1539,     7,     5,   384,  3376,    18,  3446,     7,\n",
       "         29080,   632,  1520,     7,   904, 41880,   915,  1188,     4,     2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_input = pick_sents([data[\"text\"]], nltk.sent_tokenize, tokenizer, context_size)\n",
    "model_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    2,     0,   133, 10352,    12, 21426, 44195,  1783,    21,  1595,\n",
       "            11, 26873,     4,    85,    21,  3833,     7,  2097,  1520,    31,\n",
       "           602,    15,   350,   203,  1126,     4,    20,  1783,    21,   423,\n",
       "         13522,     7,  1157,     5,  1520,     7,   185,    15,    55,  1126,\n",
       "             4,    85,    67,  1220,   106,     7,  1331,   103,     9,     5,\n",
       "          1126,     7,    97,  1520,     4,    20,  1760,    21, 29643,    11,\n",
       "          4013,     4,     2]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = model.generate(**model_input, max_length=500)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'</s><s>The Glass-Steagall Act was passed in 1933. It was intended to prevent banks from taking on too much debt. The Act was later amended to allow the banks to take on more debt. It also allowed them to sell some of the debt to other banks. The act was repealed in 2005.</s>'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The phrase \"Glass-Steagall\" generally refers to the separation of commercial banking from investment banking. Congress effected a separation of commercial and investment banking through four sections of the Banking Act of 1933Sections 16, 20, 21, and 32. These four statutory provisions are commonly referred to as the Glass-Steagall Act.\\nKey Takeaways of This Report\\nThe Glass-Steagall debate is not centered on prohibiting risky financial services; rather, the debate is about whether to permit inherently risky commercial and investment banking activities to be conducted within a single firmspecifically within firms holding federally insured deposits. Over the course of the nearly 70-year-long Glass-Steagall era, the clear-cut separation of traditional commercial banking and securities activities gradually eroded. This erosion was the result of a confluence of matters, including market changes, statutory changes, and regulatory and judicial interpretations. The Glass-Steagall era formally ended in 1999 when the Gramm-Leach-Bliley Act (GLBA) repealed the Glass-Steagall Act\\'s restrictions on affiliations between commercial and investment banks. Less than a decade after GLBA, the United States suffered its worst financial crisis since the Great Depression. Some have argued that the partial repeal either was a cause of the financial crisis that resulted in the so-called Great Recession or that it fueled and worsened the crisis\\'s deleterious effect. On the other hand, some policymakers argue that Glass-Steagall issues were not significant causes of the crisis, and that the Glass-Steagall Act would have made responding to the crisis more difficult if it had remained in place. The Dodd-Frank Wall Street Reform and Consumer Protection Act (Dodd-Frank Act; P.L. 111-203) was Congress\\'s primary legislative prescription to prevent a similar financial crisis in the future. The Dodd-Frank Act neither reinstated the sections of the Glass-Steagall Act that were repealed by GLBA nor substantially modified the ability of banking firms to affiliate with securities firms. It did, however, include some arguably Glass-Steagall-like provisions, which were designed to promote financial stability going forward, reduce various speculative activities of commercial banks, and reduce the likelihood that the U.S. government would have to provide taxpayer support to avert or minimize a future financial crisis. Some believe that a more effective way of accomplishing these policy objectives would be to fully reinstate the Glass-Steagall Act. In fact, multiple bills have been introduced in the 114th Congress with that stated purpose. These bills include: S. 1709/H.R. 3054, The 21st Century Glass-Steagall Act of 2015, and H.R. 381, the Return to Prudent Banking Act of 2015. On the other side of the policy discussion, some argue that the Glass-Steagall Act is ill-suited for the current financial system and that the recent financial crisis would have occurred even if GLBA had never partially repealed the Glass-Steagall Act. Even if the Dodd-Frank Act had completely re-enacted the repealed provisions of the Glass-Steagall Act, the financial history of the Glass-Steagall era shows that regulatory walls could be difficult to maintain or enforce.'"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"summary\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10740])"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_tokenized = tokenizer([data[\"text\"]], return_tensors=\"pt\")[\"input_ids\"]\n",
    "all_tokenized.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    0, 43480, 24474,  ..., 10914,     4,     2]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]])}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp = truncate_middle([data[\"text\"]], tokenizer, context_size, .4)\n",
    "inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    2,     0, 44189,    12, 21426, 44195,  1783,  8254,  1861,  3454,\n",
       "            31,   915,  3454,     4,  6974, 36466,     9,  1861,     8,   915,\n",
       "          3454,    64,   244,  7540, 10246, 24323,  8068,  9023,    31,  4975,\n",
       "            11,  5157,  1048,     4,    20, 10875,     6,    30,  1495,     6,\n",
       "           473,    45,  1100,   141,   915,  1520,    32, 13588,   624,  5157,\n",
       "          1048,    50,   141,   786, 27045,    64,   304,  5157,  1713,     7,\n",
       "          1391,  2267,     8,  1861,  1126,     4,     2]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = model.generate(**inp, max_length=500)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'</s><s>Glass-Steagall Act separated commercial banking from investment banking. Separation of commercial and investment banking can help insulate insured depositories from volatility in securities markets. The separation, by itself, does not address how investment banks are regulated within securities markets or how nonbanks can use securities activities to fund consumer and commercial debt.</s>'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(out[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uccs-reu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
