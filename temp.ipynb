{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sys import getsizeof\n",
    "from time import sleep\n",
    "import pickle\n",
    "import json\n",
    "import re\n",
    "import inspect\n",
    "from warnings import filterwarnings\n",
    "\n",
    "import numpy as np\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from transformers import (\n",
    "\tBartTokenizer, BartForConditionalGeneration,\n",
    "\tT5Tokenizer, T5ForConditionalGeneration,\n",
    "\tPegasusForConditionalGeneration, PegasusTokenizerFast,\n",
    "\tGPT2TokenizerFast\n",
    ")\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from utils.helpers import *\n",
    "from utils.encoders import *\n",
    "from utils.pipelines import *\n",
    "from utils.trainer_utils import *\n",
    "from utils.evaluator_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_histogram(data):\n",
    "\tbins = int(len(data) ** .5)\n",
    "\tplt.hist(data, bins=bins)\n",
    "\tplt.show()\n",
    "\n",
    "inf = float(\"inf\")\n",
    "filterwarnings(\"ignore\")\n",
    "device = get_device(500)\n",
    "# device = \"cpu\"\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/Users/naman/Workspace/Data/Long-Document-Summarization\"\n",
    "data_dir = \"/home/nchibbar/Data\"\n",
    "\n",
    "bart_dir = f\"{data_dir}/Models/BART\"\n",
    "t5_dir = f\"{data_dir}/Models/T5\"\n",
    "pegasus_dir = f\"{data_dir}/Models/PEGASUS\"\n",
    "gpt_dir = f\"{data_dir}/Models/GPT-3.5-turbo-tokenizer\"\n",
    "\n",
    "govreport_dir = f\"{data_dir}/GovReport/processed\"\n",
    "bigpatent_dir = f\"{data_dir}/BigPatent/processed\"\n",
    "govreport_files = os.listdir(govreport_dir)\n",
    "bigpatent_files = os.listdir(bigpatent_dir)\n",
    "\n",
    "len(govreport_files), len(bigpatent_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentence transformer\n",
    "# Automatically loads into gpu if available\n",
    "sent_dir = f\"{data_dir}/Models/Sent-Transformer\"\n",
    "sent_encoder = SentenceTransformer(sent_dir)\n",
    "\n",
    "name = \"pegasus\"\n",
    "\n",
    "match name:\n",
    "\n",
    "\tcase \"bart\":\n",
    "\t\ttokenizer = BartTokenizer.from_pretrained(bart_dir)\n",
    "\t\tmodel = BartForConditionalGeneration.from_pretrained(bart_dir)\n",
    "\t\tcontext_size = model.config.max_position_embeddings\n",
    "\n",
    "\tcase \"t5\":\n",
    "\t\ttokenizer = T5Tokenizer.from_pretrained(t5_dir)\n",
    "\t\tmodel = T5ForConditionalGeneration.from_pretrained(t5_dir)\n",
    "\t\tcontext_size = model.config.n_positions\n",
    "\n",
    "\tcase \"pegasus\":\n",
    "\t\ttokenizer = PegasusTokenizerFast.from_pretrained(pegasus_dir)\n",
    "\t\tmodel = PegasusForConditionalGeneration.from_pretrained(pegasus_dir)\n",
    "\t\tcontext_size = model.config.max_position_embeddings\n",
    "\n",
    "\tcase \"gpt\":\n",
    "\t\ttokenizer = GPT2TokenizerFast.from_pretrained(gpt_dir)\n",
    "\t\tmodel = \"gpt-3.5-turbo\"\n",
    "\t\tcontext_size = 4096\n",
    "\n",
    "context_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = TextProcessor(preprocessing=True)\n",
    "postprocessor = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BigPatent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts = []\n",
    "for file in bigpatent_files:\n",
    "\tfile_path = f\"{bigpatent_dir}/{file}\"\n",
    "\twith open(file_path) as fp:\n",
    "\t\tdata = json.load(fp)\n",
    "\tfor text in data[\"texts\"]:\n",
    "\t\tword_counts.append(count_words(text))\n",
    "\n",
    "bins = int(len(word_counts) ** .5)\n",
    "plt.hist(word_counts, bins=bins)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(word_counts), min(word_counts), np.mean(word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum([\n",
    "\t1\n",
    "\tfor count in word_counts\n",
    "\tif count > 40_000\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_words = 20_000\n",
    "max_words = inf\n",
    "max_texts = inf\n",
    "texts, summaries = [], []\n",
    "num_texts = 0\n",
    "for file in govreport_files:\n",
    "\tfile_path = f\"{govreport_dir}/{file}\"\n",
    "\twith open(file_path) as fp:\n",
    "\t\tdata = json.load(fp)\n",
    "\tif min_words < count_words(data[\"text\"]) < max_words:\n",
    "\t\ttexts.append(data[\"text\"])\n",
    "\t\tsummaries.append(data[\"summary\"])\n",
    "\t\tnum_texts += 1\n",
    "\tif num_texts == max_texts:\n",
    "\t\tbreak\n",
    "\n",
    "num_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_min_words = 20\n",
    "text_segmenter = TextSegmenter(nltk.sent_tokenize, segment_min_words)\n",
    "keywords_preprocessor = TextProcessor(\n",
    "\tonly_words_nums = True,\n",
    "\tremove_nums = True\n",
    ")\n",
    "stop_words = get_stop_words(extra_stop_words=STOP_WORDS)\n",
    "len(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_tokens_frac = .5\n",
    "min_summary_tokens = 300\n",
    "head_size = .5\n",
    "threshold = .7\n",
    "boost = .03\n",
    "num_keywords = 20\n",
    "seed = 69\n",
    "system_prompt = \"You will be given some segments of a very long document. Your task is to summarize the entire document as a whole by extracting key information and ideas from the segments. Generate a detailed, concise, and coherent summary in 500 words. Do not refer to the document in the summary in any way.\"\n",
    "\n",
    "temperature = 2.\n",
    "repetition_penalty = 3.\n",
    "top_p = .95\n",
    "\n",
    "encoders = [\n",
    "\tTruncateMiddle(\n",
    "\t\ttokenizer, context_size, 1, preprocessor\n",
    "\t),\n",
    "\tTruncateMiddle(\n",
    "\t\ttokenizer, context_size, head_size, preprocessor, True\n",
    "\t),\n",
    "\tUniformSampler(\n",
    "\t\ttokenizer, min_tokens_frac * context_size, context_size,\n",
    "\t\ttext_segmenter, preprocessor, True, seed\n",
    "\t),\n",
    "\tSegmentSampler(\n",
    "\t\ttokenizer, min_tokens_frac * context_size, context_size,\n",
    "\t\ttext_segmenter, sent_encoder, preprocessor, threshold, boost, seed\n",
    "\t),\n",
    "\tRemoveRedundancy(\n",
    "\t\ttokenizer, min_tokens_frac * context_size, context_size,\n",
    "\t\ttext_segmenter, sent_encoder, preprocessor, threshold, seed\n",
    "\t),\n",
    "\tKeywordScorer(\n",
    "\t\ttokenizer, context_size, text_segmenter, sent_encoder,\n",
    "\t\tpreprocessor, num_keywords, keywords_preprocessor, stop_words\n",
    "\t)\n",
    "]\n",
    "\n",
    "match name:\n",
    "\n",
    "\tcase \"gpt\":\n",
    "\t\tgpt_pipelines = [\n",
    "\t\t\tOpenAIPipeline(\n",
    "\t\t\t\tmodel, enc, system_prompt=system_prompt\n",
    "\t\t\t) for enc in encoders\n",
    "\t\t]\n",
    "\n",
    "\tcase _:\n",
    "\t\tpipelines = [\n",
    "\t\t\tSummarizationPipeline(\n",
    "\t\t\t\tmodel, enc, postprocessor, min_summary_tokens,\n",
    "\t\t\t\tcontext_size, device, temperature, repetition_penalty, top_p\n",
    "\t\t\t) for enc in encoders\n",
    "\t\t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_texts = preprocessor(texts)\n",
    "max_word_segments = []\n",
    "avg_segment = 0\n",
    "total_segments = 0\n",
    "for text in processed_texts:\n",
    "\tsegments = text_segmenter(text)\n",
    "\tfor segment in segments:\n",
    "\t\tavg_segment += count_words(segment)\n",
    "\t\ttotal_segments += 1\n",
    "\tmax_word_segments.append(\n",
    "\t\tmax(segments, key=lambda segment: count_words(segment))\n",
    "\t)\n",
    "avg_segment /= total_segments\n",
    "avg_segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bearable_words = 150\n",
    "for i, segment in enumerate(max_word_segments):\n",
    "\twords = count_words(segment)\n",
    "\tif words > bearable_words:\n",
    "\t\tprint(\n",
    "\t\t\tf\"{i} {words}: {repr(segment)}\",\n",
    "\t\t\tend = \"\\n\\n\"\n",
    "\t\t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = 7143\n",
    "print(\n",
    "\tprocessed_texts[ind], texts[ind],\n",
    "\tsep = f\"\\n\\n{\"=\" * 400}\\n\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{data_dir}/pegasus-govreport.pkl\", \"rb\") as fp:\n",
    "\tresults = pickle.load(fp)\n",
    "scores = results[\"scores\"]\n",
    "sort1, sort2, sort3 = results[\"sort1\"], results[\"sort2\"], results[\"sort3\"]\n",
    "gen_summaries = results[\"gen_summaries\"]\n",
    "scores[0][sort1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = 0\n",
    "problem_text = results[\"texts\"][sort1[ind]]\n",
    "print(gen_summaries[sort1[ind]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pipelines[-1](problem_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = encoders[-1](problem_text, return_batch=False)\n",
    "sent = tokenizer.decode(enc, skip_special_tokens=True)\n",
    "print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_problem = preprocessor(problem_text)\n",
    "segments = text_segmenter(processed_problem)\n",
    "segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results[\"summaries\"][sort1[ind]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uccs-reu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
