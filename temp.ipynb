{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sys import getsizeof\n",
    "import json\n",
    "import re\n",
    "import pickle\n",
    "import inspect\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "\n",
    "import numpy as np\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from transformers import (\n",
    "\tBartTokenizer, BartForConditionalGeneration,\n",
    "\tT5Tokenizer, T5ForConditionalGeneration,\n",
    "\tGPT2TokenizerFast\n",
    ")\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from dotenv import load_dotenv\n",
    "import tiktoken\n",
    "import openai\n",
    "\n",
    "from utils.helpers import *\n",
    "from utils.encoders import *\n",
    "from utils.pipelines import *\n",
    "from utils.trainer_utils import *\n",
    "from utils.evaluator_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "crs files: 7238, gao files: 12228\n"
     ]
    }
   ],
   "source": [
    "# data_dir = \"/Users/naman/Workspace/Data/UCCS-REU\"\n",
    "data_dir = \"/home/nchibbar/Data\"\n",
    "\n",
    "crs_files = os.listdir(crs_dir := f\"{data_dir}/GovReport/crs\")\n",
    "gao_files = os.listdir(gao_dir := f\"{data_dir}/GovReport/gao\")\n",
    "\n",
    "print(f\"crs files: {len(crs_files)}, gao files: {len(gao_files)}\")\n",
    "\n",
    "crs_out = f\"{data_dir}/GovReport/crs-processed\"\n",
    "gao_out = f\"{data_dir}/GovReport/gao-processed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_tokens = 512\n",
    "\n",
    "# Sentence transformer\n",
    "sent_dir = f\"{data_dir}/Models/Sent-Transformer\"\n",
    "sent_checkpoint = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "sent_encoder = SentenceTransformer(sent_dir)\n",
    "\n",
    "# BART\n",
    "bart_dir = f\"{data_dir}/Models/BART\"\n",
    "bart_fine_tuned = f\"{data_dir}/Models/BART-GovReport-SentenceSampler\"\n",
    "bart_checkpoint = \"facebook/bart-large-cnn\"\n",
    "tokenizer = BartTokenizer.from_pretrained(bart_dir)\n",
    "model = BartForConditionalGeneration.from_pretrained(bart_fine_tuned)\n",
    "context_size = model.config.max_position_embeddings\n",
    "\n",
    "# T5\n",
    "# t5_dir = f\"{data_dir}/Models/T5\"\n",
    "# t5_checkpoint = \"google/flan-t5-base\"\n",
    "# t5_checkpoint = \"pszemraj/long-t5-tglobal-base-16384-book-summary\"\n",
    "# tokenizer = T5Tokenizer.from_pretrained(t5_dir)\n",
    "# model = T5ForConditionalGeneration.from_pretrained(t5_dir)\n",
    "# context_size = model.config.n_positions\n",
    "\n",
    "# GPT 3.5 turbo tokenizer\n",
    "gpt_dir = f\"{data_dir}/Models/GPT-3.5-turbo-tokenizer\"\n",
    "gpt_tokenizer = GPT2TokenizerFast.from_pretrained(gpt_dir)\n",
    "\n",
    "context_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>', '</s>', '<unk>', '</s>', '<pad>', '<s>', '<mask>']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "special_tokens = extract_special_tokens(\n",
    "\ttokenizer.special_tokens_map.values()\n",
    ")\n",
    "preprocessor = TextProcessor(preprocessing=True)\n",
    "postprocessor = None\n",
    "# postprocessor = TextProcessor(ignore_tokens=special_tokens)\n",
    "special_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "head_size = .5\n",
    "threshold = .7\n",
    "seed = 69\n",
    "device = get_device()\n",
    "device = \"cpu\"\n",
    "\n",
    "encoders = [\n",
    "\tTruncateMiddle(\n",
    "\t\ttokenizer=tokenizer, max_tokens=context_size,\n",
    "\t\thead_size=head_size, preprocessor=preprocessor\n",
    "\t),\n",
    "\tUniformSampler(\n",
    "\t\ttokenizer=tokenizer, max_tokens=context_size,\n",
    "\t\tsent_tokenizer=nltk.sent_tokenize, preprocessor=preprocessor,\n",
    "\t\tseed=seed\n",
    "\t),\n",
    "\tSentenceSampler(\n",
    "\t\ttokenizer=tokenizer, max_tokens=context_size,\n",
    "\t\tsent_tokenizer=nltk.sent_tokenize, sent_encoder=sent_encoder,\n",
    "\t\tpreprocessor=preprocessor, threshold=threshold,\n",
    "\t\tdevice=device, seed=seed\n",
    "\t),\n",
    "\tRemoveRedundancy(\n",
    "\t\ttokenizer=tokenizer, max_tokens=context_size,\n",
    "\t\tsent_tokenizer=nltk.sent_tokenize, sent_encoder=sent_encoder,\n",
    "\t\tpreprocessor=preprocessor, threshold=threshold,\n",
    "\t\tdevice=device, seed=seed\n",
    "\t)\n",
    "]\n",
    "\n",
    "pipelines = [\n",
    "\tSummarizationPipeline(\n",
    "\t\tmodel, encoder, max_tokens, postprocessor, device\n",
    "\t) for encoder in encoders\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GovReport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_subsections(sections):\n",
    "\ttext = \"\"\n",
    "\tfor sec in sections:\n",
    "\t\tsec_text = \"\\n\\n\".join(sec[\"paragraphs\"])\n",
    "\t\tif sec[\"section_title\"]:\n",
    "\t\t\tsec_text = f\"Section {sec[\"section_title\"]}:\\n\\n{sec_text}\"\n",
    "\t\ttext = f\"{text}\\n\\n{sec_text}\" if text else sec_text\n",
    "\t\tif sec[\"subsections\"]:\n",
    "\t\t\tsub_text = combine_subsections(sec[\"subsections\"])\n",
    "\t\t\ttext = f\"{text}\\n\\n{sub_text}\" if text else sub_text\n",
    "\treturn text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in crs_files:\n",
    "\twith open(f\"{crs_dir}/{file}\") as fp:\n",
    "\t\tdata = json.load(fp)\n",
    "\ttext = combine_subsections([data[\"reports\"]])\n",
    "\ttext = preprocessor.process(text)\n",
    "\tsummary = \"\\n\".join(data[\"summary\"])\n",
    "\tsummary = preprocessor.process(summary)\n",
    "\twith open(f\"{crs_out}/{file}\", \"w\") as fp:\n",
    "\t\tjson.dump({\n",
    "\t\t\t\"text\": text,\n",
    "\t\t\t\"summary\": summary\n",
    "\t\t}, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in gao_files:\n",
    "\twith open(f\"{gao_dir}/{file}\") as fp:\n",
    "\t\tdata = json.load(fp)\n",
    "\ttext = combine_subsections(data[\"report\"])\n",
    "\ttext = preprocessor.process(text)\n",
    "\tprint(data[\"highlight\"])\n",
    "\tsummary = \"\\n\".join(data[\"highlight\"])\n",
    "\tsummary = preprocessor.preprocess(summary)\n",
    "\twith open(f\"{gao_out}/{file}\", \"w\") as fp:\n",
    "\t\tjson.dump({\n",
    "\t\t\t\"text\": text,\n",
    "\t\t\t\"summary\": summary\n",
    "\t\t}, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(stop_words=\"english\")\n",
    "vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm = vectorizer.fit_transform([data[\"text\"]])\n",
    "vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = 4\n",
    "lda = LatentDirichletAllocation(n_components=topics)\n",
    "lda.fit(dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_dist = lda.transform(dtm)\n",
    "print(topic_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_topics(model, feature_names, num_top_words):\n",
    "\tfor topic_idx, topic in enumerate(model.components_):\n",
    "\t\tprint(f\"Topic {topic_idx}:\")\n",
    "\t\tprint(\" \".join([feature_names[i] for i in topic.argsort()[:-num_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_top_words = 10\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "display_topics(lda, feature_names, num_top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts, summaries = [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(53559, 500)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# max 73_791\n",
    "min_words_text = 50_000\n",
    "for file in crs_files:\n",
    "\twith open(f\"{crs_out}/{file}\") as fp:\n",
    "\t\tdata = json.load(fp)\n",
    "\tif count_words(data[\"text\"]) >= min_words_text:\n",
    "\t\tbreak\n",
    "texts.append(data[\"text\"])\n",
    "summaries.append(data[\"summary\"])\n",
    "\n",
    "count_words(data[\"text\"]), count_words(data[\"summary\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7238"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts, summaries = [], []\n",
    "for file in crs_files:\n",
    "\twith open(f\"{crs_out}/{file}\") as fp:\n",
    "\t\tdata = json.load(fp)\n",
    "\ttexts.append(data[\"text\"])\n",
    "\tsummaries.append(data[\"summary\"])\n",
    "\n",
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<s>Participation data: In June 2009, a total of 18 million families, composed of 43 million recipients (including 33 million children), received TANF- or MOE-funded cash assistance In June 2010, a total of 19 million families, composed of 45 million recipients (including 34 million children), received TANF- or MOE-funded cash assistance The larger number of individuals or families receiving any TANF- or MOE-funded benefit or service is not known\\n\\nCRS report: CRS Report R40946, The Temporary Assistance for Needy Families Block Grant: An Introduction, by [author name scrubbed].\\nIn some limited circumstances, families may be low-income, with incomes as high as 80% of area median income\\n\\nForm and recipient of federal assistance: Project-based rental assistance contracts between HUD and private property owners HUD has not had the authority to enter into new contracts since 1983, but does have the authority to renew existing contracts when they expire There are properties with project-based rental assistance contracts in the territories (U.S Virgin Islands, Puerto Rico, and Guam).\\nAllocation formula: Portions of available annual funds are allocated under four different formulasBasic, Concentration, Targeted, and Education Finance Incentive Grants (EFIG)although funds are then combined and used for the same purposes by recipient LEAs Although the allocation formulas have several distinctive elements, the primary factors used in all four formulas are an eligible child count and an expenditure factor The eligible child count includes children aged 5-17: (a) in poor families; (b) in institutions for neglected or delinquent children or in foster homes; and (c) in families receiving Temporary Assistance for Needy Families payments above the poverty level Each element of the population factor is updated annually The expenditure factor is the state average per pupil expenditure for public K-12 education (subject to a minimum of 80% and maximum of 120% of the national average, further multiplied by 040), and is the same for all LEAs in the same state Both the Targeted and EFIG formulas include weighting schemes to increase aid to LEAs with the highest numbers or concentrations of eligible children The EFIG formula also includes an effort factor, based on average per pupil expenditure for public K-12 education compared to personal income per capita for each state compared to the nation as a whole, and an equity factor, based on variations in average per pupil expenditures among the LEAs in each state Each formula has a hold-harmless provision (no LEA may receive less than 85%-95% of its previous year grant, depending on the LEA's poverty level and whether the LEA continues to meet the formula's eligibility threshold).\\nNew obligations: FY2009: $2.324 billion FY2008: $2.038 billion\\n\\nBudgetary classification: Mandatory (open-ended entitlement to states).\\n</s>\""
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp = tokenizer.decode(encoders[2](texts)[\"input_ids\"][0])\n",
    "inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "\tresponse = openai.chat.completions.create(\n",
    "\t\tmodel=\"gpt-3.5-turbo\",\n",
    "\t\tmessages=[\n",
    "\t\t\t{\"role\": \"system\", \"content\": \"\"},\n",
    "\t\t\t{\"role\": \"user\", \"content\": \"\"},\n",
    "\t\t\t{\"role\": \"assistant\", \"content\": \"\"},\n",
    "\t\t\t{\"role\": \"user\", \"content\": \"\"},\n",
    "\t\t\t{\"role\": \"assistant\", \"content\": \"\"},\n",
    "\t\t\t{\"role\": \"user\", \"content\": \"\"},\n",
    "\t\t],\n",
    "\t\tmax_tokens=10\n",
    "\t)\n",
    "except Exception as e:\n",
    "\tshow_exception(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CompletionUsage(completion_tokens=7, prompt_tokens=27, total_tokens=34)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello! How can I assist you today?'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_tokens(\"system\", gpt_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You are an expert summarizer.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system_prompt = \"You are an expert summarizer. Your task is to summarize long texts into concise and informative summaries. When provided with key segments of a longer text, please summarize these segments while ensuring the main ideas and important details are preserved.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OpenAIPipeline:\n",
    "\n",
    "\tdef __init__(\n",
    "\t\tself, model: str, encoder: Encoder, max_tokens: int,\n",
    "\t\tprompt_template: str=\"\", system_prompt: str=\"\"\n",
    "\t) -> None:\n",
    "\t\tself.model = model\n",
    "\t\tself.encoder = encoder\n",
    "\t\tself.max_tokens = max_tokens\n",
    "\t\tself.prompt_template = prompt_template\n",
    "\t\tself.system_prompt = system_prompt\n",
    "\t\tself.call_inputs = None\n",
    "\t\tself.response = None\n",
    "\t\n",
    "\tdef create_inputs(\n",
    "\t\tself, text: str, previous_messages: list[str]|None=None\n",
    "\t):\n",
    "\t\tencoder = self.encoder\n",
    "\t\tmax_tokens = self.max_tokens\n",
    "\t\tprompt_template = self.prompt_template\n",
    "\t\ttokenizer = encoder.tokenizer\n",
    "\n",
    "\t\t# Tokens used to create OpenAI prompt template\n",
    "\t\t# 3 tokens for prompt base\n",
    "\t\t# 4 tokens each for every message\n",
    "\t\tnum_prev_msgs = 0 if previous_messages is None else len(previous_messages)\n",
    "\t\ttokens_used = 3 + 4 * (2 * num_prev_msgs + 1)\n",
    "\n",
    "\t\t# Create system prompt\n",
    "\t\tsystem_prompt = self.system_prompt\n",
    "\t\tmessages = []\n",
    "\t\tif system_prompt:\n",
    "\t\t\tmessages.append({\"role\": \"system\", \"content\": system_prompt})\n",
    "\t\t\ttokens_used += count_tokens(system_prompt, tokenizer) + 4\n",
    "\t\tif num_prev_msgs:\n",
    "\t\t\tfor text, summary in previous_messages:\n",
    "\t\t\t\tmessages.append({\"role\": \"user\", \"content\": text})\n",
    "\t\t\t\tmessages.append({\"role\": \"assistant\", \"content\": summary})\n",
    "\t\t\t\ttokens_used += count_tokens([text, summary], tokenizer)\n",
    "\t\ttokens_used += count_tokens(prompt_template, tokenizer)\n",
    "\t\tencodings = encoder(text, max_tokens - tokens_used)\n",
    "\t\ttext = tokenizer.decode(encodings, ignore_special_tokens=True)\n",
    "\t\tprompt = f\"{prompt_template}{text}\"\n",
    "\t\tmessages.append({\"role\": \"user\", \"content\": prompt})\n",
    "\t\tself.call_inputs = {\n",
    "\t\t\t\"model\": self.model,\n",
    "\t\t\t\"messages\": messages,\n",
    "\t\t\t\"max_tokens\": max_tokens\n",
    "\t\t}\n",
    "\t\treturn tokens_used\n",
    "\t\n",
    "\tdef send_call(self):\n",
    "\t\tcall_inputs = self.call_inputs\n",
    "\t\tassert call_inputs is not None, \"Call inputs not created\"\n",
    "\t\ttry:\n",
    "\t\t\tself.response = openai.chat.completions.create(**call_inputs)\n",
    "\t\texcept Exception as e:\n",
    "\t\t\tshow_exception(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uccs-reu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
