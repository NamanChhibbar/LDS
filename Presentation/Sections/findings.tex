\section{Experiments}


	\subsection{Summarizers Used}

		\begin{frame}{Summarizers Used}

			We have tested our methods on the following summarizers:

			\begin{itemize}
				\item \textbf{BART} (Bidirectional and Auto-Regressive Transformer)
				\citep{lewis-etal-2020-bart} fine-tuned on the CNN/Daily Mail dataset
				with a context size of 1024.
				\item \textbf{LongT5} \citep{guo2021longt5}, a variant of Text-to-Text
				Transfer Transformer (T5) \citep{raffel2020exploring} fine-tuned on the
				BookSum dataset with a context size of 4096.
				\item \textbf{GPT-3.5 Turbo} \citep{brown2020language} with a context
				size of 4096.
			\end{itemize}

		\end{frame}


	\subsection{Findings}

		\begin{frame}{Experimental Findings}

			\begin{table}[!ht]
				\centering
				\tiny

				\begin{tabular}{c c c c c}
					\hline
					Model & ROUGE-1 & ROUGE-2 & ROUGE-L & BERTScore \\
					\hline
					BART w/ & 53.4 & 22.5 & 22.5 & 66.0 \\
					Unlimiformer (1,024) & & & & \\
					PRIMERA w/ & 56.5 & 24.8 & 26.3 & 67.7 \\
					Unlimiformer (4,096) & & & & \\
					Hepos (10,240) & 51.34 & 19.09 & \textbf{48.73} & - \\
					PEGASUS-X w/ Staggered & 60.3 & \textbf{30.0} & 31.5 & - \\
					Block-Local Attention (16k) & & & & \\
					LLaMA-7B w/ Positional & 60.0 & 28.0 & 29.5 & - \\
					Interpolation (15k) & & & & \\
					\hline
					Summarization w/ Extraction & \textbf{61.99} & 18.52 & 38.46 & \textbf{86.20} \\
					+ GPT-3.5 Turbo (4,096) & & & & \\
					Central truncation & 46.20 & 4.38 & 38.27 & \textbf{82.19} \\
					+ LongT5 (4,096) & & & & \\
					Skimming w/ post-sampling & 46.76 & 4.56 & 39.61 & \textbf{81.96} \\
					removal + LongT5 (4,096) & & & & \\
					\hline
				\end{tabular}
			
				\caption{Evaluation on the GovReport dataset.}
			\end{table}

		\end{frame}

		\begin{frame}{Experimental Findings (contd.)}

			\begin{table}[!ht]
				\centering
				\tiny
			
				\begin{tabular}{c c c c c}
					\hline
					Model & ROUGE-1 & ROUGE-2 & ROUGE-L & BERTScore \\
					\hline
					BigBird-Pegasus (16k) & \textbf{60.64} & \textbf{42.46} & \textbf{50.01} & - \\
					\hline
					Skimming w/ pre-sampling & 27.40 & 3.31 & 21.25 & \textbf{82.62} \\
					removal + GPT-3.5 Turbo (4,096) & & & & \\
					Central truncation + & 27.77 & 3.09 & 20.56 & \textbf{82.57} \\
					GPT-3.5 Turbo (4,096) & & & & \\
					Skimming w/ post-sampling & 26.16 & 2.13 & 20.21 & \textbf{82.40} \\
					removal + GPT-3.5 Turbo (4,096) & & & & \\
					\hline
				\end{tabular}
			
				\caption{Evaluation on the BigPatent dataset.}
			\end{table}

		\end{frame}

		\begin{frame}{Time Analysis}

			\begin{figure}
				\centering
				\includegraphics[width=.8\textwidth]{../Report/Images/encoder-times.png}
			\end{figure}
			
		\end{frame}
