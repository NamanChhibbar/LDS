\section{Problem Statement}

\begin{frame}{The Problem}

\begin{itemize}
	\item LLMs suffer from one major limitation: limited context size.
	\item<2-> This is due to the quadratic self-attention mechanism in the transformer
	architecture.
	\item<3-> For example, BERT has a context size of just 512 tokens (about 1.5 pages
	of text).
	\item<4> Due to this, LLMs can not process long texts wherein summarization is
	valuable for efficient extraction of information.
\end{itemize}

\end{frame}
