\section{Problem Statement}


\begin{frame}{The Problem}

	\begin{itemize}
		\item LLMs suffer from one major limitation: limited context size.
		\item This is due to the quadratic self-attention mechanism in the transformer
		architecture.
		\item For example, BERT has a context size of just 512 tokens (about half page
		of text).
		\item Due to this, LLMs can not process long texts wherein summarization is
		valuable for efficient extraction of information.
	\end{itemize}

\end{frame}
