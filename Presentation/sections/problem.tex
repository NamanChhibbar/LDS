\section{The Problem}

\begin{frame}{The Problem}

  \begin{itemize}
    \item Transformers suffer from one major issue: limited context size.
    \item This is due to the quadratic complexity of the self-attention mechanism in the its architecture.
    \item For example, BART has a context size of just 1024 tokens (about 770 words).
    \item Due to this, transformers can not process long texts.
  \end{itemize}

\end{frame}
